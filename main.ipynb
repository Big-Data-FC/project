{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the **Big Data FC** project is to **predict** how many **points** a **football team** belonging to the main European football leagues will end the season with, according to the **characteristics of its players**.\n",
    "\n",
    "To reach the goal, data relative to the **football players** will first be loaded, in order to then compose the **football teams**.\n",
    "After that, a second dataset will be used to gather seasonal **rankings**, for every football team.\n",
    "\n",
    "The project as a whole is composed of:\n",
    "\n",
    "* This **notebook**, containing all steps of:\n",
    "  * Data loading.\n",
    "  * Data cleaning and pre-processing\n",
    "  * Data visualization.\n",
    "  * Data analysis.\n",
    "  * Learning and evaluation.\n",
    "* A custom [**scraper**](https://github.com/Big-Data-FC/scraper), to gather further players data.\n",
    "* A set of **REST APIs** to query the loaded data and the prediction model.\n",
    "* The collection of [scraped datasets](https://github.com/Big-Data-FC/datasets).\n",
    "\n",
    "During the project, multiple approaches and techniques were explored, all described in this notebook.\n",
    "\n",
    "_By [Daniele Solombrino](https://github.com/dansolombrino) and [Davide Quaranta](https://github.com/fortym2)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook configuration, global parameters and utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[state that we trained the models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 5)\n"
     ]
    }
   ],
   "source": [
    "attempts = range(1, 5)\n",
    "\n",
    "print(attempts)\n",
    "\n",
    "TRAINED_MODELS_DIRS = {\n",
    "    \"Attempt \" + str(k) : {\n",
    "        \"Regression\": {\n",
    "            \"Linear Regression\": f\"trained_models/attempt_{k}/regression/linear_regression\",\n",
    "            \"Prediction Tree\": f\"trained_models/attempt_{k}/regression/prediction_tree\",\n",
    "            \"Gradient Boosted Tree\": f\"trained_models/attempt_{k}/regression/gradient_boosted_tree\",\n",
    "            \"Random Forest\": f\"trained_models/attempt_{k}/regression/random_forest\",\n",
    "        },\n",
    "        \"Classification\": {\n",
    "            \"SVM\": f\"trained_models/attempt_{k}/classification/svm\",\n",
    "            \"Decision Tree\": f\"trained_models/attempt_{k}/classification/decision_tree\",\n",
    "            \"Logistic Regression\": f\"trained_models/attempt_{k}/classification/logistic_regression\",\n",
    "            \"Random Forest\": f\"trained_models/attempt_{k}/classification/random_forest\",\n",
    "            \"MLP\": f\"trained_models/attempt_{k}/classification/mlp\",\n",
    "        }\n",
    "    } for k in attempts\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_exists(model_path):\n",
    "    return os.path.isdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_disk(model_type, model_path):\n",
    "    return model_type.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, a series of components that are used across the entire project are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PySpark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "import gc\n",
    "\n",
    "import builtins\n",
    "import operator\n",
    "import json\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "%matplotlib widget\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/24 16:46:47 WARN Utils: Your hostname, rig-2022 resolves to a loopback address: 127.0.1.1; using 192.168.1.190 instead (on interface wlp5s0)\n",
      "22/06/24 16:46:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/24 16:46:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.ui.port\", \"4050\")\n",
    "    .set(\"spark.executor.memory\", \"4G\")\n",
    "    .set(\"spark.driver.memory\", \"20G\")\n",
    "    .set(\"spark.driver.maxResultSize\", \"10G\")\n",
    ")\n",
    "# .set(\"spark.master\", \"spark://192.168.1.189:4050\")\n",
    "\n",
    "\n",
    "# create the context\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random_seed = random.randint(0, sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_target_relation(\n",
    "    data, x, y, n_cols = 2, figsize = (15, 30), color = \"#000000\"\n",
    "):\n",
    "\n",
    "    n_rows = int(len(x) / n_cols) if len(x) >= n_cols else n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "    for x_ind, x_value in enumerate(x):\n",
    "        ax = sns.regplot(\n",
    "            data=data,\n",
    "            x=x_value,\n",
    "            y=y,\n",
    "            color = color,\n",
    "            ax=axes[x_ind // n_cols, x_ind % n_cols] if n_rows > 1 else axes,\n",
    "        )\n",
    "\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distribution(\n",
    "    data,\n",
    "    features,\n",
    "    figsize=(4,4), \n",
    "    color=\"#000000\",\n",
    "    n_cols=2\n",
    "):\n",
    "\n",
    "    n_rows = int(len(features) / n_cols) if len(features) >= n_cols else n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "    for feature_ind, feature in enumerate(features):\n",
    "        _ = sns.histplot(\n",
    "            data[feature],\n",
    "            kde=True,\n",
    "            color=color,\n",
    "            facecolor=color,\n",
    "            ax=axes[feature_ind // n_cols, feature_ind % n_cols] if n_rows > 1 else axes,\n",
    "        )\n",
    "\n",
    "    fig.tight_layout(pad=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(\n",
    "    data, features, title=\"Pearson Correlation Matrix\", figsize=(16,12)\n",
    "):\n",
    "\n",
    "    mask = np.zeros_like(data[features].corr(), dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    with sns.axes_style(\"white\"):  # Temporarily set the background to white\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        plt.title(title, fontsize=24)\n",
    "\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "        _ = sns.heatmap(\n",
    "            data[features].corr(),\n",
    "            linewidths=0.25,\n",
    "            vmax=0.7,\n",
    "            square=True,\n",
    "            ax=ax,\n",
    "            cmap=cmap,\n",
    "            linecolor=\"w\",\n",
    "            annot=True,\n",
    "            annot_kws={\"size\": 8},\n",
    "            mask=mask,\n",
    "            cbar_kws={\"shrink\": 0.9},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.ml.regression import Regressor\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel, LinearRegressionTrainingSummary\n",
    "from pyspark.ml.regression import RandomForestRegressor, RandomForestRegressionModel\n",
    "from pyspark.ml.classification import Classifier\n",
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import RandomForestClassificationTrainingSummary\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationSummary\n",
    "\n",
    "from pyspark.ml.regression import RegressionModel\n",
    "from pyspark.ml.classification import ClassificationModel\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluators(estimator, label_col, prediction_col, evaluation_metrics):\n",
    "    evaluators = dict()\n",
    "\n",
    "    if isinstance(estimator, Regressor) or isinstance(estimator, RegressionModel):\n",
    "        evaluators = {\n",
    "            metric: RegressionEvaluator(\n",
    "                labelCol=label_col,\n",
    "                predictionCol=prediction_col,\n",
    "                metricName=metric,\n",
    "            )\n",
    "            for metric in evaluation_metrics\n",
    "        }\n",
    "    elif isinstance(estimator, Classifier) or isinstance(estimator, OneVsRest) or isinstance(estimator, ClassificationModel):\n",
    "        evaluators = {\n",
    "            metric: MulticlassClassificationEvaluator(\n",
    "                labelCol=label_col,\n",
    "                predictionCol=prediction_col,\n",
    "                metricName=metric,\n",
    "            )\n",
    "            for metric in evaluation_metrics\n",
    "        }\n",
    "    else:\n",
    "        raise Exception(\"Unexpected estimator, got\" + str(type(estimator)))\n",
    "\n",
    "    return evaluators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_best_model(\n",
    "    estimator, \n",
    "    param_grid,\n",
    "    evaluator_cv\n",
    "):\n",
    "    cross_validator = CrossValidator(\n",
    "        estimator=estimator,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator_cv,\n",
    "        numFolds=5,\n",
    "        collectSubModels=False\n",
    "    )\n",
    "    \n",
    "    cross_validated_model = cross_validator.fit(train_df)\n",
    "\n",
    "    return cross_validated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_learning_models(\n",
    "    best_model, \n",
    "    evaluators,\n",
    "    save_training_result_path=None\n",
    "):\n",
    "    has_summary = True\n",
    "    \n",
    "    out_dict = {\n",
    "        \"train_set_evaluation\": dict(),\n",
    "        \"test_set_evaluation\": dict()\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        has_summary = best_model.hasSummary\n",
    "    except AttributeError as e:\n",
    "        # since the only models that have summary\n",
    "        # have the hasSummary field,\n",
    "        # it is needed to catch this exception\n",
    "        has_summary = False\n",
    "    \n",
    "    if has_summary:\n",
    "        print(\"Summary available, retrieving data...\")\n",
    "\n",
    "        if isinstance(\n",
    "            best_model, LinearRegressionModel\n",
    "        ) or isinstance(best_model, RandomForestClassificationModel):\n",
    "            training_result = best_model.summary\n",
    "        elif isinstance(best_model, MultilayerPerceptronClassificationModel):\n",
    "            training_result = best_model.summary()\n",
    "        \n",
    "        if any(\n",
    "            x in type(best_model).__name__ for x in [\n",
    "                \"LinearRegression\", \n",
    "                \"DecisionTreeRegressor\", \n",
    "                \"GBTRegressor\",\n",
    "                \"RandomForestRegressor\"\n",
    "            ]\n",
    "        ):\n",
    "            out_dict[\"train_set_evaluation\"][\"r2\"] = training_result.r2\n",
    "            out_dict[\"train_set_evaluation\"][\"r2adj\"] = training_result.r2adj\n",
    "            out_dict[\"train_set_evaluation\"][\"meanSquaredError\"] = training_result.meanSquaredError\n",
    "            out_dict[\"train_set_evaluation\"][\"meanAbsoluteError\"] = training_result.meanAbsoluteError\n",
    "            out_dict[\"train_set_evaluation\"][\"rootMeanSquaredError\"] = training_result.rootMeanSquaredError\n",
    "            out_dict[\"train_set_evaluation\"][\"explainedVariance\"] = training_result.explainedVariance\n",
    "        else:\n",
    "            out_dict[\"train_set_evaluation\"][\"accuracy\"] = training_result.accuracy\n",
    "            out_dict[\"train_set_evaluation\"][\"falsePositiveRateByLabel\"] = training_result.falsePositiveRateByLabel\n",
    "            out_dict[\"train_set_evaluation\"][\"precisionByLabel\"] = training_result.precisionByLabel\n",
    "            out_dict[\"train_set_evaluation\"][\"recallByLabel\"] = training_result.recallByLabel\n",
    "            out_dict[\"train_set_evaluation\"][\"truePositiveRateByLabel\"] = training_result.truePositiveRateByLabel\n",
    "            out_dict[\"train_set_evaluation\"][\"weightedFalsePositiveRate\"] = training_result.weightedFalsePositiveRate\n",
    "            out_dict[\"train_set_evaluation\"][\"weightedPrecision\"] = training_result.weightedPrecision\n",
    "            out_dict[\"train_set_evaluation\"][\"weightedRecall\"] = training_result.weightedRecall\n",
    "            out_dict[\"train_set_evaluation\"][\"weightedTruePositiveRate\"] = training_result.weightedTruePositiveRate\n",
    "            out_dict[\"train_set_evaluation\"][\"fMeasureByLabel\"] = training_result.fMeasureByLabel()\n",
    "            out_dict[\"train_set_evaluation\"][\"weightedFMeasure\"] = training_result.weightedFMeasure()\n",
    "\n",
    "        predictions = best_model.transform(test_df)\n",
    "                    \n",
    "        for e, evaluator in evaluators.items():\n",
    "            if save_training_result_path is not None:\n",
    "                out_dict[\"test_set_evaluation\"][evaluator.getMetricName()] = evaluator.evaluate(predictions)\n",
    "    \n",
    "    else: # no summary available\n",
    "        print(\"No summary available, using custom evaluation procedure...\")\n",
    "        for stage_name, stage_df in zip([\"Train\", \"Test\"], [train_df, test_df]):\n",
    "            predictions = best_model.transform(stage_df)\n",
    "\n",
    "            for e, evaluator in evaluators.items():\n",
    "                if save_training_result_path is not None:\n",
    "                    out_dict[f\"{stage_name.lower()}_set_evaluation\"][evaluator.getMetricName()] = evaluator.evaluate(predictions)\n",
    "\n",
    "    if save_training_result_path is not None:\n",
    "        os.makedirs(\n",
    "            os.path.join(\n",
    "                *save_training_result_path.split(\"/\")[:-1]\n",
    "            ), \n",
    "            exist_ok=True\n",
    "        ) \n",
    "        \n",
    "        with open(save_training_result_path, \"w\", encoding=\"utf8\") as f:\n",
    "            f.write(json.dumps(out_dict))\n",
    "\n",
    "    pprint.pprint(out_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def print_model_evaluation(model_evaluation_path):\n",
    "    with open(model_evaluation_path) as json_file:\n",
    "        pprint.pprint(json.load(json_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "PCA_NUM_COMPONENTS = 2\n",
    "\n",
    "def perform_pca(df, num_components, input_col, output_col):\n",
    "    pca = PCA(\n",
    "        k=num_components, \n",
    "        inputCol=input_col, \n",
    "        outputCol=output_col\n",
    "    )\n",
    "    pca_model = pca.fit(df)\n",
    "\n",
    "    return pca_model.transform(df), pca_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_explained_variance(pca_model, num_components_to_plot=2, figsize=(8,6)):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    _ = sns.barplot(\n",
    "        x=[i for i in range(num_components_to_plot)],\n",
    "        y=pca_model.explainedVariance.values[0:num_components_to_plot],\n",
    "        ax=ax,\n",
    "        palette=\"summer\"\n",
    "    )\n",
    "\n",
    "    _ = ax.set_xlabel(\"Eigenvalues\", labelpad=16, fontsize=16)\n",
    "    _ = ax.set_ylabel(\"Proportion of Variance\", fontsize=16)\n",
    "    _ = ax.set_xticklabels(\n",
    "        [f\"Principal Component {i}\" for i in range(num_components_to_plot)], \n",
    "        rotation=0\n",
    "    )\n",
    "    _ = ax.set_title(\"Explained variance of each Principal Component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(\n",
    "    x,\n",
    "    y,\n",
    "    x_label,\n",
    "    y_label,\n",
    "    title=\"\",\n",
    "    c=None,\n",
    "    c_map=plt.cm.get_cmap(\"tab10\"),\n",
    "    figsize=(12,8),\n",
    "    ):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    _ = plt.scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        c=y if c is None else c,\n",
    "        edgecolor=\"none\",\n",
    "        #alpha=1,\n",
    "        cmap=c_map,\n",
    "        axes=ax,\n",
    "    )\n",
    "\n",
    "    _ = ax.set_xlabel(x_label, labelpad=20, fontsize=16)\n",
    "    _ = ax.set_ylabel(y_label, fontsize=16)\n",
    "    _ = ax.set_title(title)\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGRESSION_EVALUATION_METRICS = [\"r2\", \"mse\", \"rmse\", \"mae\", \"var\"]\n",
    "REGRESSION_EVALUATION_METRIC_CV = \"r2\"\n",
    "\n",
    "REGRESSION_LABEL_COL = \"points\"\n",
    "\n",
    "regression_evaluator_cv = RegressionEvaluator(\n",
    "    metricName=REGRESSION_EVALUATION_METRIC_CV\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION_EVALUATION_METRICS = [\n",
    "    \"f1\",\n",
    "    \"accuracy\",\n",
    "    \"weightedPrecision\",\n",
    "    \"weightedRecall\",\n",
    "    \"weightedTruePositiveRate\",\n",
    "    \"weightedFalsePositiveRate\",\n",
    "    \"weightedFMeasure\",\n",
    "    \"truePositiveRateByLabel\",\n",
    "    \"falsePositiveRateByLabel\",\n",
    "    \"precisionByLabel\",\n",
    "    \"recallByLabel\",\n",
    "    \"fMeasureByLabel\"\n",
    "]\n",
    "\n",
    "CLASSIFICATION_EVALUATION_METRIC_CV = \"accuracy\"\n",
    "\n",
    "CLASSIFICATION_LABEL_COL = \"macro_place\"\n",
    "\n",
    "classification_evaluator_cv = MulticlassClassificationEvaluator(\n",
    "    metricName=CLASSIFICATION_EVALUATION_METRIC_CV\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "linear_regression_estimator = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell initializes the Spark context, with a given configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading football players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to load football players data, which comes from to two different sources:\n",
    "\n",
    "* For seasons between 2015 and 2020 (called \"modern\"): [FIFA 15-21 complete dataset](https://www.kaggle.com/datasets/stefanoleone992/fifa-21-complete-player-dataset)\n",
    "\n",
    "* For season between 2007 and 2014 (called \"legacy\"): scraped data from [sofifa.com](https://sofifa.com), a website specialized in storing data taken from EA Sports FIFA games.\n",
    "\n",
    "As introduced before, scraped datasets are committed in a [GitHub repository](https://github.com/Big-Data-FC/datasets).\n",
    "\n",
    "From now on, the terms \"modern\" and \"legacy\" will be used to refer to the two kinds of datasets.\n",
    "\n",
    "Initially, modern and legacy data will be splitted in two different dataframes, since there are some differences in the structure of their data, among these different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "modern_df = spark.read.csv(\n",
    "    \"data/players_*.csv\", sep=\",\", inferSchema=True, header=True, multiLine=True\n",
    ")\n",
    "\n",
    "legacy_df = spark.read.csv(\n",
    "    \"data/scraped_players_*.csv\", sep=\",\", inferSchema=True, header=True, \n",
    "    multiLine=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing football players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to focus the project on the major European leagues, it is useful to define a list of leagues to filter, and also to define a list of season to easily discriminate between modern an legacy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the European Leagues supported by Big-Data-FC\n",
    "leagues = [\n",
    "    \"Italian Serie A\",\n",
    "    \"Spain Primera Division\",\n",
    "    \"German 1. Bundesliga\",\n",
    "    \"French Ligue 1\",\n",
    "    \"English Premier League\",\n",
    "    \"Holland Eredivisie\",\n",
    "]\n",
    "\n",
    "# These are the seasons supported by Big-Data-FC\n",
    "seasons_modern = [\"20\", \"19\", \"18\", \"17\", \"16\", \"15\", \"14\"] \n",
    "# seasons_modern = [\"20\"] \n",
    "seasons_legacy = [\"13\", \"12\", \"11\", \"10\", \"09\", \"08\", \"07\"]\n",
    "# seasons_legacy = [\"13\"]\n",
    "seasons = seasons_legacy + seasons_modern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next definition is about **macro roles**, which is a custom-defined abstration to aggregate affine football roles.\n",
    "\n",
    "For example, all the midfield roles such as \"central midfielder\", \"advanced midfielder\", \"left|right wing\" can be **grouped together** in the same macro role \"midfielder\".\n",
    "\n",
    "Macro roles will be used later on, in a subsequent learning phase. For this reason, much more about them will be touched in future points.\n",
    "\n",
    "The following cell defines the actual aggregation from FIFA roles abbreviations into macro roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_roles = [\"0.0\", \"1.0\", \"2.0\", \"3.0\"]\n",
    "\n",
    "roles_to_macro_roles_dict = {\n",
    "    \"GK\": \"0\",\n",
    "    \"SW\": \"1\",\n",
    "    \"LB\": \"1\",\n",
    "    \"RB\": \"1\",\n",
    "    \"RWB\": \"1\",\n",
    "    \"LWB\": \"1\",\n",
    "    \"CB\": \"1\",\n",
    "    \"CDM\": \"2\",\n",
    "    \"CM\": \"2\",\n",
    "    \"RM\": \"2\",\n",
    "    \"LM\": \"2\",\n",
    "    \"CAM\": \"2\",\n",
    "    \"RW\": \"3\",\n",
    "    \"LW\": \"3\",\n",
    "    \"ST\": \"3\",\n",
    "    \"LF\": \"3\",\n",
    "    \"RF\": \"3\",\n",
    "    \"CF\": \"3\",\n",
    "}\n",
    "\n",
    "NUM_MACRO_ROLES = 4\n",
    "\n",
    "roles_to_macro_role_UDF = udf(\n",
    "    lambda roles: float(\n",
    "        roles_to_macro_roles_dict[roles.split(\",\")[0]]\n",
    "    ), \n",
    "    StringType()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the columns associeted to players also contain graphical or data that is generally not informative for the project's purpose (such as shirt number, celebration moves, etc), a list of meaningful columns has been defined, on which the actual working dataframes will be based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"short_name\",\n",
    "    \"club_name\",\n",
    "    \"league_name\",\n",
    "    \"season\",\n",
    "    \"player_positions\",\n",
    "    \"macro_role\",\n",
    "    \"overall\",\n",
    "    \"value\",\n",
    "    \"pace\",\n",
    "    \"shooting\",\n",
    "    \"passing\",\n",
    "    \"dribbling\",\n",
    "    \"defending\",\n",
    "    \"physic\",\n",
    "    \"attacking_crossing\",\n",
    "    \"attacking_finishing\",\n",
    "    \"attacking_heading_accuracy\",\n",
    "    \"attacking_short_passing\",\n",
    "    \"skill_dribbling\",\n",
    "    \"skill_fk_accuracy\",\n",
    "    \"skill_long_passing\",\n",
    "    \"skill_ball_control\",\n",
    "    \"movement_acceleration\",\n",
    "    \"movement_sprint_speed\",\n",
    "    \"movement_reactions\",\n",
    "    \"power_shot_power\",\n",
    "    \"power_stamina\",\n",
    "    \"power_strength\",\n",
    "    \"power_long_shots\",\n",
    "    \"mentality_aggression\",\n",
    "    \"mentality_penalties\",\n",
    "    \"defending_standing_tackle\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets do **not** explicitly include the **year** (season) to which the record refers to.\n",
    "\n",
    "Rather, this information is implicitly stored in a URL (also for the non-scraped ones, which still originate from the same source), which has its own field.\n",
    "For this reason, a function to extract such information from aforementioned field is needed.\n",
    "\n",
    "As an example, a if the URL is `/player/41236/zlatan-ibrahimovic/130034/`, the corresponding season is `13` (from `/13xxxx/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(url):\n",
    "    url_split = url.split(\"/\")\n",
    "\n",
    "    # FIFA years must be scaled by a negative factor of one (i.e. 2021 has to be 2020, etc.)\n",
    "    # This is needed to ensure compatibility with the seasonal score dataset\n",
    "    return str(\n",
    "        (int(url_split[-2 if url_split[-1] == \"\" else -1][0:2]) - 1)\n",
    "    ).zfill(2)\n",
    "\n",
    "get_season_UDF = udf(lambda url: get_season(url), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the **monetary value** of players is different among the modern and legacy dataset.\n",
    "\n",
    "Specifically, the legacy one abbreviates the values into the form `€10M` to represent `€10000000`.\n",
    "\n",
    "The following function is used to convert it into the extended one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def expand_value_UDF(value):\n",
    "    value = value.replace(\"€\", \"\")\n",
    "    if value[-1] not in (\"K\", \"M\"):\n",
    "        # no abbreviation at the end\n",
    "        return float(value) + 0.0000001\n",
    "\n",
    "    # extract the number and the unit\n",
    "    num = value[:-1]\n",
    "    unit = value[-1]\n",
    "\n",
    "    # decide based on the unit\n",
    "    if unit == \"M\":\n",
    "        return float(num) * 1000000\n",
    "    if unit == \"K\":\n",
    "        return float(num) * 1000\n",
    "\n",
    "    return \"ERROR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the actual pre-process.\n",
    "\n",
    "Some actions are needed by legacy and modern both, whilst other are exclusive to either one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting season from the player URL, as per previous cell\n",
    "pre_processed_modern_df = modern_df.withColumn(\n",
    "    \"season\", get_season_UDF(col(\"player_url\"))\n",
    ")\n",
    "pre_processed_legacy_df = legacy_df.withColumn(\n",
    "    \"season\", get_season_UDF(col(\"player_url\"))\n",
    ")\n",
    "\n",
    "# Taking only the players playing for teams in supported Leagues, \n",
    "# in the supported seasons\n",
    "pre_processed_modern_df = pre_processed_modern_df.where(\n",
    "    (pre_processed_modern_df.league_name.isin(leagues))\n",
    "    &\n",
    "    (pre_processed_modern_df.season.isin(seasons_modern))\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.where(\n",
    "    (pre_processed_legacy_df.league_name.isin(leagues))\n",
    "    &\n",
    "    (pre_processed_legacy_df.season.isin(seasons_legacy))\n",
    ")\n",
    "\n",
    "# Dropping duplicate players\n",
    "pre_processed_modern_df = pre_processed_modern_df.dropDuplicates([\"player_url\"])\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.dropDuplicates([\"player_url\"])\n",
    "\n",
    "# Selected columns have been checked for absence of null/missing data.\n",
    "# Nevertheless, to ensure compatibility and reusability with other datasets, \n",
    "# a null-filling sweep is done\n",
    "pre_processed_modern_df = pre_processed_modern_df.na.fill(0)\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.na.fill(0)\n",
    "\n",
    "# Getting the macro role of the player, according to its field position\n",
    "pre_processed_modern_df = pre_processed_modern_df.withColumn(\n",
    "    \"macro_role\", roles_to_macro_role_UDF(col(\"player_positions\"))\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumn(\n",
    "    \"macro_role\", roles_to_macro_role_UDF(col(\"player_positions\"))\n",
    ")\n",
    "\n",
    "# Renaming the \"value_eur\" field to \"value\" to have compatiblity with legacy\n",
    "pre_processed_modern_df = pre_processed_modern_df.withColumnRenamed(\n",
    "    \"value_eur\", \"value\"\n",
    ")\n",
    "\n",
    "# Convert the monetary value to have compatibility with modern\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumn(\n",
    "    \"value\", expand_value_UDF(col(\"value\"))\n",
    ")\n",
    "\n",
    "# Renaming some legacy columns, so as they have the same name as in the modern\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"pas\", \"passing\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"dri\", \"dribbling\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.drop(col(\"defending\"))\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"def\", \"defending\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"phy\", \"physic\"\n",
    ")\n",
    "\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"sho\", \"shooting\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"pac\", \"pace\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"bov\", \"overall\"\n",
    ")\n",
    "\n",
    "# Keeping only the needed columns.\n",
    "pre_processed_modern_df = pre_processed_modern_df.select(columns)\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.select(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking whether some monetary values have not been successfully converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if pre_processed_legacy_df.select(\"value\").where(col(\"value\") == \"ERROR\").count() > 0:\n",
    "    print(\"WARN: some abbreviated monetary values were not correctly expanded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing, both dataframes have the same set of columns, so they can be concatenated together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_df = pre_processed_modern_df.unionByName(\n",
    "    pre_processed_legacy_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del legacy_df\n",
    "del modern_df\n",
    "del pre_processed_legacy_df\n",
    "del pre_processed_modern_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result of this section.\n",
    "\n",
    "For graphical reasons, only a selection of the columns will be shown, just to give an idea of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                 (0 + 7) / 7][Stage 11:====>             (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+------+----------------+----------+-------+--------+----+--------+\n",
      "|   short_name|           club_name|         league_name|season|player_positions|macro_role|overall|   value|pace|shooting|\n",
      "+-------------+--------------------+--------------------+------+----------------+----------+-------+--------+----+--------+\n",
      "|   D. Boateng|            SD Eibar|Spain Primera Div...|    14|         CDM, CM|       2.0|     71|  950000|  69|      60|\n",
      "|    S. Padoin|            Juventus|     Italian Serie A|    14|          RM, CM|       2.0|     73| 2100000|  73|      64|\n",
      "|    S. Padoin|            Juventus|     Italian Serie A|    15|      LB, RM, CM|       1.0|     75| 2600000|  73|      63|\n",
      "|    S. Padoin|            Cagliari|     Italian Serie A|    18|      RB, LM, CM|       1.0|     71|  675000|  68|      63|\n",
      "|     C. Terzi|             Palermo|     Italian Serie A|    14|              CB|       1.0|     70| 1100000|  60|      45|\n",
      "|  A. Aquilani|       UD Las Palmas|Spain Primera Div...|    17|         CM, CDM|       2.0|     75| 5000000|  54|      70|\n",
      "|      S. Pepe|            Juventus|     Italian Serie A|    14|          RM, LM|       2.0|     75| 2900000|  80|      72|\n",
      "|  R. Jarstein|          Hertha BSC|German 1. Bundesliga|    14|              GK|       0.0|     70| 1600000|   0|       0|\n",
      "|  R. Jarstein|          Hertha BSC|German 1. Bundesliga|    19|              GK|       0.0|     83|11500000|   0|       0|\n",
      "|         Xavi|        FC Barcelona|Spain Primera Div...|    14|              CM|       2.0|     86|15500000|  66|      72|\n",
      "|   F. Modesto|Sporting Club de ...|      French Ligue 1|    14|          CB, RB|       1.0|     72|  400000|  55|      51|\n",
      "|     A. Boruc|         Bournemouth|English Premier L...|    16|              GK|       0.0|     76| 2300000|   0|       0|\n",
      "|     A. Boruc|         Bournemouth|English Premier L...|    17|              GK|       0.0|     77|  600000|   0|       0|\n",
      "|       Aduriz|Athletic Club de ...|Spain Primera Div...|    15|              ST|       3.0|     82|10500000|  71|      83|\n",
      "|       Aduriz|Athletic Club de ...|Spain Primera Div...|    19|              ST|       3.0|     82| 6500000|  61|      82|\n",
      "|Victor Valdés|   Manchester United|English Premier L...|    15|              GK|       0.0|     82|14500000|   0|       0|\n",
      "|    G. Pegolo|            Sassuolo|     Italian Serie A|    16|              GK|       0.0|     72| 1200000|   0|       0|\n",
      "|    D. Brivio|               Genoa|     Italian Serie A|    16|              LB|       1.0|     73| 2200000|  68|      56|\n",
      "|      Y. Pelé|Olympique de Mars...|      French Ligue 1|    17|              GK|       0.0|     75| 2700000|   0|       0|\n",
      "|      Y. Pelé|Olympique de Mars...|      French Ligue 1|    18|              GK|       0.0|     75| 1600000|   0|       0|\n",
      "+-------------+--------------------+--------------------+------+----------------+----------+-------+--------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pre_processed_df.select(*columns[0:10]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete list of columns is (again for graphical reasons) printed here in an horizontal format, where each item is a tuple of the form `(field_name, type)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('short_name', 'string'), ('club_name', 'string'), ('league_name', 'string'), ('season', 'string'), ('player_positions', 'string'), ('macro_role', 'string'), ('overall', 'int'), ('value', 'string'), ('pace', 'int'), ('shooting', 'int'), ('passing', 'int'), ('dribbling', 'int'), ('defending', 'int'), ('physic', 'int'), ('attacking_crossing', 'int'), ('attacking_finishing', 'int'), ('attacking_heading_accuracy', 'int'), ('attacking_short_passing', 'int'), ('skill_dribbling', 'int'), ('skill_fk_accuracy', 'int'), ('skill_long_passing', 'int'), ('skill_ball_control', 'int'), ('movement_acceleration', 'int'), ('movement_sprint_speed', 'int'), ('movement_reactions', 'int'), ('power_shot_power', 'int'), ('power_stamina', 'int'), ('power_strength', 'int'), ('power_long_shots', 'int'), ('mentality_aggression', 'int'), ('mentality_penalties', 'int'), ('defending_standing_tackle', 'int')]\n"
     ]
    }
   ],
   "source": [
    "print(pre_processed_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building football teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_teams_df = pre_processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having pre-processed the football players, it's time to build the football teams.\n",
    "\n",
    "For the sake of the learning stage of the project, **teams are differentiated across different seasons**; for example, Real Madrid of 2020 is **different** than Real Madrid of 2018.\n",
    "\n",
    "A football team is then modeled as **the set of the average of the features of all of its football players**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that are considered as features\n",
    "PLAYER_FEATURES = [\n",
    "    \"overall\",\n",
    "    \"value\",\n",
    "    \"pace\",\n",
    "    \"shooting\",\n",
    "    \"passing\",\n",
    "    \"dribbling\",\n",
    "    \"defending\",\n",
    "    \"physic\",\n",
    "    \"attacking_crossing\",\n",
    "    \"attacking_finishing\",\n",
    "    \"attacking_heading_accuracy\",\n",
    "    \"attacking_short_passing\",\n",
    "    \"skill_dribbling\",\n",
    "    \"skill_fk_accuracy\",\n",
    "    \"skill_long_passing\",\n",
    "    \"skill_ball_control\",\n",
    "    \"movement_acceleration\",\n",
    "    \"movement_sprint_speed\",\n",
    "    \"movement_reactions\",\n",
    "    \"power_shot_power\",\n",
    "    \"power_stamina\",\n",
    "    \"power_strength\",\n",
    "    \"power_long_shots\",\n",
    "    \"mentality_aggression\",\n",
    "    \"mentality_penalties\",\n",
    "    \"defending_standing_tackle\"\n",
    "]\n",
    "\n",
    "# Apposing the avg pre-fix to features\n",
    "PLAYER_FEATURES_AVG = [\n",
    "    \"avg(\" + player_feature + \")\" for player_feature in PLAYER_FEATURES\n",
    "]\n",
    "\n",
    "# Target variable of the learning stage\n",
    "TARGET_VARIABLE = \"points\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composing the football team, as introduced before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_teams_df = football_teams_df.select(\n",
    "    \"season\", \"club_name\", *PLAYER_FEATURES\n",
    ").groupBy(\n",
    "    [\"season\", \"club_name\"]\n",
    ").agg(\n",
    "    { player_feature: \"avg\" for player_feature in PLAYER_FEATURES }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result of this section.\n",
    "\n",
    "For graphical reasons, only a selection of the columns will be shown, just to give an idea of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                 (0 + 7) / 7][Stage 16:==>               (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----------------+--------------------+------------------+------------------+------------------+\n",
      "|season|           club_name|     avg(overall)|          avg(value)|         avg(pace)|     avg(shooting)|      avg(passing)|\n",
      "+------+--------------------+-----------------+--------------------+------------------+------------------+------------------+\n",
      "|    18|        ADO Den Haag|66.33333333333333|  1114166.6666666667|            56.625|46.583333333333336|51.041666666666664|\n",
      "|    16|       Chievo Verona|             71.3|  2411666.6666666665|              55.9|              48.7| 52.13333333333333|\n",
      "|    17|Athletic Club de ...|            75.75|   9794642.857142856|59.214285714285715| 53.42857142857143|59.964285714285715|\n",
      "|    16|         Southampton|71.93939393939394|   5041060.606060606| 64.66666666666667| 51.09090909090909| 57.45454545454545|\n",
      "|    19|          Hertha BSC|71.75757575757575|   5872575.757575758| 63.84848484848485|52.696969696969695|56.515151515151516|\n",
      "|    15|         Bournemouth|69.89655172413794|   2162758.620689655| 65.10344827586206| 48.58620689655172| 55.03448275862069|\n",
      "|    15|   Manchester United|76.25925925925925|1.3582037037037037E7| 59.96296296296296|51.148148148148145|58.148148148148145|\n",
      "|    19|   Manchester United|76.84848484848484|1.5187121212121213E7| 67.03030303030303| 56.06060606060606| 62.63636363636363|\n",
      "|    20|FC Girondins de B...| 70.6896551724138|  3988620.6896551726| 55.44827586206897| 48.10344827586207|52.827586206896555|\n",
      "|    15|             Watford|71.32142857142857|  2790178.5714285714|62.785714285714285|49.535714285714285|54.535714285714285|\n",
      "|    18|    AS Saint-Étienne|70.11111111111111|   4653518.518518519| 63.51851851851852|52.111111111111114| 57.44444444444444|\n",
      "|    16|     West Ham United|72.54545454545455|   7179242.424242424| 66.93939393939394|54.515151515151516| 55.90909090909091|\n",
      "|    15|          1. FC Köln|71.79166666666667|  3654166.6666666665|62.666666666666664|            50.875|54.333333333333336|\n",
      "|    16|       SC Heerenveen|66.76666666666667|  1257333.3333333333|              59.4|              45.3|              50.9|\n",
      "|    14|Sporting Club de ...|66.21212121212122|   996363.6363636364| 60.93939393939394| 50.54545454545455| 53.84848484848485|\n",
      "|    15|     Atlético Madrid|            76.92|            1.2458E7|             64.84|              53.6|             59.24|\n",
      "|    17|    Stade Rennais FC|69.15151515151516|   3382575.757575758| 61.36363636363637| 50.03030303030303|              54.0|\n",
      "|    17|        Swansea City|71.24242424242425|   5101818.181818182| 58.06060606060606| 49.45454545454545| 53.63636363636363|\n",
      "|    17| Eintracht Frankfurt|72.33333333333333|   5289393.939393939| 64.27272727272727| 53.18181818181818|57.303030303030305|\n",
      "|    15|       VfB Stuttgart|70.79310344827586|  3495862.0689655175|62.206896551724135|50.275862068965516| 55.58620689655172|\n",
      "+------+--------------------+-----------------+--------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "football_teams_df.select(\n",
    "    \"season\", \"club_name\", *PLAYER_FEATURES_AVG[0:5]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete list of columns with their type is (in a compact horizontal form):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('season', 'string'), ('club_name', 'string'), ('avg(attacking_short_passing)', 'double'), ('avg(shooting)', 'double'), ('avg(power_stamina)', 'double'), ('avg(skill_long_passing)', 'double'), ('avg(power_strength)', 'double'), ('avg(defending_standing_tackle)', 'double'), ('avg(skill_fk_accuracy)', 'double'), ('avg(skill_dribbling)', 'double'), ('avg(dribbling)', 'double'), ('avg(pace)', 'double'), ('avg(mentality_aggression)', 'double'), ('avg(movement_reactions)', 'double'), ('avg(movement_sprint_speed)', 'double'), ('avg(passing)', 'double'), ('avg(movement_acceleration)', 'double'), ('avg(attacking_heading_accuracy)', 'double'), ('avg(attacking_finishing)', 'double'), ('avg(defending)', 'double'), ('avg(attacking_crossing)', 'double'), ('avg(power_long_shots)', 'double'), ('avg(mentality_penalties)', 'double'), ('avg(overall)', 'double'), ('avg(power_shot_power)', 'double'), ('avg(value)', 'double'), ('avg(physic)', 'double'), ('avg(skill_ball_control)', 'double')]\n"
     ]
    }
   ],
   "source": [
    "print(football_teams_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading football teams seasonal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having dealt with the football players and composed them into football clubs, it's time to get **\"target\" data** for the learning stage: the final ranking, for every team of every year.\n",
    "\n",
    "First step is to load the data from disk, which has been taken from the [European Football Dataset](https://www.kaggle.com/datasets/josephvm/european-club-football-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_scores_df = (\n",
    "    spark.read.csv(\n",
    "        \"data/all_tables_fixed_renamed_leagues.csv\",\n",
    "        sep=\",\",\n",
    "        inferSchema=True,\n",
    "        header=True,\n",
    "        multiLine=True,\n",
    "    )\n",
    "    .withColumnRenamed(\"Year\", \"season\")\n",
    "    .withColumnRenamed(\"Team\", \"club_name\")\n",
    "    .withColumnRenamed(\"P\", \"points\")\n",
    "    .withColumnRenamed(\"Place\", \"place\")\n",
    "    .withColumnRenamed(\"League\", \"league\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing football teams seasonal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all the available columns, only the ones in `seasonal_scores_columns` will be taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_scores_columns = [\n",
    "    \"season\", \"league\", \"club_name\", \"points\", \"place\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seasonal scores dataset uses **abbreviated version of football team names** (for example: `BAR` for Barcelona, `LEI` for Leicester, etc.), which would cause **incompatibility** with modern and legacy FIFA datasets, which instead uses complete names.\n",
    "\n",
    "Furthermore, the abbreviations are **not standard**, so there are **conflicts** such as, among others:\n",
    "\n",
    "* `BAR` both for Barcelona and Bari (different leagues).\n",
    "* `HUE` both for Huelva and Huesca (same league).\n",
    "\n",
    "For this reason, a **custom** hand-made **mapping** procedure has been developed in order to resolve such conflicts.\n",
    "\n",
    "Another source of incompatibility originated from **inconsistencies** within FIFA datasets, from year to year; for example, some teams had slight variations in their names (e.g. Torino and Torino FC).\n",
    "\n",
    "All said inconsistencies have been **manually solved** and disambiguated at the dataset-level.\n",
    "\n",
    "Furthermore, the final **mapping** between abbreviations and FIFA names resulted into a **custom-made dataset** (also available in the linked dataset GitHub repo), which is the following form:\n",
    "\n",
    "|abbr|league|club_name|fifa_club_name|\n",
    "|---|---|---|---|\n",
    "|AAC|German Bundesliga|Aachen|Alemannia Aachen|\n",
    "|ADO|Dutch Eredivisie|Ado Den Haag|ADO Den Haag|\n",
    "|AJA|Dutch Eredivisie|Ajax|Ajax|\n",
    "|AJC|French Ligue 1|Ajaccio|AC Ajaccio|\n",
    "|ALB|Spanish La Liga|Albacete|Albacete BP|\n",
    "|...|...|...|...|\n",
    "\n",
    "Aforementioned mapping is then converted to **JSON** with an utility script in the same repository, and loaded here in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/clubs_map.json\")\n",
    "club_name_abbr_to_ext = json.load(f)\n",
    "f.close()\n",
    "\n",
    "ABBREVIATED_CLUB_NAME_NOT_FOUD = \"ABBREVIATED_CLUB_NAME_NOT_FOUD\"\n",
    "GENERAL_EXCEPTION = \"GENERAL_EXCEPTION\"\n",
    "\n",
    "def extend_club_name(club_name_abbr):\n",
    "    try:\n",
    "        return club_name_abbr_to_ext[club_name_abbr]\n",
    "    except KeyError as e:\n",
    "        return ABBREVIATED_CLUB_NAME_NOT_FOUD\n",
    "    except Exception as e:\n",
    "        return GENERAL_EXCEPTION\n",
    "\n",
    "extend_club_name_UDF = udf(\n",
    "    lambda club_name_abbr: extend_club_name(str(club_name_abbr)),\n",
    "    StringType(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rankings dataset the seasons are expressed as `YYYY`, whilst FIFA uses the `YY` encoding.\n",
    "\n",
    "For this reason, to guarantee compatibility, season values in the rankings dataset is abbreviated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviate_season_UDF = udf(\n",
    "    lambda season: str(season)[-2:],\n",
    "    StringType(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the actual **data pre-processing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_seasonal_scores_df = seasonal_scores_df\n",
    "\n",
    "# Abbreviating season, as per previous cell\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"season\", abbreviate_season_UDF(col(\"season\"))\n",
    ")\n",
    "\n",
    "# Keeping only supported leagues in supported seasons\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.where(\n",
    "    (pre_processed_seasonal_scores_df.season.isin(seasons))\n",
    "    & \n",
    "    (pre_processed_seasonal_scores_df.league.isin(leagues))\n",
    ")\n",
    "\n",
    "# Selecting only the desired columns\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.select(\n",
    "    seasonal_scores_columns\n",
    ")\n",
    "\n",
    "# Although data has been checked for duplicates and missing value, to ensure \n",
    "# operabiloty with other datasets, the pre-processing steps are still performed\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.dropDuplicates(\n",
    "    seasonal_scores_columns\n",
    ")\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.na.fill(0)\n",
    "\n",
    "# Extending club names, as per previous cell\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"club_name\", extend_club_name_UDF(col(\"club_name\"))\n",
    ")\n",
    "# Checking whether club name expansions went all good or not\n",
    "if pre_processed_seasonal_scores_df.filter(\n",
    "    col(\"club_name\") == ABBREVIATED_CLUB_NAME_NOT_FOUD\n",
    ").count() > 0:\n",
    "    print(\"WARN: some clubs have NOT been found\")\n",
    "    print(\"Please check your data\")\n",
    "\n",
    "# Casting points to float, as required by learning procedures\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"points\", pre_processed_seasonal_scores_df.points.cast(DoubleType())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result of this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+------+-----+\n",
      "|season|              league|           club_name|points|place|\n",
      "+------+--------------------+--------------------+------+-----+\n",
      "|    18|English Premier L...|   Tottenham Hotspur|  71.0|    4|\n",
      "|    18|English Premier L...|   Manchester United|  66.0|    6|\n",
      "|    08|Spain Primera Div...|RC Recreativo de ...|  33.0|   20|\n",
      "|    16|Spain Primera Div...|            RC Celta|  45.0|   13|\n",
      "|    16|Spain Primera Div...|       UD Las Palmas|  39.0|   14|\n",
      "|    15|     Italian Serie A|               Carpi|  38.0|   18|\n",
      "|    12|  Holland Eredivisie|        FC Groningen|  43.0|    7|\n",
      "|    13|  Holland Eredivisie|           FC Twente|  63.0|    3|\n",
      "|    12|      French Ligue 1|      Stade de Reims|  43.0|   14|\n",
      "|    16|      French Ligue 1|Sporting Club de ...|  34.0|   20|\n",
      "|    16|English Premier L...|   Tottenham Hotspur|  86.0|    2|\n",
      "|    07|Spain Primera Div...|          Sevilla FC|  64.0|    5|\n",
      "|    18|Spain Primera Div...|      Rayo Vallecano|  32.0|   20|\n",
      "|    17|German 1. Bundesliga|         Hannover 96|  39.0|   13|\n",
      "|    19|German 1. Bundesliga|   Borussia Dortmund|  69.0|    2|\n",
      "|    19|  Holland Eredivisie|                Ajax|  56.0|    1|\n",
      "|    09|      French Ligue 1|  Olympique Lyonnais|  72.0|    2|\n",
      "|    10|      French Ligue 1|            OGC Nice|  46.0|   17|\n",
      "|    12|      French Ligue 1|Olympique de Mars...|  71.0|    2|\n",
      "|    20|      French Ligue 1|           FC Nantes|  40.0|   18|\n",
      "+------+--------------------+--------------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_processed_seasonal_scores_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining football teams features with their seasonal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two dataframes (players and seasonal scores) need to be merged together to form a single dataframe.\n",
    "This can easily be done by joining on the key (`season`, `club_name`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = football_teams_df.join(\n",
    "    pre_processed_seasonal_scores_df,\n",
    "    on = [\"season\", \"club_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check whether some clubs were left out by the aforementioned join operation, two differences are computed:\n",
    "\n",
    "* Seasonal scores dataframe - joined dataframe.\n",
    "* Football teams dataframe - joined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "diff = pre_processed_seasonal_scores_df.select(\"club_name\").subtract(df.select(\"club_name\")).distinct()\n",
    "\n",
    "if diff.count() > 0:\n",
    "    print(\"WARN: Some football teams have been left out the join (pre_processed_seasonal_scores_df)\")\n",
    "    diff.show()\n",
    "\n",
    "diff = football_teams_df.select(\"club_name\").subtract(df.select(\"club_name\")).distinct()\n",
    "if diff.count() > 0:\n",
    "    print(\"WARN: Some football teams have been left out the join (football_teams_df)\")\n",
    "    diff.show()\n",
    "\n",
    "del diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result of this section.\n",
    "\n",
    "For graphical reasons, only a selection of the columns will be shown, just to give an idea of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-----------------+------------------+------+-----+\n",
      "|season|              league|           club_name|     avg(overall)|         avg(pace)|points|place|\n",
      "+------+--------------------+--------------------+-----------------+------------------+------+-----+\n",
      "|    18|  Holland Eredivisie|        ADO Den Haag|66.33333333333333|            56.625|  45.0|    9|\n",
      "|    17|Spain Primera Div...|Athletic Club de ...|            75.75|59.214285714285715|  43.0|   16|\n",
      "|    16|     Italian Serie A|       Chievo Verona|             71.3|              55.9|  43.0|   14|\n",
      "|    16|English Premier L...|         Southampton|71.93939393939394| 64.66666666666667|  46.0|    8|\n",
      "|    19|German 1. Bundesliga|          Hertha BSC|71.75757575757575| 63.84848484848485|  41.0|   10|\n",
      "|    15|English Premier L...|         Bournemouth|69.89655172413794| 65.10344827586206|  42.0|   16|\n",
      "|    15|English Premier L...|   Manchester United|76.25925925925925| 59.96296296296296|  66.0|    5|\n",
      "|    20|      French Ligue 1|FC Girondins de B...| 70.6896551724138| 55.44827586206897|  45.0|   12|\n",
      "|    19|English Premier L...|   Manchester United|76.84848484848484| 67.03030303030303|  66.0|    3|\n",
      "|    15|English Premier L...|             Watford|71.32142857142857|62.785714285714285|  45.0|   13|\n",
      "|    16|  Holland Eredivisie|       SC Heerenveen|66.76666666666667|              59.4|  43.0|    9|\n",
      "|    16|English Premier L...|     West Ham United|72.54545454545455| 66.93939393939394|  45.0|   11|\n",
      "|    18|      French Ligue 1|    AS Saint-Étienne|70.11111111111111| 63.51851851851852|  66.0|    4|\n",
      "|    15|German 1. Bundesliga|          1. FC Köln|71.79166666666667|62.666666666666664|  43.0|    9|\n",
      "|    14|      French Ligue 1|Sporting Club de ...|66.21212121212122| 60.93939393939394|  47.0|   12|\n",
      "|    15|Spain Primera Div...|     Atlético Madrid|            76.92|             64.84|  88.0|    3|\n",
      "|    16|Spain Primera Div...|     Atlético Madrid|77.44827586206897|61.793103448275865|  78.0|    3|\n",
      "|    17|German 1. Bundesliga| Eintracht Frankfurt|72.33333333333333| 64.27272727272727|  49.0|    8|\n",
      "|    17|English Premier L...|        Swansea City|71.24242424242425| 58.06060606060606|  33.0|   18|\n",
      "|    17|      French Ligue 1|    Stade Rennais FC|69.15151515151516| 61.36363636363637|  58.0|    5|\n",
      "+------+--------------------+--------------------+-----------------+------------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"season\", \"league\", \"club_name\", \"avg(overall)\", \"avg(pace)\", \"points\", \"place\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full list of columns with their type is (in a compact horizontal form):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('season', 'string'), ('club_name', 'string'), ('avg(attacking_short_passing)', 'double'), ('avg(shooting)', 'double'), ('avg(power_stamina)', 'double'), ('avg(skill_long_passing)', 'double'), ('avg(power_strength)', 'double'), ('avg(defending_standing_tackle)', 'double'), ('avg(skill_fk_accuracy)', 'double'), ('avg(skill_dribbling)', 'double'), ('avg(dribbling)', 'double'), ('avg(pace)', 'double'), ('avg(mentality_aggression)', 'double'), ('avg(movement_reactions)', 'double'), ('avg(movement_sprint_speed)', 'double'), ('avg(passing)', 'double'), ('avg(movement_acceleration)', 'double'), ('avg(attacking_heading_accuracy)', 'double'), ('avg(attacking_finishing)', 'double'), ('avg(defending)', 'double'), ('avg(attacking_crossing)', 'double'), ('avg(power_long_shots)', 'double'), ('avg(mentality_penalties)', 'double'), ('avg(overall)', 'double'), ('avg(power_shot_power)', 'double'), ('avg(value)', 'double'), ('avg(physic)', 'double'), ('avg(skill_ball_control)', 'double'), ('league', 'string'), ('points', 'double'), ('place', 'int')]\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MACRO_PLACES = 5\n",
    "\n",
    "def get_macro_place(place, league, complex=False):\n",
    "    if league in [\"Dutch Eredivisie\", \"German Bundesliga\"]:\n",
    "        if 1 <= place <= 3:\n",
    "            return 0.0\n",
    "        if 4 <= place <= 6:\n",
    "            return 1.0\n",
    "        if 7 <= place <= 10:\n",
    "            return 2.0\n",
    "        if 11 <= place <= 15:\n",
    "            return 3.0\n",
    "        if 16 <= place <= 18:\n",
    "            return 4.0\n",
    "    else:\n",
    "        if 1 <= place <= 4:\n",
    "            return 0.0\n",
    "        if 5 <= place <= 8:\n",
    "            return 1.0\n",
    "        if 9 <= place <= 12:\n",
    "            return 2.0\n",
    "        if 13 <= place <= 16:\n",
    "            return 3.0\n",
    "        if 17 <= place <= 20:\n",
    "            return 4.0\n",
    "\n",
    "    return None\n",
    "    \n",
    "get_macro_place_UDF = udf(\n",
    "    lambda place, league, complex: get_macro_place(float(place), league, complex),\n",
    "    DoubleType(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"macro_place\", get_macro_place_UDF(col(\"place\"), col(\"league\"), lit(False))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del seasonal_scores_df\n",
    "del pre_processed_seasonal_scores_df\n",
    "del football_teams_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good practice whenever a data-driven task is being taclked is to first visualize the data.\n",
    "\n",
    "In fact, even just by simply looking at data we may find some useful information which may have a direct impact on the subsequent learning phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points distribution\n",
    "\n",
    "Kicking the visualization stage off with data distirbutions for the end-of-season points, across all supported seasons, for all supported leagues.\n",
    "\n",
    "Data distribution is very important to monitor: exposing a model too many or too few time to a value may be source of bias, which we want to avoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.displot(df.select(\"points\").toPandas(), kde=True)\n",
    "p.fig.tight_layout(pad=1.5)\n",
    "plt.title(\"Points distribution across all leagues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations can be done:\n",
    "\n",
    "1. Data tends to approximatively follow a Gaussian/Normal distribution\n",
    "2. Said distribution appears to be slighlty skewed towards left\n",
    "\n",
    "These observations perfectly coincide with domain knowledge.\n",
    "1. In every league, there are just a few very strong teams (Champions League qualifiers), a limited number of very bad teams (fighting for relegation) and then a multitude of middle-table teams\n",
    "2. Some leagues do not have much talent in the middle part of the table, resulting in a general \"equilibrium\" between such team.\n",
    "As a result, scores of such teams not be high, but rather close to the mean value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following density plot instead focuses on each league, showing again the points distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.displot(df.toPandas(), x=\"points\", hue=\"league\", kind=\"kde\", aspect=1.7, palette=\"tab10\")\n",
    "p.fig.tight_layout(pad=1.5)\n",
    "plt.title(\"Points distribution per league\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations can be made:\n",
    "\n",
    "* The **French** and **Spanish** leagues are the less skewed among all leagues, possibly meaning that they are characterized by a majority of \"average\" teams.\n",
    "* The **Italian** and **English** leagues are very similar on low and middle points, but start to behave slightly different on the middle-high and high parts of the table.\n",
    "* The **Dutch** league is the most left-skewed, possibly hinting at the lower quality of the league, as suggested by the domain knowledge.\n",
    "* The **Spanish** league has a boost on high points, hinting at a consistent dominancy of one or more teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall player quality distribution\n",
    "\n",
    "Similar visualizations can be done to analyze the overall player quality.\n",
    "\n",
    "The following plot shows the distribution of the player quality (`avg(overall)`) across all leagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.displot(df.select(\"avg(overall)\").toPandas(), kde=True)\n",
    "p.fig.tight_layout(pad=1.5)\n",
    "plt.title(\"Player quality across all leagues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarily to what observed for the points, the plot shows that:\n",
    "\n",
    "* A vast majority of the players are of average quality.\n",
    "* A minority of the players are below-average.\n",
    "* A minority of the players are above-average.\n",
    "\n",
    "The following plot focuses on the single leagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.displot(df.toPandas(), x=\"avg(overall)\", hue=\"league\", kind=\"kde\", aspect=1.7, palette=\"tab10\")\n",
    "p.fig.tight_layout(pad=1.5)\n",
    "plt.title(\"Player quality per league\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot it is possible to observe that:\n",
    "\n",
    "* The Dutch league hosts the majority of below-average players.\n",
    "* The French league follows the Dutch one in below-average players.\n",
    "* The German league has the highest amount of average players.\n",
    "* The German, Engligh, Italian and Spanish leagues have a peak of average players.\n",
    "* The same leagues have a considerable amount of above-average players.\n",
    "* The Spanish leagues dominates on the above-average players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1: \"naive\" player features\n",
    "\n",
    "The following section represents the first attempt at building a model able to predict the points of a team, considering the features of its players.\n",
    "\n",
    "This attempt has been renamed as \"naive\", meaning that it simply consist of an analysis based on **all the features** that are related to a **player's technical abilities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that do not inherently represent players abilities\n",
    "ALL_FEATURES = PLAYER_FEATURES_AVG\n",
    "ALL_FEATURES.remove(\"avg(overall)\")\n",
    "ALL_FEATURES.remove(\"avg(value)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning procedures require that the considered features are assembled in a vector.\n",
    "\n",
    "The following cell performs this operation by adding a new \"assembled\" column to the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=ALL_FEATURES, outputCol=\"all_vec\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to define a some custom utility functions to plot multiple figures, that will be called multiple times along the course of the project.\n",
    "\n",
    "The following cells contain the definition of said functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before learning, it is good practice to first gather in-depth information on the structure of the data, as well as obtaining useful visualizations.\n",
    "\n",
    "In the following sections, multiple approaches at data visualization will be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section a first analysis is performed on _raw_ data, meaning that no normalization or standardization is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color to be used in the plots for raw data\n",
    "COLOR_RAW = \"#332FD0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Pandas for easy plotting with Seaborn\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **feature-target correlation** shows the correlation between each feature and the target variable (point).\n",
    "\n",
    "The idea is to immediately visualize whether there are specific features that are particularly correlated with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES, TARGET_VARIABLE, color=COLOR_RAW\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are too many datapoints, it is very hard to look at the plots. Let's see how visualizations change when focusing on some leagues or years, thus restricting the data scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf[pdf[\"season\"] == \"20\"], ALL_FEATURES, TARGET_VARIABLE, color=COLOR_RAW\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that:\n",
    "\n",
    "1. There is a general phenomenon of linear relation between each feature and the final points.\n",
    "2. There is a generalized high variance across all features.\n",
    "   1. E.g. taking the very last feature (`avg(defending_standing_tackle`), players with a value close to `50` result to a wide range of points.\n",
    "   2. On the contrary, in extreme values (close to `40` or `90`), the resulting variable have a narrower range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the distribution of the player features in the analyzed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES, color = COLOR_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot it is possible to observe that:\n",
    "\n",
    "1. Most features have a symmetrical distribution.\n",
    "2. Most features are centered around the middle.\n",
    "3. Some features are more skewed towards left, hinting that they may be more \"rare\".\n",
    "4. Some features don't properly follow a Gaussian distribution (`avg(pace)`, `avg(physic)`).\n",
    "\n",
    "Furthermore, observation `2` is linked to the observation `2.2` of the previous plot:\n",
    "\n",
    "* A lot of features have an average value.\n",
    "* Around the average values, there is high variance.\n",
    "\n",
    "Hence, most likely, the system will be exposed multiple times to this issue, affecting performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Pearson's Correlation Matrix** is a graphical tool to show a numerical value indicating the correlation between each variable.\n",
    "\n",
    "It is employed in the project, to find whether some features are correlated to each other, which can either be a problem for Linear Regression, but not so much for Tree-based models, since they by design perform a feature selection step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(pdf, ALL_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are:\n",
    "\n",
    "* A lot of positive correlations (red).\n",
    "* Some negative correlations (teal).\n",
    "* Some feature are tendentially not correlated to others (white).\n",
    "\n",
    "Using some domain knowledge, it is possible to further comment some correlations, like (among others):\n",
    "\n",
    "* The more a defender is good, the more aggressive they are.\n",
    "* The more a defender is good, the more able to perform standing tackles they are.\n",
    "* The more a player has good physical strenght, the more fast (pace) they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have some features with skewed distributions, it is needed to standardize, to see whether it helps to center feature distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is needed to define a scaler which takes feature from `all_vec` and places scaled versions in `all_vec_std`.\n",
    "\n",
    "The configuration is of a standard z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"all_vec\", \n",
    "    outputCol=\"all_vec_std\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_STD = \"#9254C8\"\n",
    "\n",
    "ALL_FEATURES_STD = [\n",
    "    player_feature + \"_std\" for player_feature in ALL_FEATURES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the scaler grouped all features in a single field (feature vector), but plotting requires using different fields (one per features), it is needed to define a function to extract each feature and place it in a separate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vec_to_cols(pdf, vec, columns):\n",
    "    tmp = pdf.reindex(\n",
    "        columns=list(pdf.columns) + columns\n",
    "    )\n",
    "\n",
    "    tmp[columns] = tmp[\n",
    "        vec\n",
    "    ].transform(\n",
    "        {\n",
    "            columns[i]: operator.itemgetter(i) for i, p in enumerate(columns)\n",
    "        }\n",
    "    )\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = feature_vec_to_cols(pdf, \"all_vec_std\", ALL_FEATURES_STD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see the effects of the function by looking at the columns list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As done in the _raw data_ section, a feature-target correlation analysis is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_STD, TARGET_VARIABLE, color=COLOR_STD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again zoom on a single season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf[pdf[\"season\"] == \"20\"], ALL_FEATURES_STD, TARGET_VARIABLE, color=COLOR_STD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization does not improve the variance issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf, ALL_FEATURES_STD, color = COLOR_STD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization did not change the feature distribution.\n",
    "\n",
    "It is not needed to plot the correlation matrix again, since it is not affected by a change of scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further attempt consists of apply Logarithmic Transformation, still trying to combat skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_LOG = \"#E15FED\"\n",
    "ALL_FEATURES_LOG = [\n",
    "    player_feature + \"_log\" for player_feature in ALL_FEATURES\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function translates a value to its $log_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_log_UDF = udf(\n",
    "    lambda value: float(np.log2(value)), DoubleType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, feature_log in zip(ALL_FEATURES, ALL_FEATURES_LOG):\n",
    "    df = df.withColumn(feature_log, to_log_UDF(col(feature)))\n",
    "\n",
    "df = df.withColumn(\"points_log\", to_log_UDF(col(\"points\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_LOG, \"points_log\", color=COLOR_LOG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also Logarithmic transformation did not help with the variance problem.\n",
    "\n",
    "Let's again see a zommed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf[pdf[\"season\"] == \"20\"], ALL_FEATURES_LOG, \"points_log\", color=COLOR_LOG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithmic transformation did not help with variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES_LOG, color = COLOR_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithmic transformation slightly moved the skewdness of all features towards right, still not solving it; as a consequence, centered or right-skewed features are not slightly worsened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-max transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_MIN_MAX = \"#6EDCD9\"\n",
    "ALL_FEATURES_MIN_MAX = [\n",
    "    player_feature + \"_min_max\" for player_feature in ALL_FEATURES\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"all_vec\", \n",
    "    outputCol=\"all_vec_min_max\"\n",
    ")\n",
    "\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()\n",
    "pdf = feature_vec_to_cols(pdf, vec=\"all_vec_min_max\", columns=ALL_FEATURES_MIN_MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_MIN_MAX, TARGET_VARIABLE, color=COLOR_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoomed view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf[pdf[\"season\"] == \"20\"], ALL_FEATURES_MIN_MAX, TARGET_VARIABLE, color=COLOR_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES_MIN_MAX, color=COLOR_MIN_MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following GIF compares the previously plotted feature distributions.\n",
    "\n",
    "![GIF changes illustration](https://s8.gifyu.com/images/ezgif.com-gif-maker4847ef0cb3270edd.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before learning, some further useful visualizations are given.\n",
    "\n",
    "**From now on, Min-Max transofrmed data will be used.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (on min-max normalized data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, pca_model = perform_pca(\n",
    "    df=df,\n",
    "    num_components=PCA_NUM_COMPONENTS,\n",
    "    input_col=\"all_vec_min_max\",\n",
    "    output_col=\"all_vec_min_max_pcs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_explained_variance(\n",
    "    pca_model=pca_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=pdf.all_vec_min_max_pcs.map(lambda x: x[0]),\n",
    "    y=pdf.all_vec_min_max_pcs.map(lambda x: x[1]),\n",
    "    c=pdf.points,\n",
    "    x_label=\"Principal Component 0\",\n",
    "    y_label=\"Principal Component 1\",\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA result is quite underwhelming: teams with widely different end-of-the-season placement tend to be mixed-up.\n",
    "\n",
    "Notice that Principal Component Analysis is a linear model, so it is bounded to produce linear representations: usually, non-trivial problems present non-linear data.\n",
    "\n",
    "For this reason, adding some non-linearity may possibly lead to a better lower-dimensional embedding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=pdf.all_vec_min_max_pcs.map(lambda x: x[0]),\n",
    "    y=pdf.points,\n",
    "    x_label=\"Principal Component 0\",\n",
    "    y_label=\"Points\",\n",
    ")\n",
    "scatter_plot(\n",
    "    x=pdf.all_vec_min_max_pcs.map(lambda x: x[1]),\n",
    "    y=pdf.points,\n",
    "    x_label=\"Principal Component 1\",\n",
    "    y_label=\"Points\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is one of the most used tools, when assuming non-linearity in the data space.\n",
    "\n",
    "Implementation from SciKit Learn framework will be used, since PySpark does not provide any version of this tool.\n",
    "\n",
    "SciKit Learn works with NumPy structures, so the feature vector in PySpark DataFrame must be converted to a NDArray structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vec_min_max_np = np.array(\n",
    "    list(\n",
    "        map(\n",
    "            lambda v: v[\"all_vec_min_max\"].toArray(), \n",
    "            df.select(\"all_vec_min_max\").collect()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "points_np = np.array(\n",
    "    list(\n",
    "        map(\n",
    "            lambda v: v[\"points\"], \n",
    "            df.collect()\n",
    "        )\n",
    "    )\n",
    ").reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data is ready, TNSE is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_embedding = TSNE(\n",
    "    n_components=2, learning_rate='auto', init='random', method=\"barnes_hut\"\n",
    ").fit_transform(all_vec_min_max_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, once the embedding space is populated, the result is plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=tsne_embedding[:,0],\n",
    "    y=tsne_embedding[:,1],\n",
    "    c=points_np,\n",
    "    x_label=\"TSNE Embedding Dimension 0\",\n",
    "    y_label=\"TSNE Embedding Dimension 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=tsne_embedding[:,0],\n",
    "    y=points_np,\n",
    "    x_label=\"TSNE Embedding Dimension 0\",\n",
    "    y_label=\"Points\",\n",
    ")\n",
    "scatter_plot(\n",
    "    x=tsne_embedding[:,1],\n",
    "    y=points_np,\n",
    "    x_label=\"TSNE Embedding Dimension 1\",\n",
    "    y_label=\"Points\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, adding non-linearity does not seem to improve the situation.\n",
    "\n",
    "Not much progress has been made, w.r.t. PCA \n",
    "\n",
    "[TODO] comment on variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on visualizations\n",
    "\n",
    "Data seems to be:\n",
    "\n",
    "* Not linearly separable.\n",
    "* Suffering from high variance.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning for attempt 1\n",
    "\n",
    "The problem has been treated in two different forms:\n",
    "\n",
    "* As **regression** on the **points**.\n",
    "* As **classification** on the table/ranking areas (macro places).\n",
    "\n",
    "In both cases, for all the different models will be used:\n",
    "\n",
    "* **K-Fold Cross Validation**, to perform validation tests on the model performances.\n",
    "* **Grid-search approach**, to perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.9, 0.1], seed=random_seed)\n",
    "\n",
    "FEATURES_COL = \"all_vec_min_max\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_evaluator_cv.setLabelCol(REGRESSION_LABEL_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_regression_linear_regression_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Linear Regression model NOT found in disk, training...\")\n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = LinearRegression()\n",
    "\n",
    "    linear_regression_param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "        .addGrid(estimator.solver, [\"auto\", \"normal\"])\n",
    "        .addGrid(estimator.fitIntercept, [True, False])\n",
    "        .addGrid(estimator.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=linear_regression_param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Linear Regression model found in disk, loading...\")\n",
    "    model = LinearRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Linear Regression\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Evaluating Linear Regression model trained in previous cell...\")\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/regression/linear_regression.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/regression/linear_regression.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Saving Linear Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/regression/linear_regression\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import DecisionTreeRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_regression_decision_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\"Prediction Tree model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = DecisionTreeRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Decision Tree model found in disk, loading...\")\n",
    "    model = DecisionTreeRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Prediction Tree\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Prediction Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/regression/prediction_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/regression/prediction_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\"Saving Decision Tree Regressor model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/regression/prediction_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import GBTRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_regression_gradient_boosted_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\"Regression Gradient Boosted Tree model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = GBTRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [2, 5, 10])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Gradient Boosted Tree model found in disk, loading...\")\n",
    "    model = GBTRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Gradient Boosted Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/regression/gradient_boosted_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/regression/gradient_boosted_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\"Saving Gradient Boosted Tree Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/regression/gradient_boosted_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_regression_random_forest_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Regression Random Forest model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = RandomForestRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .addGrid(estimator.numTrees, [20, 40])\n",
    "        .addGrid(\n",
    "            estimator.featureSubsetStrategy, \n",
    "            [\"auto\", \"onethird\", \"all\", \"log2\"]\n",
    "        )\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Random Forest model found in disk, loading...\")\n",
    "    model = RandomForestRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Random Forest\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Random Forest model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/regression/random_forest.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/regression/random_forest.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Saving Random Forest Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/regression/random_forest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_evaluator_cv.setLabelCol(CLASSIFICATION_LABEL_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.9, 0.1], seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    df.toPandas(), \n",
    "    CLASSIFICATION_LABEL_COL,\n",
    "    color=\"teal\",\n",
    "    n_cols=2,\n",
    "    figsize=(8,4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows comments:\n",
    "\n",
    "* **Naive approach**: sligh imbalancement against the low partition of the table.\n",
    "* **Compelx approach**: imbalancement in favor of the mid-table teams, confirming of course the visualizations in the beginning of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_COL = \"svmc_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LinearSVC(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=LABEL_COL,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics = evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models[label_col], \n",
    "        evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_COL = \"all_vec_min_max\"\n",
    "LABEL_COLS = [\"macro_place_naive\", \"macro_place_complex\"]\n",
    "PREDICTION_COL = \"lrc_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LogisticRegression(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_classification_decision_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\"Classification Decision Tree model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = DecisionTreeClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification Decision Tree model found in disk, loading...\")\n",
    "    model = DecisionTreeClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Decision Tree\"]\n",
    "    )\n",
    "    evaluators = get_evaluators(\n",
    "        model, \n",
    "        label_col=CLASSIFICATION_LABEL_COL, \n",
    "        prediction_col=PREDICTION_COL, \n",
    "        evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Decision Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/classification/decision_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/classification/decision_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\"Saving Decision Tree model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/classification/decision_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_classification_random_forest_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Classification Random Forest model NOT found in disk, training...\")\n",
    "    \n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = RandomForestClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .addGrid(estimator.numTrees, [20, 40])\n",
    "        .addGrid(estimator.featureSubsetStrategy, [\"auto\", \"onethird\", \"all\", \"log2\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification Random Forest model found in disk, loading...\")\n",
    "    model = RandomForestClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Random Forest\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Random Forest model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/classification/random_forest.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/classification/random_forest.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Saving Random Forest Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/classification/random_forest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_classification_mlp_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"MLP\"]\n",
    "):\n",
    "    print(\"Classification MLP model NOT found in disk, training...\")\n",
    "    \n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = MultilayerPerceptronClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.layers, [[24, NUM_MACRO_PLACES]])\n",
    "        .addGrid(estimator.solver, [\"gd\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification MLP model found in disk, loading...\")\n",
    "    model = MultilayerPerceptronClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"MLP\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"MLP\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating MLP model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/classification/mlp.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/classification/mlp.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"MLP\"]\n",
    "):\n",
    "    print(\"Saving MLP Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/classification/mlp\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2: \"less is more\"\n",
    "\n",
    "Ok, considering all features gives trash results.\n",
    "\n",
    "What if we embrace the \"less is more idea\" and try to improve the results by means of using less features?\n",
    "\n",
    "Nevertheless, feature correlation is very high, so, intrinsicly, it already did NOT make much sense to consider them all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log scaling reduces skewedness of \"avg(mentality_penalties)\" feature BUT it increases the skewedness of all the other features.\n",
    "The other scalings (z-score and min-max) do NOT appear to be different than the \"raw\" data distribution.\n",
    "\n",
    "For this reason and due to the limited amount of resources available on Google Colab, we decided to stick with the min-max scaled data.\n",
    "In fact, the min-max scaling places \"for free\" all the features in the same scale, which is a very important consideration for SVM, which will be used in the upcoming sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import UnivariateFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = UnivariateFeatureSelector(\n",
    "    featuresCol=\"all_vec_min_max\",\n",
    "    labelCol=TARGET_VARIABLE, \n",
    "    selectionMode=\"percentile\"\n",
    ").setFeatureType(\"continuous\").setLabelType(\n",
    "    \"categorical\"\n",
    ").setSelectionThreshold(0.08)\n",
    "\n",
    "NUM_FEATURES = [6, 12]\n",
    "THRESHOLDS = [num_features/24 for num_features in NUM_FEATURES]\n",
    "UFS_FEATURES = [\n",
    "    f\"all_vec_min_max_ufs_{num_features}\" for num_features in NUM_FEATURES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(\"all_vec_min_max_ufs_12\",\"all_vec_min_max_ufs_4\",\"all_vec_min_max_ufs_6\",\"all_vec_min_max_ufs_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fit_result = dict()\n",
    "\n",
    "for num_features, thr, ufs_features in zip(\n",
    "    NUM_FEATURES, THRESHOLDS, UFS_FEATURES\n",
    "):\n",
    "    selector.setSelectionThreshold(thr)\n",
    "    selector.setOutputCol(ufs_features),\n",
    "    fit_result[str(num_features)] = selector.fit(df)\n",
    "    df = fit_result[str(num_features)].transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Univariate Feature Selection selected these 6 features:\n",
      "['avg(pace)', 'avg(shooting)', 'avg(passing)', 'avg(dribbling)', 'avg(defending)', 'avg(attacking_crossing)']\n",
      "Univariate Feature Selection selected these 12 features:\n",
      "['avg(pace)', 'avg(shooting)', 'avg(passing)', 'avg(dribbling)', 'avg(defending)', 'avg(attacking_crossing)', 'avg(attacking_finishing)', 'avg(attacking_heading_accuracy)', 'avg(attacking_short_passing)', 'avg(skill_dribbling)', 'avg(skill_fk_accuracy)', 'avg(skill_long_passing)']\n"
     ]
    }
   ],
   "source": [
    "selected_features = dict()\n",
    "for num_features in NUM_FEATURES:\n",
    "    selected_features[str(num_features)] = list(\n",
    "        map(\n",
    "            lambda i: ALL_FEATURES[i], fit_result[str(num_features)].selectedFeatures\n",
    "        )\n",
    "    )\n",
    "    print(f\"Univariate Feature Selection selected these {num_features} features:\\n{selected_features[str(num_features)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=pdf[\"all_vec_min_max_ufs_2\"].map(lambda x: x[0]),\n",
    "    y=pdf[\"all_vec_min_max_ufs_2\"].map(lambda x: x[1]),\n",
    "    c=pdf[\"points\"],\n",
    "    x_label=\"Feature 0\",\n",
    "    y_label=\"Feature 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_key = list(selected_features.keys())[-1]\n",
    "\n",
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    selected_features[max_key], # last dictionary entry\n",
    "    figsize=(10,4),\n",
    "    color=\"orange\",\n",
    "    n_cols=(int(max_key) // 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf,\n",
    "    selected_features[max_key],\n",
    "    TARGET_VARIABLE,\n",
    "    figsize=(10,8), \n",
    "    color=COLOR_MIN_MAX,\n",
    "    n_cols=(int(max_key) // 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at it from the original pearson matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state that these data distribs are trash.\n",
    "\n",
    "So, for this reason, talk about overall and value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall as feature (min-max normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features are so correlated and performances of attempt 1 are trash, why not considering just the overall as a feature?\n",
    "\n",
    "Maybe, we're lucky and the overall captures some other characteristics thay may steer the prediction a little bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERALL = [\"avg(overall)\"]\n",
    "\n",
    "COLOR_OVERALL_MIN_MAX = \"green\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=OVERALL, outputCol=\"overall_vec\"\n",
    ")\n",
    "\n",
    "# min_max_df = assembler.transform(min_max_df)\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERALL_MIN_MAX = [\"avg(overall)_min_max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"overall_vec\", \n",
    "    outputCol=\"overall_vec_min_max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()\n",
    "\n",
    "pdf = feature_vec_to_cols(\n",
    "    pdf=pdf,\n",
    "    vec=\"overall_vec_min_max\",\n",
    "    columns=OVERALL_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf,\n",
    "    OVERALL_MIN_MAX,\n",
    "    TARGET_VARIABLE,\n",
    "    figsize=(10,4),\n",
    "    color=\"chocolate\",\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    OVERALL_MIN_MAX,\n",
    "    color=\"chocolate\",\n",
    "    figsize=(10,4),\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value as feature (min-max normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, we found this paper: \n",
    "\n",
    "So, why not try their approach as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE = [\"avg(value)\"]\n",
    "\n",
    "COLOR_VALUE_MIN_MAX = \"coral\"\n",
    "VALUE_MIN_MAX = [\"avg(value)_min_max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=VALUE, outputCol=\"value_vec\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"value_vec\", \n",
    "    outputCol=\"value_vec_min_max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()\n",
    "pdf = feature_vec_to_cols(\n",
    "    pdf=pdf,\n",
    "    vec=\"value_vec_min_max\",\n",
    "    columns=VALUE_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf,\n",
    "    VALUE_MIN_MAX,\n",
    "    TARGET_VARIABLE,\n",
    "    figsize=(10,4),\n",
    "    color=COLOR_VALUE_MIN_MAX,\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look, they are basically the same. In fact, if we check their correlation, we get... [high correlation on pearson]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    [\"avg(value)\"],\n",
    "    color=COLOR_VALUE_MIN_MAX,\n",
    "    figsize=(10,4),\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that there is a lot of skewness. We already know that the min_max normalization has the same exact curve.\n",
    "\n",
    "Let's try to see what happens by applying a $log_2$ transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"value_log\", to_log_UDF(\"avg(value)\"))\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    [\"value_log\"],\n",
    "    color=COLOR_VALUE_MIN_MAX,\n",
    "    figsize=(10,4),\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"value_log\"], outputCol=\"value_log_vec\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"value_log_vec\", \n",
    "    outputCol=\"value_log_vec_min_max\"\n",
    ")\n",
    "\n",
    "df = scaler.fit(df).transform(df)\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = feature_vec_to_cols(\n",
    "    pdf=pdf,\n",
    "    vec=\"value_log_vec_min_max\",\n",
    "    columns=[\"value_log_min_max\"]\n",
    ")\n",
    "\n",
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    [\"value_log_min_max\"],\n",
    "    color=COLOR_VALUE_MIN_MAX,\n",
    "    figsize=(10,4),\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint at a very high correlation, then show it with pearson matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = feature_vec_to_cols(\n",
    "    pdf=pdf,\n",
    "    vec=\"overall_vec_min_max\",\n",
    "    columns=OVERALL_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(\n",
    "    pdf, \n",
    "    [\"avg(overall)_min_max\", \"value_log_min_max\"],\n",
    "    figsize=(8,4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment this correlation, and move on with life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning for attempt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[\n",
    "    state that via hyperparam grid we'll set the feature column, meaning that we'll try to train the models on all of the attemps\n",
    "\n",
    "[state expected results, according to correlations and similar stuff]\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.9, 0.1], seed=random_seed)\n",
    "\n",
    "FEATURES_COL = [\"value_log_vec_min_max\"] + UFS_FEATURES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressionEvaluator_2e1244c16097"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_evaluator_cv.setLabelCol(REGRESSION_LABEL_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression model found in disk, loading...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_regression_linear_regression_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Linear Regression model NOT found in disk, training...\")\n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = LinearRegression()\n",
    "\n",
    "    linear_regression_param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "        .addGrid(estimator.solver, [\"auto\", \"normal\"])\n",
    "        .addGrid(estimator.fitIntercept, [True, False])\n",
    "        .addGrid(estimator.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=linear_regression_param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Linear Regression model found in disk, loading...\")\n",
    "    model = LinearRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Linear Regression\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing evaluation loaded from disk...\n",
      "{'test_set_evaluation': {'mae': 10.282958067971286,\n",
      "                         'mse': 157.20973969974162,\n",
      "                         'r2': 0.3898780907384233,\n",
      "                         'rmse': 12.538330817925551,\n",
      "                         'var': 113.01573971575404},\n",
      " 'train_set_evaluation': {'explainedVariance': 110.42552223114184,\n",
      "                          'meanAbsoluteError': 10.183925779761799,\n",
      "                          'meanSquaredError': 164.12135849213283,\n",
      "                          'r2': 0.40221007771142714,\n",
      "                          'r2adj': 0.39721113759472826,\n",
      "                          'rootMeanSquaredError': 12.810985851687326}}\n"
     ]
    }
   ],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Evaluating Linear Regression model trained in previous cell...\")\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/regression/linear_regression.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/regression/linear_regression.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Saving Linear Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/regression/linear_regression\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Decision Tree model found in disk, loading...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import DecisionTreeRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_regression_decision_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\"Prediction Tree model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = DecisionTreeRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Decision Tree model found in disk, loading...\")\n",
    "    model = DecisionTreeRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Prediction Tree\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing evaluation loaded from disk...\n",
      "{'test_set_evaluation': {'mae': 10.97990144675638,\n",
      "                         'mse': 195.8487180337438,\n",
      "                         'r2': 0.23992245008865476,\n",
      "                         'rmse': 13.994596029673161,\n",
      "                         'var': 156.3970812999072},\n",
      " 'train_set_evaluation': {'mae': 8.942549872827799,\n",
      "                          'mse': 132.02897833876614,\n",
      "                          'r2': 0.5191022458862111,\n",
      "                          'rmse': 11.490386344190787,\n",
      "                          'var': 142.5179023845134}}\n"
     ]
    }
   ],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Prediction Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/regression/prediction_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/regression/prediction_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\"Saving Decision Tree Regressor model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/regression/prediction_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Gradient Boosted Tree model found in disk, loading...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import GBTRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_regression_gradient_boosted_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\"Regression Gradient Boosted Tree model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = GBTRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        # .addGrid(estimator.maxDepth, [2, 5, 10])\n",
    "        # .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        # .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        # .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        # .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Gradient Boosted Tree model found in disk, loading...\")\n",
    "    model = GBTRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing evaluation loaded from disk...\n",
      "{'test_set_evaluation': {'mae': 11.099375442895116,\n",
      "                         'mse': 198.00148478356488,\n",
      "                         'r2': 0.2315676868144183,\n",
      "                         'rmse': 14.07130003885799,\n",
      "                         'var': 137.92860840415761},\n",
      " 'train_set_evaluation': {'mae': 8.000923515113074,\n",
      "                          'mse': 107.69162261660497,\n",
      "                          'r2': 0.6077477830638568,\n",
      "                          'rmse': 10.37745742542965,\n",
      "                          'var': 141.52724744034134}}\n"
     ]
    }
   ],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Gradient Boosted Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/regression/gradient_boosted_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/regression/gradient_boosted_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\"Saving Gradient Boosted Tree Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/regression/gradient_boosted_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Random Forest model found in disk, loading...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_regression_random_forest_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Regression Random Forest model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = RandomForestRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .addGrid(estimator.numTrees, [20, 40])\n",
    "        .addGrid(\n",
    "            estimator.featureSubsetStrategy, \n",
    "            [\"auto\", \"onethird\", \"all\", \"log2\"]\n",
    "        )\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Random Forest model found in disk, loading...\")\n",
    "    model = RandomForestRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Random Forest\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing evaluation loaded from disk...\n",
      "{'test_set_evaluation': {'mae': 10.595341769758344,\n",
      "                         'mse': 168.3672127162975,\n",
      "                         'r2': 0.3465765831321055,\n",
      "                         'rmse': 12.975639202609539,\n",
      "                         'var': 121.6186085584162},\n",
      " 'train_set_evaluation': {'mae': 8.799919977123281,\n",
      "                          'mse': 127.53751468765395,\n",
      "                          'r2': 0.5354617967187678,\n",
      "                          'rmse': 11.293250846751521,\n",
      "                          'var': 118.84311878579625}}\n"
     ]
    }
   ],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Random Forest model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/regression/random_forest.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/regression/random_forest.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Saving Random Forest Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/regression/random_forest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulticlassClassificationEvaluator_ac9a81b6beb7"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_evaluator_cv.setLabelCol(CLASSIFICATION_LABEL_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "FEATURES_COL = \"all_vec_min_max\"\n",
    "LABEL_COLS = [\"macro_place_naive\", \"macro_place_complex\"]\n",
    "PREDICTION_COL = \"svmc_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LinearSVC(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=LABEL_COL,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics = evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models[label_col], \n",
    "        evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "PREDICTION_COL = \"lrc_attempt2_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LogisticRegression(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Evaluating with label {label_col}\")\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Decision Tree model found in disk, loading...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_classification_decision_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\"Classification Decision Tree model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = DecisionTreeClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        # .addGrid(estimator.maxDepth, [5, 24])\n",
    "        # .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        # .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification Decision Tree model found in disk, loading...\")\n",
    "    model = DecisionTreeClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Decision Tree\"]\n",
    "    )\n",
    "    evaluators = get_evaluators(\n",
    "        model, \n",
    "        label_col=CLASSIFICATION_LABEL_COL, \n",
    "        prediction_col=PREDICTION_COL, \n",
    "        evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing evaluation loaded from disk...\n",
      "{'test_set_evaluation': {'accuracy': 0.2556818181818182,\n",
      "                         'f1': 0.2621327546299155,\n",
      "                         'fMeasureByLabel': 0.5070422535211268,\n",
      "                         'falsePositiveRateByLabel': 0.08888888888888889,\n",
      "                         'precisionByLabel': 0.6,\n",
      "                         'recallByLabel': 0.43902439024390244,\n",
      "                         'truePositiveRateByLabel': 0.43902439024390244,\n",
      "                         'weightedFMeasure': 0.2621327546299155,\n",
      "                         'weightedFalsePositiveRate': 0.17162426614481407,\n",
      "                         'weightedPrecision': 0.30235205548930355,\n",
      "                         'weightedRecall': 0.2556818181818182,\n",
      "                         'weightedTruePositiveRate': 0.2556818181818182},\n",
      " 'train_set_evaluation': {'accuracy': 0.43162983425414364,\n",
      "                          'f1': 0.42540806139901605,\n",
      "                          'fMeasureByLabel': 0.631768953068592,\n",
      "                          'falsePositiveRateByLabel': 0.07285342584562012,\n",
      "                          'precisionByLabel': 0.6756756756756757,\n",
      "                          'recallByLabel': 0.5932203389830508,\n",
      "                          'truePositiveRateByLabel': 0.5932203389830508,\n",
      "                          'weightedFMeasure': 0.42540806139901605,\n",
      "                          'weightedFalsePositiveRate': 0.14135957622336306,\n",
      "                          'weightedPrecision': 0.4441995770480976,\n",
      "                          'weightedRecall': 0.4316298342541436,\n",
      "                          'weightedTruePositiveRate': 0.4316298342541436}}\n"
     ]
    }
   ],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Decision Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/classification/decision_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/classification/decision_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\"Saving Decision Tree model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/classification/decision_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Random Forest model NOT found in disk, training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_classification_random_forest_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Classification Random Forest model NOT found in disk, training...\")\n",
    "    \n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = RandomForestClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        # .addGrid(estimator.maxDepth, [5, 24])\n",
    "        # .addGrid(estimator.maxBins, [32, 64])\n",
    "        # .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        # .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        # .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        # .addGrid(estimator.numTrees, [20, 40])\n",
    "        # .addGrid(estimator.featureSubsetStrategy, [\"auto\", \"onethird\", \"all\", \"log2\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification Random Forest model found in disk, loading...\")\n",
    "    model = RandomForestClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Random Forest\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Random Forest model trained in previous cell...\n",
      "Summary available, retrieving data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_set_evaluation': {'accuracy': 0.2897727272727273,\n",
      "                         'f1': 0.28261428653354287,\n",
      "                         'fMeasureByLabel': 0.4444444444444444,\n",
      "                         'falsePositiveRateByLabel': 0.1111111111111111,\n",
      "                         'precisionByLabel': 0.5161290322580645,\n",
      "                         'recallByLabel': 0.3902439024390244,\n",
      "                         'truePositiveRateByLabel': 0.3902439024390244,\n",
      "                         'weightedFMeasure': 0.28261428653354287,\n",
      "                         'weightedFalsePositiveRate': 0.16839470325282455,\n",
      "                         'weightedPrecision': 0.30845178310143934,\n",
      "                         'weightedRecall': 0.2897727272727273,\n",
      "                         'weightedTruePositiveRate': 0.2897727272727273},\n",
      " 'train_set_evaluation': {'accuracy': 0.4903314917127072,\n",
      "                          'fMeasureByLabel': [0.6677685950413222,\n",
      "                                              0.4346456692913386,\n",
      "                                              0.388646288209607,\n",
      "                                              0.47403462050599204,\n",
      "                                              0.46085011185682323],\n",
      "                          'falsePositiveRateByLabel': [0.09366869037294015,\n",
      "                                                       0.16652059596844873,\n",
      "                                                       0.06660899653979238,\n",
      "                                                       0.23380035026269702,\n",
      "                                                       0.08],\n",
      "                          'precisionByLabel': [0.6516129032258065,\n",
      "                                               0.42073170731707316,\n",
      "                                               0.536144578313253,\n",
      "                                               0.4,\n",
      "                                               0.5175879396984925],\n",
      "                          'recallByLabel': [0.6847457627118644,\n",
      "                                            0.4495114006514658,\n",
      "                                            0.3047945205479452,\n",
      "                                            0.5816993464052288,\n",
      "                                            0.4153225806451613],\n",
      "                          'truePositiveRateByLabel': [0.6847457627118644,\n",
      "                                                      0.4495114006514658,\n",
      "                                                      0.3047945205479452,\n",
      "                                                      0.5816993464052288,\n",
      "                                                      0.4153225806451613],\n",
      "                          'weightedFMeasure': 0.4856754791313273,\n",
      "                          'weightedFalsePositiveRate': 0.13093012485658548,\n",
      "                          'weightedPrecision': 0.5032503221758635,\n",
      "                          'weightedRecall': 0.4903314917127072,\n",
      "                          'weightedTruePositiveRate': 0.4903314917127072}}\n"
     ]
    }
   ],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Random Forest model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/classification/random_forest.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/classification/random_forest.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Random Forest Classification model on disk...\n"
     ]
    }
   ],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Saving Random Forest Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/classification/random_forest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 3: clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import DenseVector, Vectors, VectorUDT\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(\n",
    "    dataset,\n",
    "    n_clusters,\n",
    "    distance_measure=\"euclidean\",\n",
    "    max_iter=20,\n",
    "    features_col=\"features\",\n",
    "    prediction_col=\"cluster\",\n",
    "):\n",
    "\n",
    "    print(\n",
    "        f\"\"\"Training K-means clustering using the following parameters: \n",
    "        - K (n. of clusters) = {n_clusters}\n",
    "        - max_iter (max n. of iterations) = {max_iter}\n",
    "        - distance measure = {distance_measure}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # if distance_measure == \"cosine\":\n",
    "\n",
    "        # NOTE we already start from normalized/std data, so, NO need to do it\n",
    "\n",
    "        # Normalize inputs to unit-length vectors\n",
    "        # dataset = Normalizer(\n",
    "        #     inputCol=features_col, outputCol=features_col + \"_norm\", p=1\n",
    "        # ).transform(dataset)\n",
    "\n",
    "        # features_col = features_col + \"_norm\"\n",
    "\n",
    "    # Train a K-means model\n",
    "    kmeans = KMeans(\n",
    "        featuresCol=features_col,\n",
    "        predictionCol=prediction_col,\n",
    "        k=n_clusters,\n",
    "        initMode=\"k-means||\",\n",
    "        initSteps=5,\n",
    "        tol=0.000001,\n",
    "        maxIter=max_iter,\n",
    "        distanceMeasure=distance_measure,\n",
    "    )\n",
    "\n",
    "    model = kmeans.fit(dataset)\n",
    "    # here there are all the relevant clustering information\n",
    "\n",
    "    # Make clusters\n",
    "    clusters_df = model.transform(dataset)\n",
    "\n",
    "    return model, clusters_df\n",
    "\n",
    "\n",
    "def evaluate_k_means(\n",
    "    clusters,\n",
    "    metric_name=\"silhouette\",\n",
    "    distance_measure=\"squaredEuclidean\",  # cosine\n",
    "    prediction_col=\"cluster\",\n",
    "    featuresCol = \"features\"\n",
    "):\n",
    "\n",
    "    # Evaluate clustering by computing Silhouette score\n",
    "    evaluator = ClusteringEvaluator(\n",
    "        metricName=metric_name,\n",
    "        distanceMeasure=distance_measure,\n",
    "        predictionCol=prediction_col,\n",
    "        featuresCol=featuresCol\n",
    "    )\n",
    "\n",
    "    return evaluator.evaluate(clusters)\n",
    "\n",
    "\n",
    "def do_clustering(\n",
    "    k_range, \n",
    "    input_df, \n",
    "    max_iter,\n",
    "    featuresCol,\n",
    "    clusterCol\n",
    "):\n",
    "    clustering_results = {}\n",
    "\n",
    "    clusters_df = input_df\n",
    "\n",
    "    # for k in tqdm( range(5, max_k_clusters + 1, 5), desc = \"Performing clustering\" ):\n",
    "    # for k in tqdm(range(2, max_k_clusters + 1, 4), desc=\"Performing clustering\"):\n",
    "    for k in k_range:\n",
    "        print(f\"Running K-means using K = {k}\")\n",
    "\n",
    "        # model, clusters_df = k_means(tf_idf_df, k, max_iter=50, distance_measure=\"cosine\") # Alternatively, distance_measure=\"euclidean\"\n",
    "        # model, clusters_df = k_means(input_df, k, max_iter=max_iter, distance_measure=\"cosine\") # Alternatively, distance_measure=\"euclidean\"\n",
    "        model, clusters_df = k_means(\n",
    "            # input_df,\n",
    "            clusters_df, \n",
    "            k, \n",
    "            max_iter=max_iter, \n",
    "            distance_measure=\"cosine\", \n",
    "            features_col=featuresCol,\n",
    "            prediction_col=clusterCol + \"_k_\" + str(k),\n",
    "        )  # Alternatively, distance_measure=\"euclidean\"\n",
    "        # silhouette_k = evaluate_k_means(clusters_df, distance_measure=\"cosine\") # Alternatively, distance_measure=\"squaredEuclidean\"\n",
    "        silhouette_k = evaluate_k_means(\n",
    "            clusters_df, \n",
    "            distance_measure=\"cosine\",\n",
    "            prediction_col=clusterCol + \"_k_\" + str(k),\n",
    "            featuresCol=featuresCol\n",
    "\n",
    "        )  # Alternatively, distance_measure=\"squaredEuclidean\"\n",
    "        wssd_k = model.summary.trainingCost\n",
    "        # wssd_k = model.summary\n",
    "\n",
    "        print(\n",
    "            \"Silhouette coefficient computed with cosine distance: {:.3f}\".format(\n",
    "                silhouette_k\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"Within-cluster Sum of Squared Distances (using cosine distance): {:.3f}\".format(\n",
    "                wssd_k\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"--------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "\n",
    "        l = list(\n",
    "            enumerate(\n",
    "                model.clusterCenters()\n",
    "            )\n",
    "        )\n",
    "        l = [(ind, DenseVector(c)) for ind, c in l]\n",
    "        # print(l)\n",
    "        schema = [\"cluster_id\"  + \"_k_\" + str(k), \"centroid\"  + \"_k_\" + str(k)]\n",
    "\n",
    "        schema = StructType([ \n",
    "            StructField(\"cluster_id\" + \"_k_\" + str(k),IntegerType(),True), \n",
    "            StructField(\"centroid\" + \"_k_\" + str(k),VectorUDT(),True), \n",
    "        ])\n",
    "        centr_df = spark.createDataFrame(data=l, schema=schema)\n",
    "\n",
    "        # centr_df.show()\n",
    "\n",
    "        # df_with_centroids = clusters_df.join(\n",
    "        #     centr_df, on=[\"cluster_id\"]\n",
    "        # )\n",
    "\n",
    "        clusters_df = clusters_df.join(\n",
    "            centr_df, on=[\"cluster_id\" + \"_k_\" + str(k)]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        clustering_results[str(k)] = {\n",
    "            \"silhouette_k\"      : silhouette_k,\n",
    "            \"wssd_k\"            : wssd_k,\n",
    "            \"model\"             : model,\n",
    "            \"df\"                : clusters_df,\n",
    "            # \"cluster_centroids\" : model.clusterCenters(),\n",
    "            # \"centr_df\" : centr_df,\n",
    "            # \"df_with_centroids\" : df_with_centroids\n",
    "        }\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Free up memory space at the end of each iteration\n",
    "        # del model\n",
    "        # del clusters_df\n",
    "        # gc.collect() # garbage collector\n",
    "    \n",
    "    clustering_results[\"df_with_centroid_full\"] = clusters_df\n",
    "\n",
    "    return clustering_results\n",
    "\n",
    "\n",
    "def plot_clustering_results(clustering_results, k_range):\n",
    "\n",
    "    # print(clustering_results)\n",
    "\n",
    "    # k_col = list(clustering_results.keys())[:-1]\n",
    "    k_col = [str(x) for x in k_range]\n",
    "    wssd_col = [\n",
    "        clustering_results[k][\"wssd_k\"] for k in k_col \n",
    "    ]\n",
    "    silhouette_col = [\n",
    "        clustering_results[k][\"silhouette_k\"] for k in k_col \n",
    "    ]\n",
    "\n",
    "    # print(f\"k_col: {k_col}\")\n",
    "    # print(wssd_col)\n",
    "    # print(silhouette_col)\n",
    "\n",
    "    plot_df_temp = pd.DataFrame([k_col, wssd_col, silhouette_col]).transpose()\n",
    "    plot_df_temp.columns = [\"K\", \"WSSD\", \"Silhouette\"]\n",
    "    # print(plot_df_temp)\n",
    "\n",
    "    # Create a 1x1 figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    _ = sns.pointplot(\n",
    "        data=plot_df_temp, x=\"K\", y=\"WSSD\", ax=ax, color=\"orangered\"\n",
    "    )\n",
    "    _ = ax.set_xlabel(\"K\")\n",
    "    _ = ax.set_ylabel(\"WSSD\")\n",
    "    \n",
    "    # Create a 1x1 figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    _ = sns.pointplot(\n",
    "        data=plot_df_temp, x=\"K\", y=\"Silhouette\", ax=ax, color=\"orangered\"\n",
    "    )\n",
    "    _ = ax.set_xlabel(\"K\")\n",
    "    _ = ax.set_ylabel(\"Silhouette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "CLUSTERING_FEATURES = copy.deepcopy(PLAYER_FEATURES)\n",
    "CLUSTERING_FEATURES.remove(\"value\")\n",
    "CLUSTERING_FEATURES.remove(\"overall\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=CLUSTERING_FEATURES,\n",
    "    outputCol=\"clustering_features\"\n",
    ")\n",
    "\n",
    "pre_processed_df = assembler.transform(pre_processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"clustering_features\",\n",
    "    outputCol=\"clustering_features_min_max\"    \n",
    ")\n",
    "\n",
    "pre_processed_df = scaler.fit(pre_processed_df).transform(pre_processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\n",
    "    s: pre_processed_df.filter(col(\"season\") == s) for s in seasons\n",
    "}\n",
    "\n",
    "MAX_K_CLUSTERS = 8\n",
    "MAX_ITER = 20\n",
    "\n",
    "k_range = [2, 4]\n",
    "# k_range = range(2, MAX_K_CLUSTERS, 4)\n",
    "K_RANGE = [str(k) for k in k_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_results_dict = dict()\n",
    "\n",
    "for s in seasons:\n",
    "    clustering_results_dict[s] = do_clustering(\n",
    "        k_range=k_range, \n",
    "        input_df=df_dict[s], \n",
    "        max_iter=2,\n",
    "        # max_iter=MAX_ITER,\n",
    "        featuresCol=\"clustering_features_min_max\",\n",
    "        clusterCol=\"cluster_id\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_df = reduce(\n",
    "    DataFrame.unionAll, \n",
    "    [\n",
    "        clustering_results_dict[s][\"df_with_centroid_full\"] for s in seasons\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutating single seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in seasons:\n",
    "    plot_clustering_results(clustering_results_dict[s], k_range=K_RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating seasons all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_results_dict[\"avg\"] = dict.fromkeys(K_RANGE)\n",
    "\n",
    "for k in clustering_results_dict[\"avg\"].keys():\n",
    "    clustering_results_dict[\"avg\"][k] = {\n",
    "        \"wssd_k\": 0,\n",
    "        \"silhouette_k\": 0\n",
    "    }\n",
    "\n",
    "sum_wssd = dict.fromkeys(K_RANGE, 0)\n",
    "sum_silhouette = dict.fromkeys(K_RANGE, 0)\n",
    "\n",
    "for s in seasons:\n",
    "    for k in K_RANGE:\n",
    "        sum_wssd[k] += clustering_results_dict[s][k][\"wssd_k\"]\n",
    "        sum_silhouette[k] += clustering_results_dict[s][k][\"silhouette_k\"]\n",
    "    \n",
    "avg_wssd = dict()\n",
    "avg_silhouette = dict()\n",
    "\n",
    "for k in K_RANGE:\n",
    "    avg_wssd[k] = sum_wssd[k] / len(k_range)\n",
    "    avg_silhouette[k] = sum_silhouette[k] / len(k_range)\n",
    "\n",
    "    clustering_results_dict[\"avg\"][k][\"wssd_k\"] = avg_wssd[k]\n",
    "    clustering_results_dict[\"avg\"][k][\"silhouette_k\"] = avg_silhouette[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_results(\n",
    "    clustering_results_dict[\"avg\"], k_range=K_RANGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute distance between player and centroid of its cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_distance_from_centroid_UDF = udf(\n",
    "    lambda player, centroid: float(\n",
    "        Vectors.squared_distance(\n",
    "            player, centroid\n",
    "        )\n",
    "    ), FloatType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K_RANGE:\n",
    "    pre_processed_df = pre_processed_df.withColumn(\n",
    "        \"distance_from_centroid\" + \"_k_\" + k,\n",
    "        compute_distance_from_centroid_UDF(\n",
    "            col(\"clustering_features_min_max\"),\n",
    "            col(\"centroid\" + \"_k_\" + k)\n",
    "        )\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From players to teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = pre_processed_df.groupBy(\n",
    "    [\"season\", \"club_name\", \"macro_role\"]\n",
    ").agg(\n",
    "    {\n",
    "        \"distance_from_centroid\" + \"_k_\" + str(k): \"avg\" for k in K_RANGE \n",
    "    }\n",
    ")\n",
    "\n",
    "for k in K_RANGE:\n",
    "    teams_df = teams_df.withColumnRenamed(\n",
    "        \"avg(distance_from_centroid\" + \"_k_\" + str(k) + \")\",\n",
    "        \"avg_distance_from_centroid\" + \"_k_\" + str(k)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subquery(macro_role, k):\n",
    "    return f\"\"\"(\n",
    "        case\n",
    "            when macro_role='{macro_role}' then avg_distance_from_centroid_k_{k} \n",
    "        else NULL\n",
    "        end\n",
    "    ) as avg_dist_macro_role_{int(macro_role)}_k_{k}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df.createOrReplaceTempView(\"t\")\n",
    "\n",
    "temp = dict()\n",
    "\n",
    "for k in K_RANGE:\n",
    "    temp[k] = (\n",
    "        spark.sql(\n",
    "            f\"\"\"\n",
    "                select season, club_name, {generate_subquery(0.0, k)}, {generate_subquery(1.0, k)}, {generate_subquery(2.0, k)}, {generate_subquery(3.0, k)}, {generate_subquery(4.0, k)}, {generate_subquery(5.0, k)}, {generate_subquery(6.0, k)}, {generate_subquery(7.0, k)}\n",
    "                from t\n",
    "            \"\"\"\n",
    "        )\n",
    "        .groupBy(\"season\", \"club_name\")\n",
    "        .agg(\n",
    "            # TODO use for loop as in second cell of \"from players to teams\"\n",
    "            sum(f\"avg_dist_macro_role_0_k_{k}\").alias(f\"avg_dist_macro_role_0_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_1_k_{k}\").alias(f\"avg_dist_macro_role_1_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_2_k_{k}\").alias(f\"avg_dist_macro_role_2_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_3_k_{k}\").alias(f\"avg_dist_macro_role_3_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_4_k_{k}\").alias(f\"avg_dist_macro_role_4_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_5_k_{k}\").alias(f\"avg_dist_macro_role_5_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_6_k_{k}\").alias(f\"avg_dist_macro_role_6_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_7_k_{k}\").alias(f\"avg_dist_macro_role_7_k_{k}\"),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE executing this cell n times, w/o restoring teams_df --> \n",
    "# n copies of avg_dist_macro_role_[0:7]_k_[2, 6]\n",
    "\n",
    "teams_df = temp[K_RANGE[0]]\n",
    "\n",
    "for i in range(1, len(K_RANGE)):\n",
    "    teams_df = teams_df.join(\n",
    "        temp[K_RANGE[i]],\n",
    "        on=[\"season\", \"club_name\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_distances_dict = dict()\n",
    "avg_distances_vec_dict = dict()\n",
    "\n",
    "for k in K_RANGE:\n",
    "    \n",
    "    avg_distances_dict[k] = [\n",
    "        f\"avg_dist_macro_role_{i}_k_{k}\" for i in range(0, NUM_MACRO_ROLES)\n",
    "    ]\n",
    "\n",
    "    avg_distances_vec_dict[k] = f\"avg_dist_vec_k_{k}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "import itertools\n",
    "\n",
    "global_max = teams_df.select(\n",
    "    greatest(\n",
    "        *list(\n",
    "            itertools.chain.from_iterable(\n",
    "                avg_distances_dict.values()\n",
    "            )\n",
    "        )\n",
    "    ).alias(\"row_wise_max\")\n",
    ").collect()\n",
    "\n",
    "global_max = [row[\"row_wise_max\"] for row in global_max]\n",
    "\n",
    "global_max = builtins.max(global_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = teams_df.fillna(global_max * 1.5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K_RANGE:\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=avg_distances_dict[k], outputCol=avg_distances_vec_dict[k]\n",
    "    )\n",
    "\n",
    "    teams_df = assembler.transform(teams_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(\n",
    "    teams_df,\n",
    "    on=[\"club_name\", \"season\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit(\n",
    "    [0.9, 0.1]\n",
    "    # NOTE reactive 90/10 split, keep 70/30 just when using one league\n",
    "    # [0.7, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS_CV = 4\n",
    "FEATURES_COL = [f\"avg_dist_vec_k_{k}\" for k in K_RANGE]\n",
    "LABEL_COL = \"points\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = [\"r2\", \"mse\"]\n",
    "evaluation_metrics_cv = [\"r2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"lr_attempt3_predictions\"\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    # .addGrid(lr_raw.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "    # .addGrid(lr_raw.solver, [\"auto\", \"normal\", \"l-bfgs\"])\n",
    "    # .addGrid(lr_raw.standardization, [True, False])\n",
    "    # .addGrid(lr_raw.loss, [\"squaredError\", \"huber\"])\n",
    "    # huber loss does NOT support ElasticNet Regularization :(\n",
    "    # .addGrid(lr_raw.fitIntercept, [True, False])\n",
    "    # .addGrid(lr_raw.elasticNetParam, [0.0, 0.25, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"dtr_attempt3_predictions\"\n",
    "\n",
    "estimator = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"rfr_attempt3_predictions\"\n",
    "\n",
    "estimator = RandomForestRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"gbtr_attempt3_predictions\"\n",
    "\n",
    "estimator = GBTRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = [\"accuracy\", \"f1\"]\n",
    "evaluation_metrics_cv = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "FEATURES_COL = [f\"avg_dist_vec_k_{k}\" for k in K_RANGE]\n",
    "LABEL_COLS = [\"macro_place_naive\", \"macro_place_complex\"]\n",
    "PREDICTION_COL = \"svmc_attempt3_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LinearSVC(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "# for label_col in LABEL_COLS:\n",
    "#     cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "#         estimator=estimator,\n",
    "#         param_grid=param_grid,\n",
    "#         feature_vec=[FEATURES_COL],\n",
    "#         label_col=LABEL_COL,\n",
    "#         prediction_col=PREDICTION_COL,\n",
    "#         evaluation_metrics = evaluation_metrics,\n",
    "#         evaluation_metrics_cv=evaluation_metrics_cv\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models[label_col], \n",
    "        evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "PREDICTION_COL = \"lrc_attempt3_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LogisticRegression(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "# for label_col in LABEL_COLS:\n",
    "#     print(f\"Learning with label {label_col}\")\n",
    "#     cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "#         estimator=estimator,\n",
    "#         param_grid=param_grid,\n",
    "#         feature_vec=[FEATURES_COL],\n",
    "#         label_col=label_col,\n",
    "#         prediction_col=PREDICTION_COL,\n",
    "#         evaluation_metrics=evaluation_metrics,\n",
    "#         evaluation_metrics_cv=evaluation_metrics_cv\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Evaluating with label {label_col}\")\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"dtc_attempt3_predictions\"\n",
    "\n",
    "estimator = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Evaluating with label {label_col}\")\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "PREDICTION_COL = \"rfc_attempt3_predictions\"\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics = [\n",
    "            \"accuracy\",\n",
    "            \"falsePositiveRateByLabel\",\n",
    "            \"precisionByLabel\",\n",
    "            \"recallByLabel\",\n",
    "            \"truePositiveRateByLabel\",\n",
    "            \"weightedFalsePositiveRate\",\n",
    "            \"weightedTruePositiveRate\",\n",
    "            \"weightedPrecision\",\n",
    "            \"weightedRecall\",\n",
    "        ],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Evaluating with label {label_col}\")\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "PREDICTION_COL = \"mlpc_attempt3_predictions\"\n",
    "\n",
    "estimator = MultilayerPerceptronClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .addGrid(estimator.layers, [[NUM_MACRO_ROLES, NUM_MACRO_PLACES]])\n",
    "    .addGrid(estimator.solver, [\"gd\"])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in [\"macro_place_naive\"]:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=[\n",
    "            \"accuracy\",\n",
    "            \"falsePositiveRateByLabel\",\n",
    "            \"precisionByLabel\",\n",
    "            \"recallByLabel\",\n",
    "            \"truePositiveRateByLabel\",\n",
    "            \"weightedFalsePositiveRate\",\n",
    "            \"weightedTruePositiveRate\",\n",
    "            \"weightedPrecision\",\n",
    "            \"weightedRecall\",\n",
    "        ],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in [\"macro_place_naive\"]:\n",
    "    print(f\"Evaluating with label {label_col}\")\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering-learning cross evaluation\n",
    "\n",
    "Classic left-right plot, with:\n",
    "Left Y --> elbow result\n",
    "Right Y --> accuracy\n",
    "X axis --> # clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 4: thinking \"Deep\", shallow injecting some priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PLACE = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the prior (RP coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df.select(\"season\", \"club_name\", \"place\").createOrReplaceTempView(\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = spark.sql(\n",
    "    f\"\"\"\n",
    "    select t.season, t.club_name,\n",
    "        avg(\n",
    "            (\n",
    "                select sub.place\n",
    "                where sub.season < t.season and sub.club_name == t.club_name\n",
    "            )\n",
    "        ) as rp_coeff\n",
    "    from t, t as sub\n",
    "    group by t.season, t.club_name\n",
    "    order by t.season desc\n",
    "    \"\"\"\n",
    ").fillna(MAX_PLACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = rp_df.withColumn(\"rp_coeff\", MAX_PLACE - col(\"rp_coeff\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_RP_TRADEOFF = [0.5, 1, 1.5]\n",
    "\n",
    "add_normalize_by_rp_UDF = udf(\n",
    "    lambda points, rp, tradeoff: points + tradeoff * rp, DoubleType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = rp_df.join(\n",
    "    df, on=[\"club_name\", \"season\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RP_NORMALIZED_COLS = [\n",
    "    f\"avg(overall)_rp_normalized_tradeoff_{tradeoff}\".replace(\n",
    "        \".\", \"-\"\n",
    "    ) for tradeoff in ADD_RP_TRADEOFF\n",
    "]\n",
    "\n",
    "for tradeoff, col_name in zip(ADD_RP_TRADEOFF, RP_NORMALIZED_COLS):\n",
    "    rp_df = rp_df.withColumn(\n",
    "        col_name, add_normalize_by_rp_UDF(\n",
    "            col(\"avg(overall)\"), col(\"rp_coeff\"), lit(tradeoff)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[col_name], \n",
    "        outputCol=col_name + \"_vec\"\n",
    "    )\n",
    "\n",
    "    rp_df = assembler.transform(rp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = rp_df.randomSplit(\n",
    "    # [0.9, 0.1]\n",
    "    [0.4, 0.6]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS_CV = 4\n",
    "# TODO restore all values\n",
    "FEATURES_COL = [RP_NORMALIZED_COLS[0] + \"_vec\"]\n",
    "# FEATURES_COL = [\n",
    "#     rp_normalized_col + \"_vec\" for rp_normalized_col in RP_NORMALIZED_COLS \n",
    "# ]\n",
    "LABEL_COL = \"points\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = [\"r2\", \"mse\"]\n",
    "evaluation_metrics_cv = [\"r2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"lr_attempt4_predictions\"\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    # .addGrid(lr_raw.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "    # .addGrid(lr_raw.solver, [\"auto\", \"normal\", \"l-bfgs\"])\n",
    "    # .addGrid(lr_raw.standardization, [True, False])\n",
    "    # .addGrid(lr_raw.loss, [\"squaredError\", \"huber\"])\n",
    "    # huber loss does NOT support ElasticNet Regularization :(\n",
    "    # .addGrid(lr_raw.fitIntercept, [True, False])\n",
    "    # .addGrid(lr_raw.elasticNetParam, [0.0, 0.25, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"dtr_attempt4_predictions\"\n",
    "\n",
    "estimator = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"rfr_attempt4_predictions\"\n",
    "\n",
    "estimator = RandomForestRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"gbtr_attempt4_predictions\"\n",
    "\n",
    "estimator = GBTRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = [\"accuracy\", \"f1\"]\n",
    "evaluation_metrics_cv = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO restore all values\n",
    "# LABEL_COLS = [\"macro_place_naive\", \"macro_place_complex\"]\n",
    "LABEL_COLS = [\"macro_place_naive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "PREDICTION_COL = \"svmc_attempt4_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LinearSVC(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "# for label_col in LABEL_COLS:\n",
    "#     cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "#         estimator=estimator,\n",
    "#         param_grid=param_grid,\n",
    "#         feature_vec=[FEATURES_COL],\n",
    "#         label_col=LABEL_COL,\n",
    "#         prediction_col=PREDICTION_COL,\n",
    "#         evaluation_metrics = evaluation_metrics,\n",
    "#         evaluation_metrics_cv=evaluation_metrics_cv\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "PREDICTION_COL = \"lrc_attempt4_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LogisticRegression(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "# for label_col in LABEL_COLS:\n",
    "#     print(f\"Learning with label {label_col}\")\n",
    "#     cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "#         estimator=estimator,\n",
    "#         param_grid=param_grid,\n",
    "#         feature_vec=[FEATURES_COL],\n",
    "#         label_col=label_col,\n",
    "#         prediction_col=PREDICTION_COL,\n",
    "#         evaluation_metrics=evaluation_metrics,\n",
    "#         evaluation_metrics_cv=evaluation_metrics_cv\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"dtc_attempt3_predictions\"\n",
    "\n",
    "estimator = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "PREDICTION_COL = \"rfc_attempt3_predictions\"\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics = [\n",
    "            \"accuracy\",\n",
    "            \"falsePositiveRateByLabel\",\n",
    "            \"precisionByLabel\",\n",
    "            \"recallByLabel\",\n",
    "            \"truePositiveRateByLabel\",\n",
    "            \"weightedFalsePositiveRate\",\n",
    "            \"weightedTruePositiveRate\",\n",
    "            \"weightedPrecision\",\n",
    "            \"weightedRecall\",\n",
    "        ],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "PREDICTION_COL = \"mlpc_attempt4_predictions\"\n",
    "\n",
    "estimator = MultilayerPerceptronClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .addGrid(estimator.layers, [[NUM_MACRO_ROLES, NUM_MACRO_PLACES]])\n",
    "    .addGrid(estimator.solver, [\"gd\"])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in [\"macro_place_naive\"]:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=[\n",
    "            \"accuracy\",\n",
    "            \"falsePositiveRateByLabel\",\n",
    "            \"precisionByLabel\",\n",
    "            \"recallByLabel\",\n",
    "            \"truePositiveRateByLabel\",\n",
    "            \"weightedFalsePositiveRate\",\n",
    "            \"weightedTruePositiveRate\",\n",
    "            \"weightedPrecision\",\n",
    "            \"weightedRecall\",\n",
    "        ],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RP impact\n",
    "\n",
    "plot showing that the more the weight of the RP coefficient is increased, the more the accuracy ofc goes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
