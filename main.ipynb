{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wall of imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# %matplotlib inline\n",
    "%matplotlib widget\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "RANDOM_SEED = None\n",
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import json\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from pyspark_dist_explore import hist\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "from pyspark.ml.feature import UnivariateFeatureSelector\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import RFE\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "import warnings # supress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/15 09:53:02 WARN Utils: Your hostname, RTX-2070-Rig resolves to a loopback address: 127.0.1.1; using 192.168.1.189 instead (on interface wlp7s0)\n",
      "22/06/15 09:53:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/15 09:53:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.ui.port\", \"4050\")\n",
    "    .set(\"spark.executor.memory\", \"4G\")\n",
    "    .set(\"spark.driver.memory\", \"20G\")\n",
    "    .set(\"spark.driver.maxResultSize\", \"10G\")\n",
    ")\n",
    "# .set(\"spark.master\", \"spark://192.168.1.189:4050\")\n",
    "\n",
    "\n",
    "# create the context\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading football players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "modern_df = spark.read.csv(\n",
    "    \"data/players_*.csv\", sep=\",\", inferSchema=True, header=True, multiLine=True\n",
    ")\n",
    "\n",
    "legacy_df = spark.read.csv(\n",
    "    \"data/scraped_players_*.csv\", sep=\",\", inferSchema=True, header=True, \n",
    "    multiLine=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing football players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "leagues = [\n",
    "    \"Spain Primera Division\",\n",
    "    # \"German 1. Bundesliga\",\n",
    "    # \"French Ligue 1\",\n",
    "    # \"English Premier League\",\n",
    "    # \"Italian Serie A\",\n",
    "    # \"Holland Eredivisie\",\n",
    "]\n",
    "\n",
    "# seasons_modern = [\"20\", \"19\", \"18\", \"17\", \"16\", \"15\", \"14\"] \n",
    "seasons_modern = [\"20\", \"19\"] \n",
    "# seasons_legacy = [\"13\", \"12\", \"11\", \"10\", \"09\", \"08\", \"07\"]\n",
    "seasons_legacy = [ ]\n",
    "seasons = seasons_legacy + seasons_modern\n",
    "\n",
    "# football_teams = [\n",
    "#     row[\"club_name\"] for row in modern_df.select(\n",
    "#         \"club_name\"\n",
    "#     ).distinct().collect()\n",
    "# ]\n",
    "\n",
    "macro_roles = [\"0.0\", \"1.0\", \"2.0\", \"3.0\", \"4.0\", \"5.0\", \"6.0\", \"7.0\"]\n",
    "\n",
    "roles_to_macro_roles_dict = {\n",
    "    \"GK\": \"0\",\n",
    "    \"LB\": \"1\",\n",
    "    \"RB\": \"1\",\n",
    "    \"RWB\": \"1\",\n",
    "    \"LWB\": \"1\",\n",
    "    \"CB\": \"2\",\n",
    "    \"CDM\": \"3\",\n",
    "    \"CM\": \"4\",\n",
    "    \"RM\": \"4\",\n",
    "    \"LM\": \"4\",\n",
    "    \"CAM\": \"5\",\n",
    "    \"RW\": \"6\",\n",
    "    \"LW\": \"6\",\n",
    "    \"ST\": \"7\",\n",
    "    \"LF\": \"7\",\n",
    "    \"RF\": \"7\",\n",
    "    \"CF\": \"7\",\n",
    "}\n",
    "NUM_MACRO_ROLES = 8\n",
    "\n",
    "columns = [\n",
    "    \"short_name\",\n",
    "    \"club_name\",\n",
    "    \"league_name\",\n",
    "    \"season\",\n",
    "    \"player_positions\",\n",
    "    \"macro_role\",\n",
    "    \"overall\",\n",
    "    \"value\",\n",
    "    \"pace\",\n",
    "    \"shooting\",\n",
    "    \"passing\",\n",
    "    \"dribbling\",\n",
    "    \"defending\",\n",
    "    \"physic\",\n",
    "    \"attacking_crossing\",\n",
    "    \"attacking_finishing\",\n",
    "    \"attacking_heading_accuracy\",\n",
    "    \"attacking_short_passing\",\n",
    "    \"skill_dribbling\",\n",
    "    \"skill_fk_accuracy\",\n",
    "    \"skill_long_passing\",\n",
    "    \"skill_ball_control\",\n",
    "    \"movement_acceleration\",\n",
    "    \"movement_sprint_speed\",\n",
    "    \"movement_reactions\",\n",
    "    \"power_shot_power\",\n",
    "    \"power_stamina\",\n",
    "    \"power_strength\",\n",
    "    \"power_long_shots\",\n",
    "    \"mentality_aggression\",\n",
    "    \"mentality_penalties\",\n",
    "    \"defending_standing_tackle\"\n",
    "]\n",
    "\n",
    "def get_season(url):\n",
    "    url_split = url.split(\"/\")\n",
    "\n",
    "    # -1 to scale FIFA years down, to have compatibility with all_tables_fixed\n",
    "    return str(\n",
    "        (int(url_split[-2 if url_split[-1] == \"\" else -1][0:2]) - 1)\n",
    "    ).zfill(2)\n",
    "\n",
    "\n",
    "get_season_UDF = udf(lambda url: get_season(url), StringType())\n",
    "\n",
    "roles_to_macro_role_UDF = udf(\n",
    "    lambda roles: float(roles_to_macro_roles_dict[roles.split(\",\")[0]]), StringType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_modern_df = modern_df.dropDuplicates([\"player_url\"])\n",
    "pre_processed_legacy_df = legacy_df.dropDuplicates([\"player_url\"])\n",
    "\n",
    "pre_processed_modern_df = pre_processed_modern_df.na.fill(0)\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.na.fill(0)\n",
    "\n",
    "pre_processed_modern_df = pre_processed_modern_df.withColumn(\n",
    "    \"season\", get_season_UDF(col(\"player_url\"))\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumn(\n",
    "    \"season\", get_season_UDF(col(\"player_url\"))\n",
    ")\n",
    "\n",
    "pre_processed_modern_df = pre_processed_modern_df.withColumn(\n",
    "    \"macro_role\", roles_to_macro_role_UDF(col(\"player_positions\"))\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumn(\n",
    "    \"macro_role\", roles_to_macro_role_UDF(col(\"player_positions\"))\n",
    ")\n",
    "\n",
    "pre_processed_modern_df = pre_processed_modern_df.withColumnRenamed(\n",
    "    \"value_eur\", \"value\"\n",
    ")\n",
    "\n",
    "pre_processed_modern_df = pre_processed_modern_df.where(\n",
    "    (pre_processed_modern_df.league_name.isin(leagues))\n",
    "    &\n",
    "    (pre_processed_modern_df.season.isin(seasons_modern))\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.where(\n",
    "    (pre_processed_legacy_df.league_name.isin(leagues))\n",
    "    &\n",
    "    (pre_processed_legacy_df.season.isin(seasons_legacy))\n",
    ")\n",
    "\n",
    "pre_processed_modern_df = pre_processed_modern_df.select(columns)\n",
    "\n",
    "# TODO use a for loop\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"pas\", \"passing\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"dri\", \"dribbling\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.drop(col(\"defending\"))\n",
    "\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"def\", \"defending\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"phy\", \"physic\"\n",
    ")\n",
    "\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"sho\", \"shooting\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"pac\", \"pace\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"bov\", \"overall\"\n",
    ")\n",
    "\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.select(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- short_name: string (nullable = true)\n",
      " |-- club_name: string (nullable = true)\n",
      " |-- league_name: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- player_positions: string (nullable = true)\n",
      " |-- macro_role: string (nullable = true)\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      " |-- pace: integer (nullable = true)\n",
      " |-- shooting: integer (nullable = true)\n",
      " |-- passing: integer (nullable = true)\n",
      " |-- dribbling: integer (nullable = true)\n",
      " |-- defending: integer (nullable = true)\n",
      " |-- physic: integer (nullable = true)\n",
      " |-- attacking_crossing: integer (nullable = true)\n",
      " |-- attacking_finishing: integer (nullable = true)\n",
      " |-- attacking_heading_accuracy: integer (nullable = true)\n",
      " |-- attacking_short_passing: integer (nullable = true)\n",
      " |-- skill_dribbling: integer (nullable = true)\n",
      " |-- skill_fk_accuracy: integer (nullable = true)\n",
      " |-- skill_long_passing: integer (nullable = true)\n",
      " |-- skill_ball_control: integer (nullable = true)\n",
      " |-- movement_acceleration: integer (nullable = true)\n",
      " |-- movement_sprint_speed: integer (nullable = true)\n",
      " |-- movement_reactions: integer (nullable = true)\n",
      " |-- power_shot_power: integer (nullable = true)\n",
      " |-- power_stamina: integer (nullable = true)\n",
      " |-- power_strength: integer (nullable = true)\n",
      " |-- power_long_shots: integer (nullable = true)\n",
      " |-- mentality_aggression: integer (nullable = true)\n",
      " |-- mentality_penalties: integer (nullable = true)\n",
      " |-- defending_standing_tackle: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_processed_modern_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- short_name: string (nullable = true)\n",
      " |-- club_name: string (nullable = true)\n",
      " |-- league_name: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- player_positions: string (nullable = true)\n",
      " |-- macro_role: string (nullable = true)\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- pace: integer (nullable = true)\n",
      " |-- shooting: integer (nullable = true)\n",
      " |-- passing: integer (nullable = true)\n",
      " |-- dribbling: integer (nullable = true)\n",
      " |-- defending: integer (nullable = true)\n",
      " |-- physic: integer (nullable = true)\n",
      " |-- attacking_crossing: integer (nullable = true)\n",
      " |-- attacking_finishing: integer (nullable = true)\n",
      " |-- attacking_heading_accuracy: integer (nullable = true)\n",
      " |-- attacking_short_passing: integer (nullable = true)\n",
      " |-- skill_dribbling: integer (nullable = true)\n",
      " |-- skill_fk_accuracy: integer (nullable = true)\n",
      " |-- skill_long_passing: integer (nullable = true)\n",
      " |-- skill_ball_control: integer (nullable = true)\n",
      " |-- movement_acceleration: integer (nullable = true)\n",
      " |-- movement_sprint_speed: integer (nullable = true)\n",
      " |-- movement_reactions: integer (nullable = true)\n",
      " |-- power_shot_power: integer (nullable = true)\n",
      " |-- power_stamina: integer (nullable = true)\n",
      " |-- power_strength: integer (nullable = true)\n",
      " |-- power_long_shots: integer (nullable = true)\n",
      " |-- mentality_aggression: integer (nullable = true)\n",
      " |-- mentality_penalties: integer (nullable = true)\n",
      " |-- defending_standing_tackle: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_processed_legacy_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_df = pre_processed_modern_df.unionByName(\n",
    "    pre_processed_legacy_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- short_name: string (nullable = true)\n",
      " |-- club_name: string (nullable = true)\n",
      " |-- league_name: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- player_positions: string (nullable = true)\n",
      " |-- macro_role: string (nullable = true)\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- pace: integer (nullable = true)\n",
      " |-- shooting: integer (nullable = true)\n",
      " |-- passing: integer (nullable = true)\n",
      " |-- dribbling: integer (nullable = true)\n",
      " |-- defending: integer (nullable = true)\n",
      " |-- physic: integer (nullable = true)\n",
      " |-- attacking_crossing: integer (nullable = true)\n",
      " |-- attacking_finishing: integer (nullable = true)\n",
      " |-- attacking_heading_accuracy: integer (nullable = true)\n",
      " |-- attacking_short_passing: integer (nullable = true)\n",
      " |-- skill_dribbling: integer (nullable = true)\n",
      " |-- skill_fk_accuracy: integer (nullable = true)\n",
      " |-- skill_long_passing: integer (nullable = true)\n",
      " |-- skill_ball_control: integer (nullable = true)\n",
      " |-- movement_acceleration: integer (nullable = true)\n",
      " |-- movement_sprint_speed: integer (nullable = true)\n",
      " |-- movement_reactions: integer (nullable = true)\n",
      " |-- power_shot_power: integer (nullable = true)\n",
      " |-- power_stamina: integer (nullable = true)\n",
      " |-- power_strength: integer (nullable = true)\n",
      " |-- power_long_shots: integer (nullable = true)\n",
      " |-- mentality_aggression: integer (nullable = true)\n",
      " |-- mentality_penalties: integer (nullable = true)\n",
      " |-- defending_standing_tackle: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_processed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building football teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_teams_df = pre_processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYER_FEATURES = [\n",
    "    \"overall\",\n",
    "    \"value\",\n",
    "    \"pace\",\n",
    "    \"shooting\",\n",
    "    \"passing\",\n",
    "    \"dribbling\",\n",
    "    \"defending\",\n",
    "    \"physic\",\n",
    "    \"attacking_crossing\",\n",
    "    \"attacking_finishing\",\n",
    "    \"attacking_heading_accuracy\",\n",
    "    \"attacking_short_passing\",\n",
    "    \"skill_dribbling\",\n",
    "    \"skill_fk_accuracy\",\n",
    "    \"skill_long_passing\",\n",
    "    \"skill_ball_control\",\n",
    "    \"movement_acceleration\",\n",
    "    \"movement_sprint_speed\",\n",
    "    \"movement_reactions\",\n",
    "    \"power_shot_power\",\n",
    "    \"power_stamina\",\n",
    "    \"power_strength\",\n",
    "    \"power_long_shots\",\n",
    "    \"mentality_aggression\",\n",
    "    \"mentality_penalties\",\n",
    "    \"defending_standing_tackle\"\n",
    "]\n",
    "\n",
    "PLAYER_FEATURES_AVG = [\n",
    "    \"avg(\" + player_feature + \")\" for player_feature in PLAYER_FEATURES\n",
    "]\n",
    "\n",
    "TARGET_VARIABLE = \"points\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_teams_df = football_teams_df.select(\n",
    "    \"season\", \"club_name\", *PLAYER_FEATURES\n",
    ").groupBy(\n",
    "    [\"season\", \"club_name\"]\n",
    ").agg(\n",
    "    { player_feature: \"avg\" for player_feature in PLAYER_FEATURES }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading football teams seasonal scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_scores_df = (\n",
    "    spark.read.csv(\n",
    "        \"data/all_tables_fixed_renamed_leagues.csv\",\n",
    "        sep=\",\",\n",
    "        inferSchema=True,\n",
    "        header=True,\n",
    "        multiLine=True,\n",
    "    )\n",
    "    .withColumnRenamed(\"Year\", \"year\")\n",
    "    .withColumnRenamed(\"Team\", \"club_name_abbr\")\n",
    "    .withColumnRenamed(\"P\", \"points\")\n",
    "    .withColumnRenamed(\"Place\", \"place\")\n",
    "    .withColumnRenamed(\"League\", \"league\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing football teams seasonal scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_scores_columns = [\n",
    "    \"year\", \"club_name_abbr\", \"points\", \"place\", \"league\"\n",
    "]\n",
    "\n",
    "f = open(\"data/clubs_map.json\")\n",
    "club_name_abbr_to_ext = json.load(f)\n",
    "f.close()\n",
    "\n",
    "abbreviate_season_UDF = udf(\n",
    "    lambda season: str(season)[-2:],\n",
    "    StringType(),\n",
    ")\n",
    "\n",
    "def extend_club_name(club_name_abbr):\n",
    "    try:\n",
    "        return club_name_abbr_to_ext[club_name_abbr]\n",
    "    except KeyError as e:\n",
    "        return \"NOT_FOUND\"\n",
    "    except Exception as e:\n",
    "        return \"GENERAL_EXCEPTION\"\n",
    "\n",
    "extend_club_name_UDF = udf(\n",
    "    lambda club_name_abbr: extend_club_name(str(club_name_abbr)),\n",
    "    StringType(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_seasonal_scores_df = seasonal_scores_df\n",
    "\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.dropDuplicates(\n",
    "    seasonal_scores_columns\n",
    ")\n",
    "\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.na.fill(0)\n",
    "\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.select(\n",
    "    seasonal_scores_columns\n",
    ")\n",
    "\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"year\", abbreviate_season_UDF(col(\"year\"))\n",
    ")\n",
    "\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"club_name_ext\", extend_club_name_UDF(col(\"club_name_abbr\"))\n",
    ")\n",
    "\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"points\", pre_processed_seasonal_scores_df.points.cast(DoubleType())\n",
    ")\n",
    "\n",
    "if pre_processed_seasonal_scores_df.filter(\n",
    "    col(\"club_name_ext\") == \"NOT_FOUND\"\n",
    ").count() > 0:\n",
    "    print(\"WARN: some clubs have NOT been found\")\n",
    "    print(\"Please check your data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.where(\n",
    "    (pre_processed_seasonal_scores_df.year.isin(seasons))\n",
    "    & (pre_processed_seasonal_scores_df.league.isin(leagues))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_scores_pre_join_columns = [\n",
    "    \"year\", \"league\", \"club_name_ext\", \"points\", \"place\"\n",
    "]\n",
    "\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.select(\n",
    "    seasonal_scores_pre_join_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining football teams features with their seasonal scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = football_teams_df.join(\n",
    "    pre_processed_seasonal_scores_df,\n",
    "    (football_teams_df.season == pre_processed_seasonal_scores_df.year)\n",
    "    & (\n",
    "        football_teams_df.club_name\n",
    "        == pre_processed_seasonal_scores_df.club_name_ext\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_processed_seasonal_scores_df.select(\"club_name_ext\").distinct().subtract(\n",
    "    df.select(\"club_name_ext\").distinct()\n",
    ").count() > 0:\n",
    "    print(\"WARN: Some football teams have been left out the join (pre_processed_seasonal_scores_df)\")\n",
    "    print(\"Please, check your data!\")\n",
    "\n",
    "if football_teams_df.select(\"club_name\").distinct().subtract(\n",
    "    df.select(\"club_name_ext\").distinct()\n",
    ").count() > 0:\n",
    "    print(\"WARN: Some football teams have been left out the join (football_teams_df)\")\n",
    "    print(\"Please, check your data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1: \"naive\" player features\n",
    "\n",
    "Naive --> simply take the given features and NOT crafting them from other learning processes (i.e. NO clustering or stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FEATURES = PLAYER_FEATURES_AVG\n",
    "ALL_FEATURES.remove(\"avg(overall)\")\n",
    "ALL_FEATURES.remove(\"avg(value)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=ALL_FEATURES, outputCol=\"all_vec\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_target_relation(\n",
    "    data, x, y, n_rows = 12, n_cols = 2, figsize = (20, 40), color = \"#000000\"\n",
    "):\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "    for x_ind, x_value in enumerate(x):\n",
    "        ax = sns.regplot(\n",
    "            data=pdf,\n",
    "            x=x_value,\n",
    "            y=y,\n",
    "            color = color,\n",
    "            ax=axes[x_ind // n_cols, x_ind % n_cols],\n",
    "        )\n",
    "\n",
    "\n",
    "    fig.tight_layout(pad=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distribution(\n",
    "    data, features, figsize = (20, 40), \n",
    "    color = \"#000000\"\n",
    "):\n",
    "\n",
    "    n_cols = 2\n",
    "    n_rows = int(len(features) / n_cols) if len(features) >= n_cols else n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 30))\n",
    "\n",
    "    for feature_ind, feature in enumerate(features):\n",
    "        _ = sns.histplot(\n",
    "            data[feature],\n",
    "            kde=True,\n",
    "            color=color,\n",
    "            facecolor=color,\n",
    "            ax=axes[feature_ind // n_cols, feature_ind % n_cols],\n",
    "        )\n",
    "\n",
    "    fig.tight_layout(pad=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(\n",
    "    data, features, title = \"Pearson Correlation Matrix\", figsize = (16,12)\n",
    "):\n",
    "\n",
    "    mask = np.zeros_like(data[features].corr(), dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    with sns.axes_style(\"white\"):  # Temporarily set the background to white\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        plt.title(title, fontsize=24)\n",
    "\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "        _ = sns.heatmap(\n",
    "            data[features].corr(),\n",
    "            linewidths=0.25,\n",
    "            vmax=0.7,\n",
    "            square=True,\n",
    "            ax=ax,\n",
    "            cmap=cmap,\n",
    "            linecolor=\"w\",\n",
    "            annot=True,\n",
    "            annot_kws={\"size\": 8},\n",
    "            mask=mask,\n",
    "            cbar_kws={\"shrink\": 0.9},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[explain what does \"raw\" mean]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_RAW = \"#332FD0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES, TARGET_VARIABLE, figsize=(4,4), color=COLOR_RAW\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES, color = COLOR_RAW, figsize=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(pdf, ALL_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have some features with skewed distributions, we'll try to standardize, to see whether it helps with feature skewness or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(\n",
    "    inputCol=\"all_vec\", \n",
    "    outputCol=\"all_vec_std\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized_df = scaler.fit(standardized_df).transform(standardized_df)\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_STD = \"#9254C8\"\n",
    "\n",
    "ALL_FEATURES_STD = [\n",
    "    player_feature + \"_std\" for player_feature in ALL_FEATURES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = standardized_df.toPandas()\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO make a commodity function, because it will be used in other normalizations as well\n",
    "\n",
    "pdf = pdf.reindex(\n",
    "    columns=list(pdf.columns) + ALL_FEATURES_STD\n",
    ")\n",
    "\n",
    "pdf[ALL_FEATURES_STD] = pdf[\n",
    "    \"all_vec_std\"\n",
    "].transform(\n",
    "    {\n",
    "        ALL_FEATURES_STD[i]: itemgetter(i) for i, p in enumerate(ALL_FEATURES_STD)\n",
    "    }\n",
    "\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_STD, TARGET_VARIABLE, figsize=(4,4), color=COLOR_STD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf, ALL_FEATURES_STD, color = COLOR_STD, figsize=(10,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[state why pearson correlation matrix does NOT make sense to be plotted again]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FEATURES_LOG = [\n",
    "    player_feature + \"_log\" for player_feature in ALL_FEATURES\n",
    "]\n",
    "\n",
    "COLOR_LOG = \"#E15FED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_log_UDF = udf(\n",
    "    lambda value: float(np.log2(value)), DoubleType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, fl in zip(ALL_FEATURES, ALL_FEATURES_LOG):\n",
    "    # log_df = log_df.withColumn(fl, to_log_UDF(col(f)))\n",
    "    df = df.withColumn(fl, to_log_UDF(col(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = log_df.toPandas()\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_LOG, TARGET_VARIABLE, figsize=(4,4), color=COLOR_LOG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES_LOG, color = COLOR_LOG, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-max transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FEATURES_MIN_MAX = [\n",
    "    player_feature + \"_min_max\" for player_feature in ALL_FEATURES\n",
    "]\n",
    "\n",
    "COLOR_MIN_MAX = \"#6EDCD9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"all_vec\", \n",
    "    outputCol=\"all_vec_min_max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_df = scaler.fit(min_max_df).transform(min_max_df)\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = min_max_df.toPandas()\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO make a commodity function, because it will be used in other normalizations as well\n",
    "\n",
    "pdf = pdf.reindex(\n",
    "    columns=list(pdf.columns) + ALL_FEATURES_MIN_MAX\n",
    ")\n",
    "\n",
    "pdf[ALL_FEATURES_MIN_MAX] = pdf[\n",
    "    \"all_vec_min_max\"\n",
    "].transform(\n",
    "    {\n",
    "        ALL_FEATURES_MIN_MAX[i]: itemgetter(i) for i, p in enumerate(ALL_FEATURES_MIN_MAX)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_MIN_MAX, TARGET_VARIABLE, figsize=(4,4), color=COLOR_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES_MIN_MAX, color = COLOR_MIN_MAX, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO place learning on ## titles here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2: \"less is more\"\n",
    "\n",
    "Ok, considering all features gives trash results.\n",
    "\n",
    "What if we embrace the \"less is more idea\" and try to improve the results by means of using less features?\n",
    "\n",
    "Nevertheless, feature correlation is very high, so, intrinsicly, it already did NOT make much sense to consider them all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (on min-max normalized data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_df = min_max_df\n",
    "# pca_df = df\n",
    "\n",
    "PCA_NUM_COMPONENTS = 5\n",
    "PCA_NUM_COMPONENTS_TO_PLOT = 2\n",
    "\n",
    "pca = PCA(\n",
    "    k=PCA_NUM_COMPONENTS, \n",
    "    inputCol=\"all_vec_min_max\", \n",
    "    outputCol=\"all_vec_min_max_pcs\"\n",
    ")\n",
    "\n",
    "# pca_model = pca.fit(pca_df)\n",
    "# pca_df = pca_model.transform(pca_df)\n",
    "pca_model = pca.fit(df)\n",
    "df = pca_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "_ = sns.barplot(\n",
    "    x=[i for i in range(PCA_NUM_COMPONENTS_TO_PLOT)],\n",
    "    y=pca_model.explainedVariance.values[0:PCA_NUM_COMPONENTS_TO_PLOT],\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_xlabel(\"Eigenvalues\", labelpad=16, fontsize=16)\n",
    "_ = ax.set_ylabel(\"Proportion of Variance\", fontsize=16)\n",
    "_ = ax.set_xticklabels(\n",
    "    [f\"Principal Component {i}\" for i in range(PCA_NUM_COMPONENTS_TO_PLOT)], \n",
    "    rotation=0\n",
    ")\n",
    "_ = ax.set_title(\"Explained variance of each Principal Component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(data, x, y, c, x_label, y_label):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "    _ = plt.scatter(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        c = c,\n",
    "        edgecolor=\"none\",\n",
    "        alpha=1,\n",
    "        cmap=\"rainbow\",\n",
    "        axes=ax\n",
    "    )\n",
    "\n",
    "    _ = ax.set_xlabel(x_label, labelpad=20, fontsize=16)\n",
    "    _ = ax.set_ylabel(y_label, fontsize=16)\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_pdf = pca_df.toPandas()\n",
    "pca_pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    pca_pdf, \n",
    "    pca_pdf.all_vec_min_max_pcs.map(lambda x: x[0]),\n",
    "    pca_pdf.all_vec_min_max_pcs.map(lambda x: x[1]),\n",
    "    pca_pdf.points,\n",
    "    \"Principal Component 0\",\n",
    "    \"Principal Component 1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate whether points should be normlized to same scale as PCA values\n",
    "scatter_plot(\n",
    "    pca_pdf, \n",
    "    pca_pdf.all_vec_min_max_pcs.map(lambda x: x[0]),\n",
    "    pca_pdf.points,\n",
    "    pca_pdf.points,\n",
    "    \"Principal Component 0\",\n",
    "    \"Points\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate whether points should be normlized to same scale as PCA values\n",
    "scatter_plot(\n",
    "    pca_pdf, \n",
    "    pca_pdf.all_vec_min_max_pcs.map(lambda x: x[1]),\n",
    "    pca_pdf.points,\n",
    "    pca_pdf.points,\n",
    "    \"Principal Component 1\",\n",
    "    \"Points\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[bridge between this and Feature selection]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TNSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log scaling reduces skewedness of \"avg(mentality_penalties)\" feature BUT it increases the skewedness of all the other features.\n",
    "The other scalings (z-score and min-max) do NOT appear to be different than the \"raw\" data distribution.\n",
    "\n",
    "For this reason and due to the limited amount of resources available on Google Colab, we decided to stick with the min-max scaled data.\n",
    "In fact, the min-max scaling places \"for free\" all the features in the same scale, which is a very important consideration for SVM, which will be used in the upcoming sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection_df = min_max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = UnivariateFeatureSelector(\n",
    "    featuresCol=\"all_vec\",\n",
    "    labelCol=TARGET_VARIABLE, \n",
    "    selectionMode=\"percentile\"\n",
    ").setFeatureType(\"continuous\").setLabelType(\"categorical\").setSelectionThreshold(0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result = dict()\n",
    "\n",
    "# for thr in range(0.1, 1.1, 0.1):\n",
    "# for thr in np.linspace(1, 1, 1):\n",
    "for thr in [0.1]:\n",
    "\n",
    "    selector.setSelectionThreshold(thr)\n",
    "    selector.setOutputCol(\"ufs_\" + str(thr).replace(\".\", \"-\")),\n",
    "    fit_result[str(thr)] = selector.fit(df)\n",
    "    # feature_selection_df = fit_result[str(thr)].transform(\n",
    "    #     feature_selection_df\n",
    "    # )\n",
    "    df = fit_result[str(thr)].transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection_pdf = feature_selection_df.toPandas()\n",
    "feature_selection_pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    feature_selection_pdf,\n",
    "    feature_selection_pdf[\"ufs_0-1\"].map(lambda x: x[0]),\n",
    "    feature_selection_pdf[\"ufs_0-1\"].map(lambda x: x[1]),\n",
    "    feature_selection_pdf.points,\n",
    "    \"Feature 0\",\n",
    "    \"Feature 1\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    feature_selection_pdf,\n",
    "    feature_selection_pdf[\"ufs_0-1\"].map(lambda x: x[0]),\n",
    "    feature_selection_pdf.points,\n",
    "    feature_selection_pdf.points,\n",
    "    \"Feature 0\",\n",
    "    \"Points\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    feature_selection_pdf,\n",
    "    feature_selection_pdf[\"ufs_0-1\"].map(lambda x: x[1]),\n",
    "    feature_selection_pdf.points,\n",
    "    feature_selection_pdf.points,\n",
    "    \"Feature 1\",\n",
    "    \"Points\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = list(\n",
    "    map(\n",
    "        lambda i: ALL_FEATURES[i], fit_result[\"0.1\"].selectedFeatures\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, selected_features, TARGET_VARIABLE, figsize=(4,4), \n",
    "    color=COLOR_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at it from the original pearson matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state that these data distribs are trash.\n",
    "\n",
    "So, for this reason, talk about overall and value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall as feature (min-max normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features are so correlated and performances of attempt 1 are trash, why not considering just the overall as a feature?\n",
    "\n",
    "Maybe, we're lucky and the overall captures some other characteristics thay may steer the prediction a little bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERALL = [\"avg(overall)\"]\n",
    "\n",
    "COLOR_OVERALL_MIN_MAX = \"green\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=OVERALL, outputCol=\"overall_vec\"\n",
    ")\n",
    "\n",
    "# min_max_df = assembler.transform(min_max_df)\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERALL_MIN_MAX = [\"avg(overall)_min_max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"overall_vec\", \n",
    "    outputCol=\"overall_vec_min_max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_df = scaler.fit(min_max_df).transform(min_max_df)\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = min_max_df.toPandas()\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO make a commodity function, because it will be used in other normalizations as well\n",
    "\n",
    "pdf = pdf.reindex(\n",
    "    columns=list(pdf.columns) + OVERALL_MIN_MAX\n",
    ")\n",
    "\n",
    "pdf[OVERALL_MIN_MAX] = pdf[\n",
    "    \"overall_vec_min_max\"\n",
    "].transform(\n",
    "    {\n",
    "        OVERALL_MIN_MAX[i]: itemgetter(i) for i, p in enumerate(OVERALL_MIN_MAX)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, OVERALL_MIN_MAX, TARGET_VARIABLE, figsize=(4,4), color=COLOR_OVERALL_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look, they are basically the same. In fact, if we check their correlation, we get... [high correlation on pearson]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, OVERALL_MIN_MAX, color = COLOR_OVERALL_MIN_MAX, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value as feature (min-max normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, we found this paper: \n",
    "\n",
    "So, why not try their approach as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE = [\"avg(value)\"]\n",
    "\n",
    "COLOR_VALUE_MIN_MAX = \"lime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=VALUE, outputCol=\"value_vec\"\n",
    ")\n",
    "\n",
    "# min_max_df = assembler.transform(min_max_df)\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT BREAKS HERE, if using legacy data as well :C\n",
    "\n",
    "# TODO handle value attribute in legacy datasets\n",
    "# in legacy datasets it's encoded as \"ValueMagnitudeCurrency\" (i.e. 70M)\n",
    "# gotta convert it to full length, to be compatible with modern (and actually usable!)\n",
    "\n",
    "# df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE_MIN_MAX = [\"avg(value)_min_max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"value_vec\", \n",
    "    outputCol=\"value_vec_min_max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_df = scaler.fit(min_max_df).transform(min_max_df)\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = min_max_df.toPandas()\n",
    "# NOTE keep the following line disabled!\n",
    "# gotta accumulate overall and value in the same pandas DF, so as we can use it\n",
    "# for the Pearson correlation matrix!\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO make a commodity function, because it will be used in other normalizations as well\n",
    "\n",
    "pdf = pdf.reindex(\n",
    "    columns=list(pdf.columns) + VALUE_MIN_MAX\n",
    ")\n",
    "\n",
    "pdf[VALUE_MIN_MAX] = pdf[\n",
    "    \"value_vec_min_max\"\n",
    "].transform(\n",
    "    {\n",
    "        VALUE_MIN_MAX[i]: itemgetter(i) for i, p in enumerate(VALUE_MIN_MAX)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, VALUE_MIN_MAX, TARGET_VARIABLE, figsize=(4,4), color=COLOR_VALUE_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, VALUE_MIN_MAX, color = COLOR_VALUE_MIN_MAX, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint at a very high correlation, then show it with pearson matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.select(\"overall_vec_min_max\", \"value_vec_min_max\").toPandas()\n",
    "\n",
    "# need to flat every element of the columns, because they are in vectors\n",
    "pdf[\"overall_vec_min_max\"] = pdf[\"overall_vec_min_max\"].map(\n",
    "    lambda x: x[0]\n",
    ")\n",
    "pdf[\"value_vec_min_max\"] = pdf[\"value_vec_min_max\"].map(\n",
    "    lambda x: x[0]\n",
    ")\n",
    "\n",
    "plot_correlation_matrix(\n",
    "    pdf, \n",
    "    [\"overall_vec_min_max\", \"value_vec_min_max\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment this correlation, and move on with life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning for attempt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[\n",
    "    state that via hyperparam grid we'll set the feature column, meaning that we'll try to train the models on all of the attemps\n",
    "\n",
    "[state expected results, according to correlations and similar stuff]\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection_learning_df = feature_selection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection_train_df, feature_selection_test_df = feature_selection_learning_df.randomSplit(\n",
    "#     # [0.9, 0.1]\n",
    "#     [0.7, 0.3]\n",
    "# )\n",
    "\n",
    "learning_train_df, learning_test_df = df.randomSplit(\n",
    "    # [0.9,0.1]\n",
    "    # NOTE reactive 90/10 split, keep 70/30 just when using one league\n",
    "    [0.7,0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS_CV = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_regressor(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    regressor,\n",
    "    regressor_evaluation_metrics,\n",
    "    cv_evaluation_metrics,\n",
    "    hyperparams_grid\n",
    "):\n",
    "\n",
    "    cv_evaluators = {\n",
    "        metric: RegressionEvaluator(\n",
    "            labelCol=\"points\",\n",
    "            metricName=metric,\n",
    "        )\n",
    "        for metric in cv_evaluation_metrics\n",
    "    }\n",
    "\n",
    "    regressor_evaluators = {\n",
    "        metric: RegressionEvaluator(\n",
    "            labelCol=\"points\",\n",
    "            metricName=metric,\n",
    "        )\n",
    "        for metric in regressor_evaluation_metrics\n",
    "    }\n",
    "\n",
    "    cross_validations = {\n",
    "        metric: CrossValidator(\n",
    "            estimator=regressor,\n",
    "            estimatorParamMaps=hyperparams_grid,\n",
    "            evaluator=cv_evaluators[metric],\n",
    "            numFolds=NUM_FOLDS_CV,\n",
    "            collectSubModels=True\n",
    "        )\n",
    "        for metric in cv_evaluation_metrics\n",
    "    }\n",
    "\n",
    "    cross_validated = dict()\n",
    "\n",
    "    for metric in cv_evaluation_metrics:\n",
    "\n",
    "        cross_validated[metric] = cross_validations[\n",
    "            metric\n",
    "        ].fit(train_df)\n",
    "\n",
    "    if (isinstance(regressor, LinearRegression)):\n",
    "        \n",
    "        for metric in cv_evaluation_metrics:\n",
    "\n",
    "            training_result = cross_validated[\n",
    "                metric\n",
    "            ].bestModel.summary\n",
    "\n",
    "            print(\n",
    "                \"***** Evaluating Training Set, (Linear Regression, best model according to metric {}) *****\".format(\n",
    "                    metric\n",
    "                )\n",
    "            )\n",
    "            print(\"RMSE: {:.3f}\".format(training_result.rootMeanSquaredError))\n",
    "            print(\"R2: {:.3f}\".format(training_result.r2))\n",
    "            print(\"Adjusted R2: {:.3f}\".format(training_result.r2adj))\n",
    "            print()\n",
    "\n",
    "        predictions = {\n",
    "            metric: cross_validated[\n",
    "                metric\n",
    "            ].bestModel.transform(test_df)\n",
    "            for metric in cv_evaluation_metrics\n",
    "        }\n",
    "\n",
    "        for m, model in cross_validated.items():\n",
    "\n",
    "                print(\n",
    "                    \"*** {} Set, (rf, best model elected by {}) ***\".format(\n",
    "                        \"Test\", m\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                for e, evaluator in regressor_evaluators.items():\n",
    "\n",
    "                    print(\n",
    "                        \"{}: {}\".format(\n",
    "                            evaluator.getMetricName(), evaluator.evaluate(\n",
    "                                predictions[m]\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                print(\"*******************************\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        for stage_name, stage_df in zip([\"Train\", \"Test\"], [train_df, test_df]):\n",
    "            predictions = {\n",
    "                metric: cross_validated[\n",
    "                    metric\n",
    "                ].bestModel.transform(stage_df)\n",
    "                for metric in cv_evaluation_metrics\n",
    "            }\n",
    "\n",
    "            for m, model in cross_validated.items():\n",
    "\n",
    "                print(\n",
    "                    \"*** {} Set, (rf, best model elected by {}) ***\".format(\n",
    "                        stage_name, m\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                for e, evaluator in regressor_evaluators.items():\n",
    "\n",
    "                    print(\n",
    "                        \"{}: {}\".format(\n",
    "                            evaluator.getMetricName(), evaluator.evaluate(\n",
    "                                predictions[m]\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                print(\"*******************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_classifier(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    classifier,\n",
    "    classifier_evaluation_metrics,\n",
    "    cv_evaluation_metrics,\n",
    "    hyperparams_grid\n",
    "):\n",
    "\n",
    "    cv_evaluators = {\n",
    "        metric: MulticlassClassificationEvaluator(\n",
    "            labelCol=\"macro_place\",\n",
    "            metricName=metric,\n",
    "        )\n",
    "        for metric in cv_evaluation_metrics\n",
    "    }\n",
    "\n",
    "    classifier_evaluators = {\n",
    "        metric: MulticlassClassificationEvaluator(\n",
    "            labelCol=\"macro_place\",\n",
    "            metricName=metric,\n",
    "        )\n",
    "        for metric in classifier_evaluation_metrics\n",
    "    }\n",
    "\n",
    "    cross_validations = {\n",
    "        metric: CrossValidator(\n",
    "            estimator=classifier,\n",
    "            estimatorParamMaps=hyperparams_grid,\n",
    "            evaluator=cv_evaluators[metric],\n",
    "            numFolds=NUM_FOLDS_CV,\n",
    "            collectSubModels=True\n",
    "        )\n",
    "        for metric in cv_evaluation_metrics\n",
    "    }\n",
    "\n",
    "    cross_validated = dict()\n",
    "\n",
    "    for metric in cv_evaluation_metrics:\n",
    "\n",
    "        cross_validated[metric] = cross_validations[\n",
    "            metric\n",
    "        ].fit(train_df)\n",
    "\n",
    "    if (isinstance(classifier, LinearRegression)):\n",
    "        \n",
    "        for metric in cv_evaluation_metrics:\n",
    "\n",
    "            training_result = cross_validated[\n",
    "                metric\n",
    "            ].bestModel.summary\n",
    "\n",
    "            print(\n",
    "                \"***** Evaluating Training Set, (Linear Regression, best model according to metric {}) *****\".format(\n",
    "                    metric\n",
    "                )\n",
    "            )\n",
    "            print(\"RMSE: {:.3f}\".format(training_result.rootMeanSquaredError))\n",
    "            print(\"R2: {:.3f}\".format(training_result.r2))\n",
    "            print(\"Adjusted R2: {:.3f}\".format(training_result.r2adj))\n",
    "            print()\n",
    "\n",
    "        predictions = {\n",
    "            metric: cross_validated[\n",
    "                metric\n",
    "            ].bestModel.transform(test_df)\n",
    "            for metric in cv_evaluation_metrics\n",
    "        }\n",
    "\n",
    "        for m, model in cross_validated.items():\n",
    "\n",
    "                print(\n",
    "                    \"*** {} Set, (rf, best model elected by {}) ***\".format(\n",
    "                        \"Test\", m\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                for e, evaluator in classifier_evaluators.items():\n",
    "\n",
    "                    print(\n",
    "                        \"{}: {}\".format(\n",
    "                            evaluator.getMetricName(), evaluator.evaluate(\n",
    "                                predictions[m]\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                print(\"*******************************\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        for stage_name, stage_df in zip([\"Train\", \"Test\"], [train_df, test_df]):\n",
    "            predictions = {\n",
    "                metric: cross_validated[\n",
    "                    metric\n",
    "                ].bestModel.transform(stage_df)\n",
    "                for metric in cv_evaluation_metrics\n",
    "            }\n",
    "\n",
    "            for m, model in cross_validated.items():\n",
    "\n",
    "                print(\n",
    "                    \"*** {} Set, (rf, best model elected by {}) ***\".format(\n",
    "                        stage_name, m\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                for e, evaluator in classifier_evaluators.items():\n",
    "\n",
    "                    print(\n",
    "                        \"{}: {}\".format(\n",
    "                            evaluator.getMetricName(), evaluator.evaluate(\n",
    "                                predictions[m]\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                print(\"*******************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_train_df, lr_test_df = feature_selection_train_df, feature_selection_test_df\n",
    "lr_train_df, lr_test_df = learning_train_df, learning_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(\n",
    "    # featuresCol=\"feature_vec_ufs_0.1\", \n",
    "    labelCol=\"points\"\n",
    ")\n",
    "\n",
    "lr_evaluation_metrics = [\"r2\", \"mse\"]\n",
    "lr_evaluation_metrics_cv = [\"r2\"]\n",
    "\n",
    "lr_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        lr.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    #TODO add intermediate values: prof uses [0.0, 0.5, 1]\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 1])\n",
    "    #TODO add intermediate values: prof uses [0.0, 0.05, 0.1]\n",
    "    # .addGrid(lr.regParam, [0.0, 0.1])\n",
    "    # .addGrid(lr.fitIntercept, [True, False])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "find_best_regressor(\n",
    "    train_df=lr_train_df, \n",
    "    test_df=lr_test_df, \n",
    "    regressor=lr,\n",
    "    regressor_evaluation_metrics=lr_evaluation_metrics,\n",
    "    cv_evaluation_metrics=lr_evaluation_metrics_cv,\n",
    "    hyperparams_grid=lr_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_train_df, dt_test_df = feature_selection_train_df, feature_selection_test_df\n",
    "dt_train_df, dt_test_df = learning_train_df, learning_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(\n",
    "    # featuresCol=\"feature_vec\", \n",
    "    labelCol=\"points\"\n",
    ")\n",
    "\n",
    "dt_evaluation_metrics = [\"r2\", \"mse\"]\n",
    "dt_evaluation_metrics_cv = [\"r2\"]\n",
    "\n",
    "dt_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        dt.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    # .addGrid(dt.standardization, [True, False])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "find_best_regressor(\n",
    "    train_df=dt_train_df, \n",
    "    test_df=dt_test_df, \n",
    "    regressor=dt,\n",
    "    regressor_evaluation_metrics=dt_evaluation_metrics,\n",
    "    cv_evaluation_metrics=dt_evaluation_metrics_cv,\n",
    "    hyperparams_grid=dt_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_df, rf_test_df = learning_train_df, learning_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(\n",
    "    # featuresCol=\"feature_vec\", \n",
    "    labelCol=\"points\"\n",
    ")\n",
    "\n",
    "rf_evaluation_metrics = [\"r2\", \"mse\"]\n",
    "rf_evaluation_metrics_cv = [\"r2\"]\n",
    "\n",
    "rf_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        rf.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    # .addGrid(rf.standardization, [True, False])\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_regressor(\n",
    "    train_df=rf_train_df, \n",
    "    test_df=rf_test_df, \n",
    "    regressor=rf,\n",
    "    regressor_evaluation_metrics=rf_evaluation_metrics,\n",
    "    cv_evaluation_metrics=rf_evaluation_metrics_cv,\n",
    "    hyperparams_grid=rf_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_train_df, gbt_test_df = learning_train_df, learning_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(\n",
    "    # featuresCol=\"feature_vec\", \n",
    "    labelCol=\"points\"\n",
    ")\n",
    "\n",
    "gbt_evaluation_metrics = [\"r2\", \"mse\"]\n",
    "gbt_evaluation_metrics_cv = [\"r2\"]\n",
    "\n",
    "gbt_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        gbt.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    # .addGrid(gbt.standardization, [True, False])\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_regressor(\n",
    "    train_df=gbt_train_df, \n",
    "    test_df=gbt_test_df, \n",
    "    regressor=gbt,\n",
    "    regressor_evaluation_metrics=gbt_evaluation_metrics,\n",
    "    cv_evaluation_metrics=gbt_evaluation_metrics_cv,\n",
    "    hyperparams_grid=gbt_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate more advanced, dynamic macro placements\n",
    "def get_macro_place(place, league):\n",
    "\n",
    "    if league == \"German Bundesliga\":\n",
    "        if 1 <= place <= 4:\n",
    "            return 0.0\n",
    "        if 5 <= place <= 7:\n",
    "            return 1.0\n",
    "        if 8 <= place <= 10:\n",
    "            return 2.0\n",
    "        if 11 <= place <= 15:\n",
    "            return 3.0\n",
    "        if 16 <= place <= 18:\n",
    "            return 4.0\n",
    "\n",
    "    elif league == \"Holland Eredivise\":\n",
    "        if place == 1:\n",
    "            return 0.0\n",
    "        if 2 <= place <= 3:\n",
    "            return 1.0\n",
    "        if 4 <= place <= 9:\n",
    "            return 2.0\n",
    "        if 10 <= place <= 15:\n",
    "            return 3.0\n",
    "        if 16 <= place <= 18:\n",
    "            return 4.0\n",
    "\n",
    "\n",
    "    elif league == \"French League 1\":\n",
    "        if 1 <= place <= 2:\n",
    "            return 0.0\n",
    "        if 3 <= place <= 5:\n",
    "            return 1.0\n",
    "        if 6 <= place <= 11:\n",
    "            return 2.0\n",
    "        if 12 <= place <= 17:\n",
    "            return 3.0\n",
    "        if 18 <= place <= 20:\n",
    "            return  4.0\n",
    "    \n",
    "    else: #It, Sp, En\n",
    "        if 1 <= place <= 4:\n",
    "            return 0.0\n",
    "        if 5 <= place <= 7:\n",
    "            return 1.0\n",
    "        if 8 <= place <= 12:\n",
    "            return 2.0\n",
    "        if 13 <= place <= 17:\n",
    "            return 3.0\n",
    "        if 18 <= place <= 20:\n",
    "            return 4.0\n",
    "\n",
    "    return None\n",
    "\n",
    "get_macro_place_UDF = udf(\n",
    "    lambda place, league: get_macro_place(float(place), league),\n",
    "    DoubleType(),\n",
    ")\n",
    "\n",
    "NUM_MACRO_PLACES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_df = feature_selection_learning_df\n",
    "classification_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = classification_df.withColumn(\n",
    "    \"macro_place\", get_macro_place_UDF(col(\"place\"), col(\"League\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_train_df, classification_test_df = classification_df.randomSplit(\n",
    "    # [0.9,0.1]\n",
    "    # NOTE reactive 90/10 split, keep 70/30 just when using one league\n",
    "    [0.7,0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    classification_df.toPandas(), \n",
    "    [\"macro_place\"], \n",
    "    color = \"teal\", \n",
    "    figsize=(10,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_train_df, svm_test_df = classification_train_df, classification_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(\n",
    "    featuresCol=\"ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "ovr = OneVsRest(\n",
    "    classifier=svm,\n",
    "    featuresCol=\"feature_vec_ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "svm_evaluation_metrics = [\"accuracy\"]\n",
    "svm_evaluation_metrics_cv = [\"accuracy\"]\n",
    "\n",
    "svm_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        ovr.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_best_classifier(\n",
    "#     train_df=svm_train_df, \n",
    "#     test_df=svm_test_df, \n",
    "#     classifier=ovr,\n",
    "#     classifier_evaluation_metrics=svm_evaluation_metrics,\n",
    "#     cv_evaluation_metrics=svm_evaluation_metrics_cv,\n",
    "#     hyperparams_grid=svm_param_grid\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lor_train_df, lor_test_df = classification_train_df, classification_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO same as SVM\n",
    "\n",
    "lor = LogisticRegression(\n",
    "    # featuresCol=\"ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "ovr = OneVsRest(\n",
    "    classifier=lor,\n",
    "    # featuresCol=\"ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "lor_evaluation_metrics = [\"accuracy\"]\n",
    "lor_evaluation_metrics_cv = [\"accuracy\"]\n",
    "\n",
    "lor_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        ovr.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_classifier(\n",
    "    train_df=lor_train_df, \n",
    "    test_df=lor_test_df, \n",
    "    classifier=ovr,\n",
    "    classifier_evaluation_metrics=lor_evaluation_metrics,\n",
    "    cv_evaluation_metrics=lor_evaluation_metrics_cv,\n",
    "    hyperparams_grid=lor_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_train_df, ct_test_df = classification_train_df, classification_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = DecisionTreeClassifier(\n",
    "    # featuresCol=\"ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "# ovr = OneVsRest(\n",
    "#     classifier=ct,\n",
    "#     featuresCol=\"feature_vec_ufs_0-1\", \n",
    "#     labelCol=\"macro_place\"\n",
    "# )\n",
    "\n",
    "ct_evaluation_metrics = [\"accuracy\"]\n",
    "ct_evaluation_metrics_cv = [\"accuracy\"]\n",
    "\n",
    "ct_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        ct.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_classifier(\n",
    "    train_df=ct_train_df, \n",
    "    test_df=ct_test_df, \n",
    "    classifier=ct,\n",
    "    classifier_evaluation_metrics=ct_evaluation_metrics,\n",
    "    cv_evaluation_metrics=ct_evaluation_metrics_cv,\n",
    "    hyperparams_grid=ct_param_grid\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_train_df, rfc_test_df = classification_train_df, classification_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(\n",
    "    # featuresCol=\"feature_vec_ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "# ovr = OneVsRest(\n",
    "#     classifier=rfc,\n",
    "#     featuresCol=\"feature_vec_ufs_0-1\", \n",
    "#     labelCol=\"macro_place\"\n",
    "# )\n",
    "\n",
    "rfc_evaluation_metrics = [\"accuracy\"]\n",
    "rfc_evaluation_metrics_cv = [\"accuracy\"]\n",
    "\n",
    "rfc_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        rfc.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_classifier(\n",
    "    train_df=rfc_train_df, \n",
    "    test_df=rfc_test_df, \n",
    "    classifier=rfc,\n",
    "    classifier_evaluation_metrics=rfc_evaluation_metrics,\n",
    "    cv_evaluation_metrics=rfc_evaluation_metrics_cv,\n",
    "    hyperparams_grid=rfc_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_train_df, mlp_test_df = classification_train_df, classification_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultilayerPerceptronClassifier(\n",
    "    # featuresCol=\"feature_vec_ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "# ovr = OneVsRest(\n",
    "#     classifier=mlp,\n",
    "#     featuresCol=\"feature_vec_ufs_0-1\", \n",
    "#     labelCol=\"macro_place\"\n",
    "# )\n",
    "\n",
    "mlp_evaluation_metrics = [\"accuracy\"]\n",
    "mlp_evaluation_metrics_cv = [\"accuracy\"]\n",
    "\n",
    "mlp_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(mlp.layers, [[NUM_FEATURES, 4, 4, 2, NUM_MACRO_PLACES]])\n",
    "    .addGrid(\n",
    "        mlp.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_classifier(\n",
    "    train_df=mlp_train_df, \n",
    "    test_df=mlp_test_df, \n",
    "    classifier=mlp,\n",
    "    classifier_evaluation_metrics=mlp_evaluation_metrics,\n",
    "    cv_evaluation_metrics=mlp_evaluation_metrics_cv,\n",
    "    hyperparams_grid=mlp_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom ranking-based evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare ranking of prediction value with actual ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 3: clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_df = pre_processed_df\n",
    "\n",
    "CLUSTERING_FEATURES = copy.deepcopy(PLAYER_FEATURES)\n",
    "CLUSTERING_FEATURES.remove(\"overall\")\n",
    "CLUSTERING_FEATURES.remove(\"value\")\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=CLUSTERING_FEATURES, outputCol=\"all_vec\"\n",
    ")\n",
    "\n",
    "clustering_df = assembler.transform(clustering_df)\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"all_vec\", outputCol=\"all_vec_min_max\"\n",
    ")\n",
    "\n",
    "clustering_df = scaler.fit(clustering_df).transform(clustering_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- short_name: string (nullable = true)\n",
      " |-- club_name: string (nullable = true)\n",
      " |-- league_name: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- player_positions: string (nullable = true)\n",
      " |-- macro_role: string (nullable = true)\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- pace: integer (nullable = true)\n",
      " |-- shooting: integer (nullable = true)\n",
      " |-- passing: integer (nullable = true)\n",
      " |-- dribbling: integer (nullable = true)\n",
      " |-- defending: integer (nullable = true)\n",
      " |-- physic: integer (nullable = true)\n",
      " |-- attacking_crossing: integer (nullable = true)\n",
      " |-- attacking_finishing: integer (nullable = true)\n",
      " |-- attacking_heading_accuracy: integer (nullable = true)\n",
      " |-- attacking_short_passing: integer (nullable = true)\n",
      " |-- skill_dribbling: integer (nullable = true)\n",
      " |-- skill_fk_accuracy: integer (nullable = true)\n",
      " |-- skill_long_passing: integer (nullable = true)\n",
      " |-- skill_ball_control: integer (nullable = true)\n",
      " |-- movement_acceleration: integer (nullable = true)\n",
      " |-- movement_sprint_speed: integer (nullable = true)\n",
      " |-- movement_reactions: integer (nullable = true)\n",
      " |-- power_shot_power: integer (nullable = true)\n",
      " |-- power_stamina: integer (nullable = true)\n",
      " |-- power_strength: integer (nullable = true)\n",
      " |-- power_long_shots: integer (nullable = true)\n",
      " |-- mentality_aggression: integer (nullable = true)\n",
      " |-- mentality_penalties: integer (nullable = true)\n",
      " |-- defending_standing_tackle: integer (nullable = true)\n",
      " |-- all_vec: vector (nullable = true)\n",
      " |-- all_vec_min_max: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustering_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(\n",
    "    dataset,\n",
    "    n_clusters,\n",
    "    distance_measure=\"euclidean\",\n",
    "    max_iter=20,\n",
    "    features_col=\"features\",\n",
    "    prediction_col=\"cluster\",\n",
    "    random_seed=RANDOM_SEED,\n",
    "):\n",
    "\n",
    "    print(\n",
    "        f\"\"\"Training K-means clustering using the following parameters: \n",
    "        - K (n. of clusters) = {n_clusters}\n",
    "        - max_iter (max n. of iterations) = {max_iter}\n",
    "        - distance measure = {distance_measure}\n",
    "        - random seed = {random_seed if random_seed is not None else \"NONE USED!\"}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    if distance_measure == \"cosine\":\n",
    "\n",
    "        # Normalize inputs to unit-length vectors\n",
    "        dataset = Normalizer(\n",
    "            inputCol=features_col, outputCol=features_col + \"_norm\", p=1\n",
    "        ).transform(dataset)\n",
    "\n",
    "        features_col = features_col + \"_norm\"\n",
    "\n",
    "    # Train a K-means model\n",
    "    kmeans = KMeans(\n",
    "        featuresCol=features_col,\n",
    "        predictionCol=prediction_col,\n",
    "        k=n_clusters,\n",
    "        initMode=\"k-means||\",\n",
    "        initSteps=5,\n",
    "        tol=0.000001,\n",
    "        maxIter=max_iter,\n",
    "        # seed=random_seed,\n",
    "        distanceMeasure=distance_measure,\n",
    "    )\n",
    "\n",
    "    model = kmeans.fit(dataset)\n",
    "    # here there are all the relevant clustering information\n",
    "\n",
    "    # Make clusters\n",
    "    clusters_df = model.transform(dataset)\n",
    "\n",
    "    return model, clusters_df\n",
    "\n",
    "\n",
    "def evaluate_k_means(\n",
    "    clusters,\n",
    "    metric_name=\"silhouette\",\n",
    "    distance_measure=\"squaredEuclidean\",  # cosine\n",
    "    prediction_col=\"cluster\",\n",
    "    featuresCol = \"features\"\n",
    "):\n",
    "\n",
    "    # Evaluate clustering by computing Silhouette score\n",
    "    evaluator = ClusteringEvaluator(\n",
    "        metricName=metric_name,\n",
    "        distanceMeasure=distance_measure,\n",
    "        predictionCol=prediction_col,\n",
    "        featuresCol=featuresCol\n",
    "    )\n",
    "\n",
    "    return evaluator.evaluate(clusters)\n",
    "\n",
    "\n",
    "def do_clustering(\n",
    "    max_k_clusters, \n",
    "    input_df, \n",
    "    max_iter,\n",
    "    featuresCol,\n",
    "    clusterCol\n",
    "):\n",
    "    clustering_results = {}\n",
    "\n",
    "    clusters_df = input_df\n",
    "\n",
    "    # for k in tqdm( range(5, max_k_clusters + 1, 5), desc = \"Performing clustering\" ):\n",
    "    for k in tqdm(range(2, max_k_clusters + 1, 4), desc=\"Performing clustering\"):\n",
    "\n",
    "        print(f\"Running K-means using K = {k}\")\n",
    "\n",
    "        # model, clusters_df = k_means(tf_idf_df, k, max_iter=50, distance_measure=\"cosine\") # Alternatively, distance_measure=\"euclidean\"\n",
    "        # model, clusters_df = k_means(input_df, k, max_iter=max_iter, distance_measure=\"cosine\") # Alternatively, distance_measure=\"euclidean\"\n",
    "        model, clusters_df = k_means(\n",
    "            # input_df,\n",
    "            clusters_df, \n",
    "            k, \n",
    "            max_iter=max_iter, \n",
    "            distance_measure=\"euclidean\", \n",
    "            features_col=featuresCol,\n",
    "            prediction_col=clusterCol + \"_k_\" + str(k),\n",
    "        )  # Alternatively, distance_measure=\"euclidean\"\n",
    "        # silhouette_k = evaluate_k_means(clusters_df, distance_measure=\"cosine\") # Alternatively, distance_measure=\"squaredEuclidean\"\n",
    "        silhouette_k = evaluate_k_means(\n",
    "            clusters_df, \n",
    "            distance_measure=\"squaredEuclidean\",\n",
    "            prediction_col=clusterCol + \"_k_\" + str(k),\n",
    "            featuresCol=featuresCol\n",
    "\n",
    "        )  # Alternatively, distance_measure=\"squaredEuclidean\"\n",
    "        # wssd_k = model.summary.trainingCost\n",
    "        wssd_k = model.summary\n",
    "\n",
    "        print(\n",
    "            \"Silhouette coefficient computed with cosine distance: {:.3f}\".format(\n",
    "                silhouette_k\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"Within-cluster Sum of Squared Distances (using cosine distance): {:.3f}\".format(\n",
    "                wssd_k.trainingCost\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"--------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "\n",
    "        l = list(\n",
    "            enumerate(\n",
    "                model.clusterCenters()\n",
    "            )\n",
    "        )\n",
    "        l = [(ind, DenseVector(c)) for ind, c in l]\n",
    "        # print(l)\n",
    "        schema = [\"cluster_id\"  + \"_k_\" + str(k), \"centroid\"  + \"_k_\" + str(k)]\n",
    "\n",
    "        schema = StructType([ \n",
    "            StructField(\"cluster_id\" + \"_k_\" + str(k),IntegerType(),True), \n",
    "            StructField(\"centroid\" + \"_k_\" + str(k),VectorUDT(),True), \n",
    "        ])\n",
    "        centr_df = spark.createDataFrame(data=l, schema=schema)\n",
    "\n",
    "        # centr_df.show()\n",
    "\n",
    "        # df_with_centroids = clusters_df.join(\n",
    "        #     centr_df, on=[\"cluster_id\"]\n",
    "        # )\n",
    "\n",
    "        clusters_df = clusters_df.join(\n",
    "            centr_df, on=[\"cluster_id\" + \"_k_\" + str(k)]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        clustering_results[str(k)] = {\n",
    "            \"silhouette_k\"      : silhouette_k,\n",
    "            \"wssd_k\"            : wssd_k,\n",
    "            \"model\"             : model,\n",
    "            \"df\"                : clusters_df,\n",
    "            # \"cluster_centroids\" : model.clusterCenters(),\n",
    "            # \"centr_df\" : centr_df,\n",
    "            # \"df_with_centroids\" : df_with_centroids\n",
    "        }\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Free up memory space at the end of each iteration\n",
    "        # del model\n",
    "        # del clusters_df\n",
    "        # gc.collect() # garbage collector\n",
    "    \n",
    "    clustering_results[\"df_with_centroid_full\"] = clusters_df\n",
    "\n",
    "    return clustering_results\n",
    "\n",
    "\n",
    "def plot_clustering_results(clustering_results):\n",
    "    # load the dictionary into pandas\n",
    "    df = pd.DataFrame.from_dict(clustering_results, orient=\"index\").reset_index()\n",
    "    df.columns = [\"K\", \"Silhouette\", \"WSSD\"]\n",
    "    # Create a 1x1 figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    _ = sns.pointplot(data=df, x=\"K\", y=\"WSSD\", ax=ax, color=\"orangered\")\n",
    "    _ = ax.set_xlabel(\"K\")\n",
    "    _ = ax.set_ylabel(\"WSSD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_df_dict = {\n",
    "    s: clustering_df.filter(col(\"season\") == s) for s in seasons\n",
    "}\n",
    "\n",
    "MAX_K_CLUSTERS = 8\n",
    "MAX_ITER = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing clustering:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running K-means using K = 2\n",
      "Training K-means clustering using the following parameters: \n",
      "        - K (n. of clusters) = 2\n",
      "        - max_iter (max n. of iterations) = 4\n",
      "        - distance measure = euclidean\n",
      "        - random seed = NONE USED!\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing clustering:  50%|     | 1/2 [00:06<00:06,  6.44s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette coefficient computed with cosine distance: 0.832\n",
      "Within-cluster Sum of Squared Distances (using cosine distance): 366.828\n",
      "--------------------------------------------------------------------------------------\n",
      "Running K-means using K = 6\n",
      "Training K-means clustering using the following parameters: \n",
      "        - K (n. of clusters) = 6\n",
      "        - max_iter (max n. of iterations) = 4\n",
      "        - distance measure = euclidean\n",
      "        - random seed = NONE USED!\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing clustering: 100%|| 2/2 [00:11<00:00,  5.97s/it]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette coefficient computed with cosine distance: 0.352\n",
      "Within-cluster Sum of Squared Distances (using cosine distance): 192.038\n",
      "--------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing clustering:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running K-means using K = 2\n",
      "Training K-means clustering using the following parameters: \n",
      "        - K (n. of clusters) = 2\n",
      "        - max_iter (max n. of iterations) = 4\n",
      "        - distance measure = euclidean\n",
      "        - random seed = NONE USED!\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing clustering:  50%|     | 1/2 [00:03<00:03,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette coefficient computed with cosine distance: 0.819\n",
      "Within-cluster Sum of Squared Distances (using cosine distance): 376.169\n",
      "--------------------------------------------------------------------------------------\n",
      "Running K-means using K = 6\n",
      "Training K-means clustering using the following parameters: \n",
      "        - K (n. of clusters) = 6\n",
      "        - max_iter (max n. of iterations) = 4\n",
      "        - distance measure = euclidean\n",
      "        - random seed = NONE USED!\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing clustering: 100%|| 2/2 [00:07<00:00,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette coefficient computed with cosine distance: 0.402\n",
      "Within-cluster Sum of Squared Distances (using cosine distance): 186.611\n",
      "--------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clustering_results_dict = dict()\n",
    "\n",
    "for s in seasons:\n",
    "    clustering_results_dict[s] = do_clustering(\n",
    "        MAX_K_CLUSTERS, \n",
    "        clustering_df_dict[s], \n",
    "        MAX_ITER,\n",
    "        \"all_vec_min_max\",\n",
    "        \"cluster_id\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_together_df = reduce(\n",
    "    DataFrame.unionAll, \n",
    "    [\n",
    "        clustering_results_dict[s][\"df_with_centroid_full\"] for s in seasons\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO select K according to clustering evaluation\n",
    "\n",
    "K = [\"2\",\"6\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute distance between player and centroid of its cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_distance_from_centroid_UDF = udf(\n",
    "    lambda player, centroid: float(\n",
    "        Vectors.squared_distance(\n",
    "            player, centroid\n",
    "        )\n",
    "    ), FloatType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K:\n",
    "    all_together_df = all_together_df.withColumn(\n",
    "        \"distance_from_centroid\" + \"_k_\" + str(k),\n",
    "        compute_distance_from_centroid_UDF(\n",
    "            col(\"all_vec_min_max\"),\n",
    "            col(\"centroid\" + \"_k_\" + str(k))\n",
    "        )\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From players to teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_together_df.select([\"season\", \"club_name\", \"macro_role\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = all_together_df.groupBy(\n",
    "    [\"season\", \"club_name\", \"macro_role\"]\n",
    ").agg(\n",
    "    { \n",
    "        \"distance_from_centroid\" + \"_k_\" + str(k): \"avg\" for k in K \n",
    "    }\n",
    ")\n",
    "\n",
    "for k in K:\n",
    "    teams_df = teams_df.withColumnRenamed(\n",
    "        \"avg(distance_from_centroid\" + \"_k_\" + str(k) + \")\",\n",
    "        \"avg_distance_from_centroid\" + \"_k_\" + str(k)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subquery(macro_role, k):\n",
    "    return f\"\"\"(\n",
    "        case\n",
    "            when macro_role='{macro_role}' then avg_distance_from_centroid_k_{k} \n",
    "        else NULL\n",
    "        end\n",
    "    ) as avg_dist_macro_role_{int(macro_role)}_k_{k}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df.createOrReplaceTempView(\"t\")\n",
    "\n",
    "temp = dict()\n",
    "\n",
    "for k in K:\n",
    "\n",
    "    temp[k] = (\n",
    "        spark.sql(\n",
    "            f\"\"\"\n",
    "                select season, club_name, {generate_subquery(0.0, k)}, {generate_subquery(1.0, k)}, {generate_subquery(2.0, k)}, {generate_subquery(3.0, k)}, {generate_subquery(4.0, k)}, {generate_subquery(5.0, k)}, {generate_subquery(6.0, k)}, {generate_subquery(7.0, k)}\n",
    "                from t\n",
    "            \"\"\"\n",
    "        )\n",
    "        .groupBy(\"season\", \"club_name\")\n",
    "        .agg(\n",
    "            # TODO use for loop as in second cell of \"from players to teams\"\n",
    "            sum(f\"avg_dist_macro_role_0_k_{k}\").alias(f\"avg_dist_macro_role_0_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_1_k_{k}\").alias(f\"avg_dist_macro_role_1_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_2_k_{k}\").alias(f\"avg_dist_macro_role_2_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_3_k_{k}\").alias(f\"avg_dist_macro_role_3_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_4_k_{k}\").alias(f\"avg_dist_macro_role_4_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_5_k_{k}\").alias(f\"avg_dist_macro_role_5_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_6_k_{k}\").alias(f\"avg_dist_macro_role_6_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_7_k_{k}\").alias(f\"avg_dist_macro_role_7_k_{k}\"),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE executing this cell n times, w/o restoring teams_df --> \n",
    "# n copies of avg_dist_macro_role_[0:7]_k_[2, 6]\n",
    "\n",
    "for k in K:\n",
    "    teams_df = teams_df.join(\n",
    "        temp[k], on=[\"season\", \"club_name\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: string (nullable = true)\n",
      " |-- club_name: string (nullable = true)\n",
      " |-- macro_role: string (nullable = true)\n",
      " |-- avg_distance_from_centroid_k_6: double (nullable = true)\n",
      " |-- avg_distance_from_centroid_k_2: double (nullable = true)\n",
      " |-- avg_dist_macro_role_0_k_2: double (nullable = true)\n",
      " |-- avg_dist_macro_role_1_k_2: double (nullable = true)\n",
      " |-- avg_dist_macro_role_2_k_2: double (nullable = true)\n",
      " |-- avg_dist_macro_role_3_k_2: double (nullable = true)\n",
      " |-- avg_dist_macro_role_4_k_2: double (nullable = true)\n",
      " |-- avg_dist_macro_role_5_k_2: double (nullable = true)\n",
      " |-- avg_dist_macro_role_6_k_2: double (nullable = true)\n",
      " |-- avg_dist_macro_role_7_k_2: double (nullable = true)\n",
      " |-- avg_dist_macro_role_0_k_6: double (nullable = true)\n",
      " |-- avg_dist_macro_role_1_k_6: double (nullable = true)\n",
      " |-- avg_dist_macro_role_2_k_6: double (nullable = true)\n",
      " |-- avg_dist_macro_role_3_k_6: double (nullable = true)\n",
      " |-- avg_dist_macro_role_4_k_6: double (nullable = true)\n",
      " |-- avg_dist_macro_role_5_k_6: double (nullable = true)\n",
      " |-- avg_dist_macro_role_6_k_6: double (nullable = true)\n",
      " |-- avg_dist_macro_role_7_k_6: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teams_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering-learning cross evaluation\n",
    "\n",
    "Classic left-right plot, with:\n",
    "Left Y --> elbow result\n",
    "Right Y --> accuracy\n",
    "X axis --> # clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 4: thinking \"Deep\", shallow injecting some priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the prior (RP coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RP impact\n",
    "\n",
    "plot showing that the more the weight of the RP coefficient is increased, the more the accuracy ofc goes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d6993cb2f9ce9a59d5d7380609d9cb5192a9dedd2735a011418ad9e827eb538"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
