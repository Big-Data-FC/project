{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the **Big Data FC** project is to **predict** how many **points** a **football team** belonging to the main European football leagues will end the season with, according to the **characteristics of its players**.\n",
    "\n",
    "To reach the goal, data relative to the **football players** will first be loaded, in order to then compose the **football teams**.\n",
    "After that, a second dataset will be used to gather seasonal **rankings**, for every football team.\n",
    "\n",
    "The project as a whole is composed of:\n",
    "\n",
    "* This **notebook**, containing all steps of:\n",
    "  * Data loading.\n",
    "  * Data cleaning and pre-processing\n",
    "  * Data visualization.\n",
    "  * Data analysis.\n",
    "  * Learning and evaluation.\n",
    "* A custom [**scraper**](https://github.com/Big-Data-FC/scraper), to gather further players data.\n",
    "* A set of [**REST APIs**](https://github.com/Big-Data-FC/api) to query the loaded data and the prediction model.\n",
    "* The collection of [scraped datasets](https://github.com/Big-Data-FC/datasets).\n",
    "\n",
    "During the project, multiple approaches and techniques were explored and described in this notebook.\n",
    "\n",
    "The notebook follows the thinking flow that happened during development stage:\n",
    "1. Notebook **set-up** and **configuration**\n",
    "2. Data **loading** and **pre-processing**\n",
    "3. Preliminary data **exploration**\n",
    "4. **Multiple** **learning** attempts:\n",
    "   1. Naive\n",
    "   2. Dimensionality reduction\n",
    "   3. Learning-produced features (via Clustering)\n",
    "   4. Prior-based approach (RP coefficient)\n",
    "5. Final observations and **conclusion**\n",
    "\n",
    "_By [Daniele Solombrino](https://github.com/dansolombrino) and [Davide Quaranta](https://github.com/fortym2)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook configuration, global parameters and utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution environment settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ON_COLAB to True if the notebook is to be executed on Google Colab,\n",
    "# or generally in an environment in which PySpark needs to be installed and configured.\n",
    "ON_COLAB = True\n",
    "\n",
    "# Set DOWNLOAD_DATA to True to download and extract datasets and previously trained models.\n",
    "# It is needed to run the notebook without re-training and evaluating every model;\n",
    "# instead, training models and evaluation results will be read from disk.\n",
    "DOWNLOAD_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ON_COLAB:\n",
    "  # need to install pyspark\n",
    "  import os\n",
    "  %pip install pyspark\n",
    "  !apt install openjdk-8-jdk-headless -qq\n",
    "  %pip install ipympl\n",
    "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "\n",
    "if DOWNLOAD_DATA:\n",
    "  # download datasets, previously trained models and evaluations\n",
    "  %pip install gdown\n",
    "  !gdown 1BzAg47HS0sr034D_7-uMvKZXQZ2LKEXZ -O datasets.tar.xz\n",
    "  !gdown 1CESXCxPbXFokZbqb7WxyWT3e_VTyNgyN -O trained_models.tar.xz\n",
    "  !gdown 1oXr8HXMusO7erDjs2TiJMwqIJ0wnPN5K -O evaluation_results.tar.xz\n",
    "\n",
    "  !mkdir data\n",
    "  !tar -xvf datasets.tar.xz -C data\n",
    "  !tar -xvf trained_models.tar.xz\n",
    "  !tar -xvf evaluation_results.tar.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section contains some global constants and structures needed across the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_K_CLUSTERS = 66\n",
    "MAX_ITER = 20\n",
    "\n",
    "k_range = range(2, MAX_K_CLUSTERS, 4)\n",
    "K_RANGE = [str(k) for k in k_range]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rp_tradeoff` is a concept explored in the Attempt 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_RP_TRADEOFF = [0.5, 1, 2, 4, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains a dictionary of the path of each model that has been trained and persisted, in order to make the notebook's execution feasable in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of Learning attempts\n",
    "attempts = range(1, 5)\n",
    "\n",
    "# learning trained model directories \n",
    "TRAINED_MODELS_DIRS = {\n",
    "    \"Attempt \" + str(k) : {\n",
    "        \"Regression\": {\n",
    "            \"Linear Regression\": f\"trained_models/attempt_{k}/regression/linear_regression\",\n",
    "            \"Prediction Tree\": f\"trained_models/attempt_{k}/regression/prediction_tree\",\n",
    "            \"Gradient Boosted Tree\": f\"trained_models/attempt_{k}/regression/gradient_boosted_tree\",\n",
    "            \"Random Forest\": f\"trained_models/attempt_{k}/regression/random_forest\",\n",
    "        },\n",
    "        \"Classification\": {\n",
    "            \"SVM\": f\"trained_models/attempt_{k}/classification/svm\",\n",
    "            \"Decision Tree\": f\"trained_models/attempt_{k}/classification/decision_tree\",\n",
    "            \"Logistic Regression\": f\"trained_models/attempt_{k}/classification/logistic_regression\",\n",
    "            \"Random Forest\": f\"trained_models/attempt_{k}/classification/random_forest\",\n",
    "            \"MLP\": f\"trained_models/attempt_{k}/classification/mlp\",\n",
    "        }\n",
    "    } for k in attempts\n",
    "}\n",
    "\n",
    "TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Linear Regression multiple k\"] = {\n",
    "    k : f\"trained_models/attempt_3/regression/linear_regression_multiple_k/{k}_clusters\" for k in K_RANGE\n",
    "}\n",
    "\n",
    "TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Linear Regression multiple tradeoffs\"] = {\n",
    "    tradeoff : f\"trained_models/attempt_4/regression/linear_regression_multiple_tradeoffs/tradeoff_{tradeoff}\" for tradeoff in ADD_RP_TRADEOFF\n",
    "}\n",
    "\n",
    "CLUSTERING_DF_PATH = \"./trained_models/clustering_df.parquet\"\n",
    "CLUSTERING_EVAL_PATH = \"./evaluation_results/attempt_3/clustering/clustering_eval.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_exists(model_path):\n",
    "    return os.path.isdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_disk(model_type, model_path):\n",
    "    return model_type.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PySpark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import gc\n",
    "import builtins\n",
    "import operator\n",
    "import json\n",
    "import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator, RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "from pyspark.ml.regression import Regressor\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel, LinearRegressionTrainingSummary\n",
    "from pyspark.ml.regression import RandomForestRegressor, RandomForestRegressionModel\n",
    "from pyspark.ml.regression import RegressionModel\n",
    "\n",
    "from pyspark.ml.classification import Classifier\n",
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import RandomForestClassificationTrainingSummary\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationSummary\n",
    "from pyspark.ml.classification import ClassificationModel\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationModel\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import DenseVector, Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.ui.port\", \"4050\")\n",
    "    .set(\"spark.executor.memory\", \"4G\")\n",
    "    .set(\"spark.driver.memory\", \"8G\")\n",
    "    .set(\"spark.driver.maxResultSize\", \"8G\")\n",
    ")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random seed is used for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(\n",
    "    x,\n",
    "    y,\n",
    "    x_label,\n",
    "    y_label,\n",
    "    title=\"\",\n",
    "    c=None,\n",
    "    c_map=plt.cm.get_cmap(\"tab10\"),\n",
    "    figsize=(12,8),\n",
    "    ):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    _ = plt.scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        c=y if c is None else c,\n",
    "        edgecolor=\"none\",\n",
    "        cmap=c_map,\n",
    "        axes=ax,\n",
    "    )\n",
    "\n",
    "    _ = ax.set_xlabel(x_label, labelpad=20, fontsize=16)\n",
    "    _ = ax.set_ylabel(y_label, fontsize=16)\n",
    "    _ = ax.set_title(title)\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_target_relation(\n",
    "    data, x, y, n_cols=2, figsize=(15, 30), color=\"#000000\"\n",
    "):\n",
    "\n",
    "    n_rows = int(len(x) / n_cols) if len(x) >= n_cols else n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "    for x_ind, x_value in enumerate(x):\n",
    "        ax = sns.regplot(\n",
    "            data=data,\n",
    "            x=x_value,\n",
    "            y=y,\n",
    "            color = color,\n",
    "            ax=axes[x_ind // n_cols, x_ind % n_cols] if n_rows > 1 else axes,\n",
    "        )\n",
    "\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distribution(\n",
    "    data,\n",
    "    features,\n",
    "    figsize=(4,4), \n",
    "    color=\"#000000\",\n",
    "    n_cols=2\n",
    "):\n",
    "\n",
    "    n_rows = int(len(features) / n_cols) if len(features) >= n_cols else n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "    for feature_ind, feature in enumerate(features):\n",
    "        _ = sns.histplot(\n",
    "            data[feature],\n",
    "            kde=True,\n",
    "            color=color,\n",
    "            facecolor=color,\n",
    "            ax=axes[feature_ind // n_cols, feature_ind % n_cols] if n_rows > 1 else axes,\n",
    "        )\n",
    "\n",
    "    fig.tight_layout(pad=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(\n",
    "    data, features, title=\"Pearson Correlation Matrix\", figsize=(16,12)\n",
    "):\n",
    "\n",
    "    mask = np.zeros_like(data[features].corr(), dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    with sns.axes_style(\"white\"):  # Temporarily set the background to white\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        plt.title(title, fontsize=24)\n",
    "\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "        _ = sns.heatmap(\n",
    "            data[features].corr(),\n",
    "            linewidths=0.25,\n",
    "            vmax=0.7,\n",
    "            square=True,\n",
    "            ax=ax,\n",
    "            cmap=cmap,\n",
    "            linecolor=\"w\",\n",
    "            annot=True,\n",
    "            annot_kws={\"size\": 8},\n",
    "            mask=mask,\n",
    "            cbar_kws={\"shrink\": 0.9},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluators(estimator, label_col, prediction_col, evaluation_metrics):\n",
    "    evaluators = dict()\n",
    "\n",
    "    if isinstance(estimator, Regressor) or isinstance(estimator, RegressionModel):\n",
    "        evaluators = {\n",
    "            metric: RegressionEvaluator(\n",
    "                labelCol=label_col,\n",
    "                predictionCol=prediction_col,\n",
    "                metricName=metric,\n",
    "            )\n",
    "            for metric in evaluation_metrics\n",
    "        }\n",
    "    elif isinstance(estimator, Classifier) or isinstance(estimator, OneVsRestModel) or isinstance(estimator, ClassificationModel):\n",
    "        evaluators = {\n",
    "            metric: MulticlassClassificationEvaluator(\n",
    "                labelCol=label_col,\n",
    "                predictionCol=prediction_col,\n",
    "                metricName=metric,\n",
    "            )\n",
    "            for metric in evaluation_metrics\n",
    "        }\n",
    "    else:\n",
    "        raise Exception(\"Unexpected estimator, got\" + str(type(estimator)))\n",
    "\n",
    "    return evaluators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_best_model(\n",
    "    estimator, \n",
    "    param_grid,\n",
    "    evaluator_cv\n",
    "):\n",
    "    cross_validator = CrossValidator(\n",
    "        estimator=estimator,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator_cv,\n",
    "        numFolds=5,\n",
    "        collectSubModels=False\n",
    "    )\n",
    "    \n",
    "    cross_validated_model = cross_validator.fit(train_df)\n",
    "\n",
    "    return cross_validated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_learning_models(\n",
    "    best_model, \n",
    "    evaluators,\n",
    "    save_training_result_path=None\n",
    "):\n",
    "    has_summary = True\n",
    "    \n",
    "    out_dict = {\n",
    "        \"train_set_evaluation\": dict(),\n",
    "        \"test_set_evaluation\": dict()\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        has_summary = best_model.hasSummary\n",
    "    except AttributeError as e:\n",
    "        # since the only models that have summary\n",
    "        # have the hasSummary field,\n",
    "        # it is needed to catch this exception\n",
    "        has_summary = False\n",
    "    \n",
    "    if has_summary:\n",
    "        print(\"Summary available, retrieving data...\")\n",
    "\n",
    "        if isinstance(\n",
    "            best_model, LinearRegressionModel\n",
    "        ) or isinstance(best_model, RandomForestClassificationModel):\n",
    "            training_result = best_model.summary\n",
    "        elif isinstance(best_model, MultilayerPerceptronClassificationModel):\n",
    "            training_result = best_model.summary()\n",
    "\n",
    "        if any(\n",
    "            x in type(best_model).__name__ for x in [\n",
    "                \"LinearRegression\", \n",
    "                \"DecisionTreeRegressor\", \n",
    "                \"GBTRegressor\",\n",
    "                \"RandomForestRegressor\"\n",
    "            ]\n",
    "        ):\n",
    "            out_dict[\"train_set_evaluation\"][\"r2\"] = training_result.r2\n",
    "            out_dict[\"train_set_evaluation\"][\"r2adj\"] = training_result.r2adj\n",
    "            out_dict[\"train_set_evaluation\"][\"meanSquaredError\"] = training_result.meanSquaredError\n",
    "            out_dict[\"train_set_evaluation\"][\"meanAbsoluteError\"] = training_result.meanAbsoluteError\n",
    "            out_dict[\"train_set_evaluation\"][\"rootMeanSquaredError\"] = training_result.rootMeanSquaredError\n",
    "            out_dict[\"train_set_evaluation\"][\"explainedVariance\"] = training_result.explainedVariance\n",
    "        else:\n",
    "            out_dict[\"train_set_evaluation\"][\"accuracy\"] = training_result.accuracy\n",
    "            out_dict[\"train_set_evaluation\"][\"falsePositiveRateByLabel\"] = training_result.falsePositiveRateByLabel\n",
    "            out_dict[\"train_set_evaluation\"][\"precisionByLabel\"] = training_result.precisionByLabel\n",
    "            out_dict[\"train_set_evaluation\"][\"recallByLabel\"] = training_result.recallByLabel\n",
    "            out_dict[\"train_set_evaluation\"][\"truePositiveRateByLabel\"] = training_result.truePositiveRateByLabel\n",
    "            out_dict[\"train_set_evaluation\"][\"weightedFalsePositiveRate\"] = training_result.weightedFalsePositiveRate\n",
    "            out_dict[\"train_set_evaluation\"][\"weightedPrecision\"] = training_result.weightedPrecision\n",
    "            out_dict[\"train_set_evaluation\"][\"weightedRecall\"] = training_result.weightedRecall\n",
    "            out_dict[\"train_set_evaluation\"][\"weightedTruePositiveRate\"] = training_result.weightedTruePositiveRate\n",
    "            out_dict[\"train_set_evaluation\"][\"fMeasureByLabel\"] = training_result.fMeasureByLabel()\n",
    "            out_dict[\"train_set_evaluation\"][\"weightedFMeasure\"] = training_result.weightedFMeasure()\n",
    "\n",
    "        predictions = best_model.transform(test_df)\n",
    "                    \n",
    "        for e, evaluator in evaluators.items():\n",
    "            if save_training_result_path is not None:\n",
    "                out_dict[\"test_set_evaluation\"][evaluator.getMetricName()] = evaluator.evaluate(predictions)\n",
    "    \n",
    "    else: # no summary available\n",
    "        print(\"No summary available, using custom evaluation procedure...\")\n",
    "        for stage_name, stage_df in zip([\"Train\", \"Test\"], [train_df, test_df]):\n",
    "            predictions = best_model.transform(stage_df)\n",
    "\n",
    "            for e, evaluator in evaluators.items():\n",
    "                if save_training_result_path is not None:\n",
    "                    out_dict[f\"{stage_name.lower()}_set_evaluation\"][evaluator.getMetricName()] = evaluator.evaluate(predictions)\n",
    "\n",
    "    if save_training_result_path is not None:\n",
    "        os.makedirs(\n",
    "            os.path.join(\n",
    "                *save_training_result_path.split(\"/\")[:-1]\n",
    "            ), \n",
    "            exist_ok=True\n",
    "        ) \n",
    "        \n",
    "        with open(save_training_result_path, \"w\", encoding=\"utf8\") as f:\n",
    "            f.write(json.dumps(out_dict))\n",
    "    \n",
    "    pprint.pprint(out_dict)\n",
    "\n",
    "    return out_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_evaluation(model_evaluation_path):\n",
    "    with open(model_evaluation_path) as json_file:\n",
    "        model_evaluation = json.load(json_file)\n",
    "        \n",
    "        pprint.pprint(model_evaluation)\n",
    "        return model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "PCA_NUM_COMPONENTS = 2\n",
    "\n",
    "def perform_pca(df, num_components, input_col, output_col):\n",
    "    pca = PCA(\n",
    "        k=num_components, \n",
    "        inputCol=input_col, \n",
    "        outputCol=output_col\n",
    "    )\n",
    "    pca_model = pca.fit(df)\n",
    "\n",
    "    return pca_model.transform(df), pca_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_explained_variance(pca_model, num_components_to_plot=2, figsize=(8,6)):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    _ = sns.barplot(\n",
    "        x=[i for i in range(num_components_to_plot)],\n",
    "        y=pca_model.explainedVariance.values[0:num_components_to_plot],\n",
    "        ax=ax,\n",
    "        palette=\"summer\"\n",
    "    )\n",
    "\n",
    "    _ = ax.set_xlabel(\"Eigenvalues\", labelpad=16, fontsize=16)\n",
    "    _ = ax.set_ylabel(\"Proportion of Variance\", fontsize=16)\n",
    "    _ = ax.set_xticklabels(\n",
    "        [f\"Principal Component {i}\" for i in range(num_components_to_plot)], \n",
    "        rotation=0\n",
    "    )\n",
    "    _ = ax.set_title(\"Explained variance of each Principal Component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning constants and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGRESSION_EVALUATION_METRICS = [\"r2\", \"mse\", \"rmse\", \"mae\", \"var\"]\n",
    "REGRESSION_EVALUATION_METRIC_CV = \"r2\"\n",
    "\n",
    "REGRESSION_LABEL_COL = \"points\"\n",
    "\n",
    "regression_evaluator_cv = RegressionEvaluator(\n",
    "    metricName=REGRESSION_EVALUATION_METRIC_CV\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION_EVALUATION_METRICS = [\n",
    "    \"f1\",\n",
    "    \"accuracy\",\n",
    "    \"weightedPrecision\",\n",
    "    \"weightedRecall\",\n",
    "    \"weightedTruePositiveRate\",\n",
    "    \"weightedFalsePositiveRate\",\n",
    "    \"weightedFMeasure\",\n",
    "    \"truePositiveRateByLabel\",\n",
    "    \"falsePositiveRateByLabel\",\n",
    "    \"precisionByLabel\",\n",
    "    \"recallByLabel\",\n",
    "    \"fMeasureByLabel\"\n",
    "]\n",
    "\n",
    "CLASSIFICATION_EVALUATION_METRIC_CV = \"accuracy\"\n",
    "\n",
    "CLASSIFICATION_LABEL_COL = \"macro_place\"\n",
    "\n",
    "classification_evaluator_cv = MulticlassClassificationEvaluator(\n",
    "    metricName=CLASSIFICATION_EVALUATION_METRIC_CV\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading football players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to load football players data, which comes from to two different sources:\n",
    "\n",
    "* For seasons between 2015 and 2020 (called \"modern\"): [FIFA 15-21 complete dataset](https://www.kaggle.com/datasets/stefanoleone992/fifa-21-complete-player-dataset)\n",
    "\n",
    "* For season between 2007 and 2014 (called \"legacy\"): scraped data from [sofifa.com](https://sofifa.com), a website specialized in storing data taken from EA Sports FIFA games.\n",
    "\n",
    "As introduced before, scraped datasets are committed in a [GitHub repository](https://github.com/Big-Data-FC/datasets).\n",
    "\n",
    "From now on, the terms **modern** and **legacy** will be used to refer to the two kinds of datasets.\n",
    "\n",
    "Initially, modern and legacy data will be splitted in two different dataframes, since there are some differences in the structure of their data, among these different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_df = spark.read.csv(\n",
    "    \"data/players_*.csv\", sep=\",\", inferSchema=True, header=True, multiLine=True\n",
    ")\n",
    "\n",
    "legacy_df = spark.read.csv(\n",
    "    \"data/scraped_players_*.csv\", sep=\",\", inferSchema=True, header=True, \n",
    "    multiLine=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing football players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to focus the project on the major European leagues, it is useful to define a list of leagues to filter, and also to define a list of season to easily discriminate between modern an legacy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the European Leagues supported by Big-Data-FC\n",
    "leagues = [\n",
    "    \"Italian Serie A\",\n",
    "    \"Spain Primera Division\",\n",
    "    \"German 1. Bundesliga\",\n",
    "    \"French Ligue 1\",\n",
    "    \"English Premier League\",\n",
    "    \"Holland Eredivisie\",\n",
    "]\n",
    "\n",
    "# These are the seasons supported by Big-Data-FC\n",
    "seasons_modern = [\"20\", \"19\", \"18\", \"17\", \"16\", \"15\", \"14\"] \n",
    "seasons_legacy = [\"13\", \"12\", \"11\", \"10\", \"09\", \"08\", \"07\"]\n",
    "\n",
    "seasons = seasons_legacy + seasons_modern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next definition is about **macro roles**, which is a custom-defined abstration to **aggregate affine football roles**.\n",
    "\n",
    "For example, all the midfield roles such as \"central midfielder\", \"advanced midfielder\", \"left|right wing\" can be **grouped together** in the same macro role \"midfielder\".\n",
    "\n",
    "Macro roles will be used later on, in a subsequent learning phase. For this reason, much more about them will be touched in future points.\n",
    "\n",
    "The following cell defines the actual aggregation from FIFA roles abbreviations into macro roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_roles = [\"0.0\", \"1.0\", \"2.0\", \"3.0\"]\n",
    "\n",
    "roles_to_macro_roles_dict = {\n",
    "    \"GK\": \"0\",\n",
    "    \"SW\": \"1\",\n",
    "    \"LB\": \"1\",\n",
    "    \"RB\": \"1\",\n",
    "    \"RWB\": \"1\",\n",
    "    \"LWB\": \"1\",\n",
    "    \"CB\": \"1\",\n",
    "    \"CDM\": \"2\",\n",
    "    \"CM\": \"2\",\n",
    "    \"RM\": \"2\",\n",
    "    \"LM\": \"2\",\n",
    "    \"CAM\": \"2\",\n",
    "    \"RW\": \"3\",\n",
    "    \"LW\": \"3\",\n",
    "    \"ST\": \"3\",\n",
    "    \"LF\": \"3\",\n",
    "    \"RF\": \"3\",\n",
    "    \"CF\": \"3\",\n",
    "}\n",
    "\n",
    "NUM_MACRO_ROLES = 4\n",
    "\n",
    "roles_to_macro_role_UDF = udf(\n",
    "    lambda roles: float(\n",
    "        roles_to_macro_roles_dict[roles.split(\",\")[0]]\n",
    "    ), \n",
    "    StringType()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the columns associeted to players also contain graphical or data that is generally not informative for the project's purpose (such as shirt number, celebration moves, etc), a list of meaningful columns has been defined, on which the actual working dataframes will be based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"short_name\",\n",
    "    \"club_name\",\n",
    "    \"league_name\",\n",
    "    \"season\",\n",
    "    \"player_positions\",\n",
    "    \"macro_role\",\n",
    "    \"overall\",\n",
    "    \"value\",\n",
    "    \"pace\",\n",
    "    \"shooting\",\n",
    "    \"passing\",\n",
    "    \"dribbling\",\n",
    "    \"defending\",\n",
    "    \"physic\",\n",
    "    \"attacking_crossing\",\n",
    "    \"attacking_finishing\",\n",
    "    \"attacking_heading_accuracy\",\n",
    "    \"attacking_short_passing\",\n",
    "    \"skill_dribbling\",\n",
    "    \"skill_fk_accuracy\",\n",
    "    \"skill_long_passing\",\n",
    "    \"skill_ball_control\",\n",
    "    \"movement_acceleration\",\n",
    "    \"movement_sprint_speed\",\n",
    "    \"movement_reactions\",\n",
    "    \"power_shot_power\",\n",
    "    \"power_stamina\",\n",
    "    \"power_strength\",\n",
    "    \"power_long_shots\",\n",
    "    \"mentality_aggression\",\n",
    "    \"mentality_penalties\",\n",
    "    \"defending_standing_tackle\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets do **not** explicitly include the **year** (season) to which the record refers to.\n",
    "\n",
    "Rather, this information is implicitly stored in a URL (also for the non-scraped ones, which still originate from the same source), which has its own field.\n",
    "For this reason, a function to extract such information from aforementioned field is needed.\n",
    "\n",
    "As an example, a if the URL is `/player/41236/zlatan-ibrahimovic/130034/`, the corresponding season is `13` (from `/13xxxx/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(url):\n",
    "    url_split = url.split(\"/\")\n",
    "\n",
    "    # FIFA years must be scaled by a negative factor of one (i.e. 2021 has to be 2020, etc.)\n",
    "    # This is needed to ensure compatibility with the seasonal score dataset\n",
    "    return str(\n",
    "        (int(url_split[-2 if url_split[-1] == \"\" else -1][0:2]) - 1)\n",
    "    ).zfill(2)\n",
    "\n",
    "get_season_UDF = udf(lambda url: get_season(url), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the **monetary value** of players is different among the modern and legacy dataset.\n",
    "\n",
    "Specifically, the legacy one abbreviates the values into the form `€10M` to represent `€10000000`.\n",
    "\n",
    "The following function is used to convert it into the extended one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def expand_value_UDF(value):\n",
    "    value = value.replace(\"€\", \"\")\n",
    "    if value[-1] not in (\"K\", \"M\"):\n",
    "        # no abbreviation at the end\n",
    "        return float(value) + 0.0000001\n",
    "\n",
    "    # extract the number and the unit\n",
    "    num = value[:-1]\n",
    "    unit = value[-1]\n",
    "\n",
    "    # decide based on the unit\n",
    "    if unit == \"M\":\n",
    "        return float(num) * 1000000\n",
    "    if unit == \"K\":\n",
    "        return float(num) * 1000\n",
    "\n",
    "    return \"ERROR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the actual **pre-processing**.\n",
    "\n",
    "Some actions are needed by legacy and modern both, whilst other are exclusive to either one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting season from the player URL, as per previous cell\n",
    "pre_processed_modern_df = modern_df.withColumn(\n",
    "    \"season\", get_season_UDF(col(\"player_url\"))\n",
    ")\n",
    "pre_processed_legacy_df = legacy_df.withColumn(\n",
    "    \"season\", get_season_UDF(col(\"player_url\"))\n",
    ")\n",
    "\n",
    "# Taking only the players playing for teams in supported Leagues, \n",
    "# in the supported seasons\n",
    "pre_processed_modern_df = pre_processed_modern_df.where(\n",
    "    (pre_processed_modern_df.league_name.isin(leagues))\n",
    "    &\n",
    "    (pre_processed_modern_df.season.isin(seasons_modern))\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.where(\n",
    "    (pre_processed_legacy_df.league_name.isin(leagues))\n",
    "    &\n",
    "    (pre_processed_legacy_df.season.isin(seasons_legacy))\n",
    ")\n",
    "\n",
    "# Dropping duplicate players\n",
    "pre_processed_modern_df = pre_processed_modern_df.dropDuplicates([\"player_url\"])\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.dropDuplicates([\"player_url\"])\n",
    "\n",
    "# Selected columns have been checked for absence of null/missing data.\n",
    "# Nevertheless, to ensure compatibility and reusability with other datasets, \n",
    "# a null-filling sweep is done\n",
    "pre_processed_modern_df = pre_processed_modern_df.na.fill(0)\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.na.fill(0)\n",
    "\n",
    "# Getting the macro role of the player, according to its field position\n",
    "pre_processed_modern_df = pre_processed_modern_df.withColumn(\n",
    "    \"macro_role\", roles_to_macro_role_UDF(col(\"player_positions\"))\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumn(\n",
    "    \"macro_role\", roles_to_macro_role_UDF(col(\"player_positions\"))\n",
    ")\n",
    "\n",
    "# Renaming the \"value_eur\" field to \"value\" to have compatiblity with legacy\n",
    "pre_processed_modern_df = pre_processed_modern_df.withColumnRenamed(\n",
    "    \"value_eur\", \"value\"\n",
    ")\n",
    "\n",
    "# Convert the monetary value to have compatibility with modern\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumn(\n",
    "    \"value\", expand_value_UDF(col(\"value\"))\n",
    ")\n",
    "\n",
    "# Renaming some legacy columns, so as they have the same name as in the modern\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"pas\", \"passing\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"dri\", \"dribbling\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.drop(col(\"defending\"))\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"def\", \"defending\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"phy\", \"physic\"\n",
    ")\n",
    "\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"sho\", \"shooting\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"pac\", \"pace\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"bov\", \"overall\"\n",
    ")\n",
    "\n",
    "# Keeping only the needed columns.\n",
    "pre_processed_modern_df = pre_processed_modern_df.select(columns)\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.select(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking** whether some **monetary values** have not been successfully converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_processed_legacy_df.select(\"value\").where(col(\"value\") == \"ERROR\").count() > 0:\n",
    "    print(\"WARN: some abbreviated monetary values were not correctly expanded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing, both dataframes have the same set of columns, so they can be **concatenated** together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_df = pre_processed_modern_df.unionByName(\n",
    "    pre_processed_legacy_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del legacy_df\n",
    "del modern_df\n",
    "del pre_processed_legacy_df\n",
    "del pre_processed_modern_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the **end result** of this section.\n",
    "\n",
    "For graphical reasons, only a selection of the columns will be shown, just to give an idea of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_df.select(*columns[0:10]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete list of columns is (again for graphical reasons) printed here in an horizontal format, where each item is a tuple of the form `(field_name, type)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pre_processed_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building football teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_teams_df = pre_processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having pre-processed the football players, it's time to build the football teams.\n",
    "\n",
    "For the sake of the learning stage of the project, **teams are differentiated across different seasons**; for example, Real Madrid of 2020 is **different** than Real Madrid of 2018.\n",
    "\n",
    "A football team is then modeled as **the set of the averages of the features of all of its football players**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that are considered as features\n",
    "PLAYER_FEATURES = [\n",
    "    \"overall\",\n",
    "    \"value\",\n",
    "    \"pace\",\n",
    "    \"shooting\",\n",
    "    \"passing\",\n",
    "    \"dribbling\",\n",
    "    \"defending\",\n",
    "    \"physic\",\n",
    "    \"attacking_crossing\",\n",
    "    \"attacking_finishing\",\n",
    "    \"attacking_heading_accuracy\",\n",
    "    \"attacking_short_passing\",\n",
    "    \"skill_dribbling\",\n",
    "    \"skill_fk_accuracy\",\n",
    "    \"skill_long_passing\",\n",
    "    \"skill_ball_control\",\n",
    "    \"movement_acceleration\",\n",
    "    \"movement_sprint_speed\",\n",
    "    \"movement_reactions\",\n",
    "    \"power_shot_power\",\n",
    "    \"power_stamina\",\n",
    "    \"power_strength\",\n",
    "    \"power_long_shots\",\n",
    "    \"mentality_aggression\",\n",
    "    \"mentality_penalties\",\n",
    "    \"defending_standing_tackle\"\n",
    "]\n",
    "\n",
    "# Apposing the avg pre-fix to features\n",
    "PLAYER_FEATURES_AVG = [\n",
    "    \"avg(\" + player_feature + \")\" for player_feature in PLAYER_FEATURES\n",
    "]\n",
    "\n",
    "# Target variable of the learning stage\n",
    "TARGET_VARIABLE = \"points\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composing the football team, as introduced before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_teams_df = football_teams_df.select(\n",
    "    \"season\", \"club_name\", *PLAYER_FEATURES\n",
    ").groupBy(\n",
    "    [\"season\", \"club_name\"]\n",
    ").agg(\n",
    "    { player_feature: \"avg\" for player_feature in PLAYER_FEATURES }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the **end result** of this section.\n",
    "\n",
    "For graphical reasons, only a selection of the columns will be shown, just to give an idea of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_teams_df.select(\n",
    "    \"season\", \"club_name\", *PLAYER_FEATURES_AVG[0:5]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete list of columns with their type is (in a compact horizontal form):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(football_teams_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading football teams seasonal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having dealt with the football players and composed them into football clubs, it's time to get **\"target\" data** for the learning stage: the final ranking, for every team of every year.\n",
    "\n",
    "First step is to load the data from disk, which has been taken from the [European Football Dataset](https://www.kaggle.com/datasets/josephvm/european-club-football-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_scores_df = (\n",
    "    spark.read.csv(\n",
    "        \"data/all_tables_fixed_renamed_leagues.csv\",\n",
    "        sep=\",\",\n",
    "        inferSchema=True,\n",
    "        header=True,\n",
    "        multiLine=True,\n",
    "    )\n",
    "    .withColumnRenamed(\"Year\", \"season\")\n",
    "    .withColumnRenamed(\"Team\", \"club_name\")\n",
    "    .withColumnRenamed(\"P\", \"points\")\n",
    "    .withColumnRenamed(\"Place\", \"place\")\n",
    "    .withColumnRenamed(\"League\", \"league\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing football teams seasonal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all the available columns, only the ones in `seasonal_scores_columns` will be taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_scores_columns = [\n",
    "    \"season\", \"league\", \"club_name\", \"points\", \"place\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seasonal scores dataset uses **abbreviated version of football team names** (for example: `BAR` for Barcelona, `LEI` for Leicester, etc.), which would cause **incompatibility** with modern and legacy FIFA datasets, which instead uses complete names.\n",
    "\n",
    "Furthermore, the abbreviations are **not standard**, so there are **conflicts** such as, among others:\n",
    "\n",
    "* `BAR` both for Barcelona and Bari (different leagues).\n",
    "* `HUE` both for Huelva and Huesca (same league).\n",
    "\n",
    "For this reason, a **custom** hand-made **mapping** procedure has been developed in order to resolve such conflicts.\n",
    "\n",
    "Another source of incompatibility originated from **inconsistencies** within FIFA datasets, from year to year; for example, some teams had slight variations in their names (e.g. Torino and Torino FC).\n",
    "\n",
    "All said inconsistencies have been **manually solved** and disambiguated at dataset-level.\n",
    "\n",
    "Furthermore, the final **mapping** between abbreviations and FIFA names resulted into a **custom-made dataset** (also available in the linked dataset GitHub repo), which is the following form:\n",
    "\n",
    "|abbr|league|club_name|fifa_club_name|\n",
    "|---|---|---|---|\n",
    "|AAC|German Bundesliga|Aachen|Alemannia Aachen|\n",
    "|ADO|Dutch Eredivisie|Ado Den Haag|ADO Den Haag|\n",
    "|AJA|Dutch Eredivisie|Ajax|Ajax|\n",
    "|AJC|French Ligue 1|Ajaccio|AC Ajaccio|\n",
    "|ALB|Spanish La Liga|Albacete|Albacete BP|\n",
    "|...|...|...|...|\n",
    "\n",
    "Aforementioned mapping is then converted to **JSON** with an utility script in the same repository, and loaded here in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/clubs_map.json\")\n",
    "club_name_abbr_to_ext = json.load(f)\n",
    "f.close()\n",
    "\n",
    "ABBREVIATED_CLUB_NAME_NOT_FOUD = \"ABBREVIATED_CLUB_NAME_NOT_FOUD\"\n",
    "GENERAL_EXCEPTION = \"GENERAL_EXCEPTION\"\n",
    "\n",
    "def extend_club_name(club_name_abbr):\n",
    "    try:\n",
    "        return club_name_abbr_to_ext[club_name_abbr]\n",
    "    except KeyError as e:\n",
    "        return ABBREVIATED_CLUB_NAME_NOT_FOUD\n",
    "    except Exception as e:\n",
    "        return GENERAL_EXCEPTION\n",
    "\n",
    "extend_club_name_UDF = udf(\n",
    "    lambda club_name_abbr: extend_club_name(str(club_name_abbr)),\n",
    "    StringType(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rankings dataset the seasons are expressed as `YYYY`, whilst FIFA uses the `YY` encoding.\n",
    "\n",
    "For this reason, to guarantee compatibility, season values in the rankings dataset is abbreviated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviate_season_UDF = udf(\n",
    "    lambda season: str(season)[-2:],\n",
    "    StringType(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the actual **data pre-processing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_seasonal_scores_df = seasonal_scores_df\n",
    "\n",
    "# Abbreviating season, as per previous cell\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"season\", abbreviate_season_UDF(col(\"season\"))\n",
    ")\n",
    "\n",
    "# Keeping only supported leagues in supported seasons\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.where(\n",
    "    (pre_processed_seasonal_scores_df.season.isin(seasons))\n",
    "    & \n",
    "    (pre_processed_seasonal_scores_df.league.isin(leagues))\n",
    ")\n",
    "\n",
    "# Selecting only the desired columns\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.select(\n",
    "    seasonal_scores_columns\n",
    ")\n",
    "\n",
    "# Although data has been checked for duplicates and missing value, to ensure \n",
    "# operabiloty with other datasets, the pre-processing steps are still performed\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.dropDuplicates(\n",
    "    seasonal_scores_columns\n",
    ")\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.na.fill(0)\n",
    "\n",
    "# Extending club names, as per previous cell\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"club_name\", extend_club_name_UDF(col(\"club_name\"))\n",
    ")\n",
    "# Checking whether club name expansions went all good or not\n",
    "if pre_processed_seasonal_scores_df.filter(\n",
    "    col(\"club_name\") == ABBREVIATED_CLUB_NAME_NOT_FOUD\n",
    ").count() > 0:\n",
    "    print(\"WARN: some clubs have NOT been found\")\n",
    "    print(\"Please check your data\")\n",
    "\n",
    "# Casting points to float, as required by learning procedures\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"points\", pre_processed_seasonal_scores_df.points.cast(DoubleType())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result of this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_seasonal_scores_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining football teams features with their seasonal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two dataframes (players and seasonal scores) need to be **merged together** to form a **single dataframe**.\n",
    "This can easily be done by joining on the key (`season`, `club_name`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = football_teams_df.join(\n",
    "    pre_processed_seasonal_scores_df,\n",
    "    on=[\"season\", \"club_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to **check** whether some clubs were left out by the aforementioned join operation, two differences are computed:\n",
    "\n",
    "* `pre_processed_seasonal_scores_df - df`\n",
    "* `football_teams_df - df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = pre_processed_seasonal_scores_df.select(\"club_name\").subtract(df.select(\"club_name\")).distinct()\n",
    "\n",
    "if diff.count() > 0:\n",
    "    print(\"WARN: Some football teams have been left out the join (pre_processed_seasonal_scores_df)\")\n",
    "    diff.show()\n",
    "\n",
    "diff = football_teams_df.select(\"club_name\").subtract(df.select(\"club_name\")).distinct()\n",
    "if diff.count() > 0:\n",
    "    print(\"WARN: Some football teams have been left out the join (football_teams_df)\")\n",
    "    diff.show()\n",
    "\n",
    "del diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result of this section.\n",
    "\n",
    "For graphical reasons, only a selection of the columns will be shown, just to give an idea of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "    \"season\", \"league\", \"club_name\", \"avg(overall)\", \"avg(pace)\", \"points\", \"place\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full list of columns with their type is (in a compact horizontal form):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform **classification**, we need to transform the continuous variable `points` (on which regression will be performed) into a **categorical variable**, which will be called **`macro_place`**.\n",
    "\n",
    "The idea behind `macro_place` is pretty simple: it consists in a **partition** of the possible **table rankings**, into zones.\n",
    "Colloquially, these zones are usually referred to as:\n",
    "* Champions League Spots.\n",
    "* Europa League Spots.\n",
    "* Mid-high table.\n",
    "* Mid-low table.\n",
    "* Relegation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MACRO_PLACES = 5\n",
    "\n",
    "def get_macro_place(place, league, complex=False):\n",
    "    if league in [\"Dutch Eredivisie\", \"German Bundesliga\"]:\n",
    "        if 1 <= place <= 3:\n",
    "            return 0.0\n",
    "        if 4 <= place <= 6:\n",
    "            return 1.0\n",
    "        if 7 <= place <= 10:\n",
    "            return 2.0\n",
    "        if 11 <= place <= 15:\n",
    "            return 3.0\n",
    "        if 16 <= place <= 18:\n",
    "            return 4.0\n",
    "    else:\n",
    "        if 1 <= place <= 4:\n",
    "            return 0.0\n",
    "        if 5 <= place <= 8:\n",
    "            return 1.0\n",
    "        if 9 <= place <= 12:\n",
    "            return 2.0\n",
    "        if 13 <= place <= 16:\n",
    "            return 3.0\n",
    "        if 17 <= place <= 20:\n",
    "            return 4.0\n",
    "\n",
    "    return None\n",
    "    \n",
    "get_macro_place_UDF = udf(\n",
    "    lambda place, league, complex: get_macro_place(float(place), league, complex),\n",
    "    DoubleType(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"macro_place\", get_macro_place_UDF(col(\"place\"), col(\"league\"), lit(False))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del seasonal_scores_df\n",
    "del pre_processed_seasonal_scores_df\n",
    "del football_teams_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good practice whenever a data-driven task is being taclked is to first visualize the data.\n",
    "\n",
    "In fact, even just by simply looking at data we may find some useful information which may have a direct impact on the subsequent learning phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points distribution\n",
    "\n",
    "Starting off with **data distirbutions** of the end-of-season points, across all supported seasons, for all supported leagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.displot(df.select(\"points\").toPandas(), kde=True)\n",
    "p.fig.tight_layout(pad=1.5)\n",
    "plt.title(\"Points distribution across all leagues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations can be done:\n",
    "\n",
    "1. Data tends to approximatively follow a Gaussian/Normal distribution\n",
    "2. Said distribution appears to be slighlty skewed towards left\n",
    "\n",
    "These observations perfectly coincide with **domain knowledge**:\n",
    "\n",
    "1. In every league, there are just a few very strong teams (Champions League qualifiers), a limited number of very bad teams (fighting for relegation) and then a multitude of middle-table teams\n",
    "2. Some leagues do not have much talent in the middle part of the table, resulting in a general \"equilibrium\" between such teams.\n",
    "\n",
    "As a result, scores of such teams not be high, but rather close to the mean value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following **density plot** focuses instead on each league, showing again the points distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.displot(df.toPandas(), x=\"points\", hue=\"league\", kind=\"kde\", aspect=1.7, palette=\"tab10\")\n",
    "p.fig.tight_layout(pad=1.5)\n",
    "plt.title(\"Points distribution per league\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations can be made:\n",
    "\n",
    "* The **French** and **Spanish** leagues are the less skewed among all leagues, possibly meaning that they are characterized by a majority of \"average\" teams.\n",
    "* The **Italian** and **English** leagues are very similar on low and middle points, but start to behave slightly different on the middle-high and high parts of the table.\n",
    "* The **Dutch** league is the most left-skewed, possibly hinting at the lower quality of the league, as suggested by the domain knowledge.\n",
    "* The **Spanish** league has a boost on high points, hinting at a consistent dominancy of one or more teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall player quality distribution\n",
    "\n",
    "Similar visualizations can be done to analyze the overall player quality.\n",
    "\n",
    "The following plot shows the distribution of the player quality (`avg(overall)`) across all leagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.displot(df.select(\"avg(overall)\").toPandas(), kde=True)\n",
    "p.fig.tight_layout(pad=1.5)\n",
    "plt.title(\"Player quality across all leagues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarily to what observed for the points, the plot shows that:\n",
    "\n",
    "* A vast majority of the players are of average quality.\n",
    "* A minority of the players are below-average.\n",
    "* A minority of the players are above-average.\n",
    "\n",
    "The following plot focuses on the single leagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.displot(df.toPandas(), x=\"avg(overall)\", hue=\"league\", kind=\"kde\", aspect=1.7, palette=\"tab10\")\n",
    "p.fig.tight_layout(pad=1.5)\n",
    "plt.title(\"Player quality per league\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot it is possible to observe that:\n",
    "\n",
    "* The **Dutch** league hosts the **majority** of **below-average** players.\n",
    "* The **French** league follows the Dutch one in having **below-average** players.\n",
    "* The **German** league has the **highest** amount of **average** players.\n",
    "* The **German**, **Engligh**, **Italian** and **Spanish** leagues have a peak of **average** players.\n",
    "* The same leagues have a considerable amount of **above-average** players.\n",
    "* The **Spanish** leagues **dominates** on the **above-average** players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1: \"naive\" player features\n",
    "\n",
    "This attempt has been named \"naive\" because it simply uses **all the features** that are related to a **player's technical abilities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that do not inherently represent players abilities\n",
    "ALL_FEATURES = PLAYER_FEATURES_AVG\n",
    "ALL_FEATURES.remove(\"avg(overall)\")\n",
    "ALL_FEATURES.remove(\"avg(value)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning procedures require that the considered features are assembled in a vector.\n",
    "\n",
    "The following cell performs this operation by adding a new \"assembled\" column to the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=ALL_FEATURES, outputCol=\"all_vec\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before learning, it is good practice to first gather in-depth information on the structure of the data, as well as obtaining useful visualizations.\n",
    "\n",
    "In the following sections, additional **data visualizations** will be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section a first analysis is performed on _raw_ data, meaning that **no normalization nor standardization** is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color to be used in the plots for raw data\n",
    "COLOR_RAW = \"#332FD0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Pandas for easy plotting with Seaborn\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **feature-target correlation** shows the correlation between each feature and the target variable (`points`).\n",
    "\n",
    "The idea is to immediately visualize whether there are specific features that are particularly correlated with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES, TARGET_VARIABLE, color=COLOR_RAW\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the huge concentration of datapoints, it is still possible to see that there is **no** big amount of **outliers**, considering that 1.7k points are plotted.\n",
    "\n",
    "Let's see how visualizations change when we focus on a **specific season**, thus restricting the data scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf[pdf[\"season\"] == \"20\"], ALL_FEATURES, TARGET_VARIABLE, color=COLOR_RAW\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that:\n",
    "\n",
    "1. There is a general phenomenon of linear relation between each feature and the final points.\n",
    "2. There is a generalized **high variance** across all features.\n",
    "   1. E.g. taking the very last feature (`avg(defending_standing_tackle`), players with a value close to `50` result to a wide range of points.\n",
    "   2. On the contrary, in extreme values (close to `40` or `90`), the resulting variable have a narrower range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the distribution of the player features in the analyzed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf, \n",
    "    ALL_FEATURES, \n",
    "    color = COLOR_RAW,\n",
    "    figsize=(10,20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot it is possible to observe that:\n",
    "\n",
    "1. Most features have a symmetrical distribution.\n",
    "2. Most features are centered around the middle.\n",
    "3. Some features are more skewed towards left, hinting that they may be more \"rare\".\n",
    "4. Some features don't properly follow a Gaussian distribution (`avg(pace)`, `avg(physic)`).\n",
    "\n",
    "Furthermore, observation `2` is linked to the observation `2.2` of the previous plot:\n",
    "\n",
    "* A lot of features have an average value.\n",
    "* Around the average values, there is high variance.\n",
    "\n",
    "Hence, most likely, the system will be exposed multiple times to this issue, affecting performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Pearson's Correlation Matrix** is a graphical tool to show a numerical value indicating the correlation between each variable.\n",
    "\n",
    "It is employed in the project, to find whether some features are correlated to each other, which can either be a problem for Linear Regression, but not so much for Tree-based models, since they by design perform a feature selection step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(pdf, ALL_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of features are **positively correlated** to each other (red).\n",
    "\n",
    "Using some domain knowledge, it is possible to further comment some correlations, like (among others):\n",
    "\n",
    "* The more a defender is good, the more aggressive they are.\n",
    "* The more a defender is good, the more able to perform standing tackles they are.\n",
    "* The more a player has good physical strenght, the more fast (pace) they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have some features with **skewed distributions**, it is woth a try to **standardize**, to see whether it helps to center feature distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is needed to define a scaler which takes feature from `all_vec` and places scaled versions in `all_vec_std`.\n",
    "\n",
    "The configuration is a standard **z-score normalization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"all_vec\", \n",
    "    outputCol=\"all_vec_std\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_STD = \"#9254C8\"\n",
    "\n",
    "ALL_FEATURES_STD = [\n",
    "    player_feature + \"_std\" for player_feature in ALL_FEATURES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the scaler grouped all features in a single field (feature vector), but plotting requires using different fields (one per features), it is needed to define a function to extract each feature and place it in a separate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vec_to_cols(pdf, vec, columns):\n",
    "    tmp = pdf.reindex(\n",
    "        columns=list(pdf.columns) + columns\n",
    "    )\n",
    "\n",
    "    tmp[columns] = tmp[\n",
    "        vec\n",
    "    ].transform(\n",
    "        {\n",
    "            columns[i]: operator.itemgetter(i) for i, p in enumerate(columns)\n",
    "        }\n",
    "    )\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = feature_vec_to_cols(pdf, \"all_vec_std\", ALL_FEATURES_STD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see the effects of the function by looking at the columns list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As done in the _raw data_ section, a feature-target correlation analysis is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_STD, TARGET_VARIABLE, color=COLOR_STD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again zoom on a single season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf[pdf[\"season\"] == \"20\"], ALL_FEATURES_STD, TARGET_VARIABLE, color=COLOR_STD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization does not improve the variance issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf, ALL_FEATURES_STD, color=COLOR_STD, figsize=(10,20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization did not change the feature distribution.\n",
    "\n",
    "It is not needed to plot the correlation matrix again, since it is not affected by a change of scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further attempt consists of apply Logarithmic Transformation, still trying to combat skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_LOG = \"#E15FED\"\n",
    "ALL_FEATURES_LOG = [\n",
    "    player_feature + \"_log\" for player_feature in ALL_FEATURES\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function translates a value to its $log_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_log_UDF = udf(\n",
    "    lambda value: float(np.log2(value)), DoubleType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, feature_log in zip(ALL_FEATURES, ALL_FEATURES_LOG):\n",
    "    df = df.withColumn(feature_log, to_log_UDF(col(feature)))\n",
    "\n",
    "df = df.withColumn(\"points_log\", to_log_UDF(col(\"points\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_LOG, \"points_log\", color=COLOR_LOG, figsize=(10,20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also Logarithmic transformation did not help with the variance problem.\n",
    "\n",
    "Let's again see a zommed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf[pdf[\"season\"] == \"20\"], ALL_FEATURES_LOG, \"points_log\", color=COLOR_LOG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithmic transformation did not help with variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES_LOG, color=COLOR_LOG, figsize=(10,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithmic transformation slightly moved the skewdness of all features towards right, still not solving it; as a consequence, centered or right-skewed features are not slightly worsened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-max transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_MIN_MAX = \"#6EDCD9\"\n",
    "ALL_FEATURES_MIN_MAX = [\n",
    "    player_feature + \"_min_max\" for player_feature in ALL_FEATURES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"all_vec\", \n",
    "    outputCol=\"all_vec_min_max\"\n",
    ")\n",
    "\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()\n",
    "pdf = feature_vec_to_cols(pdf, vec=\"all_vec_min_max\", columns=ALL_FEATURES_MIN_MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_MIN_MAX, TARGET_VARIABLE, color=COLOR_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoomed view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf[pdf[\"season\"] == \"20\"], ALL_FEATURES_MIN_MAX, TARGET_VARIABLE, color=COLOR_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES_MIN_MAX, color=COLOR_MIN_MAX, figsize=(10,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the different transformations\n",
    "\n",
    "The following GIF compares the previously plotted feature distributions.\n",
    "\n",
    "![GIF changes illustration](https://s8.gifyu.com/images/ezgif.com-gif-maker4847ef0cb3270edd.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before learning, some further useful visualizations are given.\n",
    "\n",
    "**From now on, Min-Max transofrmed data will be used.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the data space, dimensionality reduction via **PCA** (Principal Component Analysis) has been conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, pca_model = perform_pca(\n",
    "    df=df,\n",
    "    num_components=PCA_NUM_COMPONENTS,\n",
    "    input_col=\"all_vec_min_max\",\n",
    "    output_col=\"all_vec_min_max_pcs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next plot shows the proportion of variance that the two principal components capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_explained_variance(\n",
    "    pca_model=pca_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cumulatively, the two components capture almost the 100% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the feature space resulting from PCA is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=pdf.all_vec_min_max_pcs.map(lambda x: x[0]),\n",
    "    y=pdf.all_vec_min_max_pcs.map(lambda x: x[1]),\n",
    "    c=pdf.points,\n",
    "    x_label=\"Principal Component 0\",\n",
    "    y_label=\"Principal Component 1\",\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA result is quite underwhelming: teams with widely different end-of-the-season placement tend to be mixed-up, meaning that data appear to be **non-linearly separable**.\n",
    "\n",
    "The following plots put in relation each component to the target variable (`points`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=pdf.all_vec_min_max_pcs.map(lambda x: x[0]),\n",
    "    y=pdf.points,\n",
    "    x_label=\"Principal Component 0\",\n",
    "    y_label=\"Points\",\n",
    ")\n",
    "scatter_plot(\n",
    "    x=pdf.all_vec_min_max_pcs.map(lambda x: x[1]),\n",
    "    y=pdf.points,\n",
    "    x_label=\"Principal Component 1\",\n",
    "    y_label=\"Points\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected from the prevous plots, there is **high variance**: for example, fixing the PC1 value, there is the whole spectrum of points on the `y` axis.\n",
    "\n",
    "Principal Component Analysis is a linear model, so it is bounded to produce linear representations: usually, non-trivial problems present non-linear data.\n",
    "\n",
    "For this reason, adding some non-linearity may possibly lead to a better lower-dimensional embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is one of the most used tools when assuming **non-linearity** in the data space.\n",
    "\n",
    "Implementation from SciKit Learn framework will be used, since PySpark does not provide any version of this tool.\n",
    "\n",
    "SciKit Learn works with NumPy structures, so the feature vector in PySpark DataFrame must be converted to a `ndarray` structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vec_min_max_np = np.array(\n",
    "    list(\n",
    "        map(\n",
    "            lambda v: v[\"all_vec_min_max\"].toArray(), \n",
    "            df.select(\"all_vec_min_max\").collect()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "points_np = np.array(\n",
    "    list(\n",
    "        map(\n",
    "            lambda v: v[\"points\"], \n",
    "            df.collect()\n",
    "        )\n",
    "    )\n",
    ").reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data is ready, TNSE is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_embedding = TSNE(\n",
    "    n_components=2, learning_rate='auto', init='random', method=\"barnes_hut\"\n",
    ").fit_transform(all_vec_min_max_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, once the embedding space is populated, the result is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=tsne_embedding[:,0],\n",
    "    y=tsne_embedding[:,1],\n",
    "    c=points_np,\n",
    "    x_label=\"TSNE Embedding Dimension 0\",\n",
    "    y_label=\"TSNE Embedding Dimension 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=tsne_embedding[:,0],\n",
    "    y=points_np,\n",
    "    x_label=\"TSNE Embedding Dimension 0\",\n",
    "    y_label=\"Points\",\n",
    ")\n",
    "scatter_plot(\n",
    "    x=tsne_embedding[:,1],\n",
    "    y=points_np,\n",
    "    x_label=\"TSNE Embedding Dimension 1\",\n",
    "    y_label=\"Points\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, adding non-linearity does not seem to improve the situation.\n",
    "\n",
    "Not much progress has been made, w.r.t. PCA:\n",
    "\n",
    "* **High variance** is still present.\n",
    "* The data space differs, but it is still of low quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final recap on visualizations\n",
    "\n",
    "Data seems to be:\n",
    "\n",
    "* Not linearly separable.\n",
    "* Suffering from high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning for attempt 1\n",
    "\n",
    "The problem has been treated in two different forms:\n",
    "\n",
    "* As **regression** on the **points**.\n",
    "* As **classification** on the table/ranking areas (macro places).\n",
    "\n",
    "In both cases, for all the different models will be used:\n",
    "\n",
    "* **K-Fold Cross Validation**, to perform validation tests on the model performances.\n",
    "* **Grid-search approach**, to perform hyperparameter tuning.\n",
    "\n",
    "Generally, all models have been **trained and persisted on disk**, in order to retreive them on subsequent executions of the notebook; hence, the model's existence will first be checked before trying to re-train again.\n",
    "\n",
    "The columns regarding features, label, prediction have been set in the **parameter grid**. \n",
    "\n",
    "The parameter grid is used to perform hyperparameter search grid, using PySpark's tools. Specifically, for each model the following parameters have been searched over:\n",
    "\n",
    "* **Linear Regression**:\n",
    "  * `regParam`: regularization coefficient parameter ($\\lambda$). Different values have been tested, to understand whether adding \"importance\" to the regularization factor helps the model or not.\n",
    "  * `solver`: the solver for the optimization steps.\n",
    "  * `fitIntercept`: whether to use the bias or not.\n",
    "  * `elasticNetParam`: the parameter that controls the tradeoff between $L_1$ and $L_2$ regularizer terms of the ElasticNet regularization\n",
    "\n",
    "* **Prediction Tree**:\n",
    "  * `maxDepth`: the maximum depth of the tree. The idea is that the more levels, the more splits are done, the better the performances should be.\n",
    "  Specifically, we try the default value and the maximum possible value (i.e. the total number of features)\n",
    "  * `maxBins`: Max number of bins used when discretizing continuous features.\n",
    "  Default, half and double values have been tried.\n",
    "  * `minInfoGain`: Minimum Information Gain to consider the feature as a discriminative candidate, during the splitting phase.\n",
    "\n",
    "* **Gradient Boosted Tree**:\n",
    "  * Same as Prediction Tree\n",
    "  * `subsamplingRate`: controls the amount of data used to train each tree.\n",
    "  Default and half value have been tested.\n",
    "  * `lossType`: the loss function to use during the optimization.\n",
    "  All available losses have been used\n",
    "\n",
    "* **Random Forest**:\n",
    "  * Same as Gradient Boosted Trees, plus:\n",
    "  * `numTrees`: how many trees to place in the random forest. Default, half and double value have been tested.\n",
    "  * `featureSubsetStrategy`: how to select the features to train the forest trees on.\n",
    "  All available values have been used.\n",
    "\n",
    "* **MLP**:\n",
    "  * `layers`: layer configuration of the Neural Network.\n",
    "  Due to the amount of data and the number of features, we used a single layer, to avoid curse of dimensionality.\n",
    "  In fact, adding another layer would've increased the number of trainable parameters too much.\n",
    "  * `solver`: the solver to use in the optimization solving stage.\n",
    "  Decided to go with Gradient Descent, since the other ones result in execution problems.\n",
    "\n",
    "70/20/10 train/validation/test split has been used; since we will use 5-fold cross validation and PySpark's cross validator automatically splits data when using the folds, we need to give to it a 90/10 split, which will result in:\n",
    "\n",
    "* $90 = train + validation$\n",
    "  * $90*4/5 = 70 = train$\n",
    "  * $90*1/5 = 20 = validation$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.9, 0.1], seed=random_seed)\n",
    "\n",
    "# as per attempt description, all columns will be trained on\n",
    "FEATURES_COL = \"all_vec_min_max\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_evaluator_cv.setLabelCol(REGRESSION_LABEL_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_COL = \"attempt_1_regression_linear_regression_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Linear Regression model NOT found in disk, training...\")\n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = LinearRegression()\n",
    "\n",
    "    linear_regression_param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "        .addGrid(estimator.solver, [\"auto\", \"normal\"])\n",
    "        .addGrid(estimator.fitIntercept, [True, False])\n",
    "        .addGrid(estimator.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=linear_regression_param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Linear Regression model found in disk, loading...\")\n",
    "    model = LinearRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Linear Regression\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Evaluating Linear Regression model trained in previous cell...\")\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/regression/linear_regression.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/regression/linear_regression.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Saving Linear Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/regression/linear_regression\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import DecisionTreeRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_regression_decision_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\"Prediction Tree model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = DecisionTreeRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Decision Tree model found in disk, loading...\")\n",
    "    model = DecisionTreeRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Prediction Tree\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Prediction Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/regression/prediction_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/regression/prediction_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\"Saving Decision Tree Regressor model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/regression/prediction_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import GBTRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_regression_gradient_boosted_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\"Regression Gradient Boosted Tree model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = GBTRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [2, 5, 10])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Gradient Boosted Tree model found in disk, loading...\")\n",
    "    model = GBTRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Gradient Boosted Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/regression/gradient_boosted_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/regression/gradient_boosted_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\"Saving Gradient Boosted Tree Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/regression/gradient_boosted_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_regression_random_forest_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Regression Random Forest model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = RandomForestRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .addGrid(estimator.numTrees, [20, 40])\n",
    "        .addGrid(\n",
    "            estimator.featureSubsetStrategy, \n",
    "            [\"auto\", \"onethird\", \"all\", \"log2\"]\n",
    "        )\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Random Forest model found in disk, loading...\")\n",
    "    model = RandomForestRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Random Forest\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Random Forest model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/regression/random_forest.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/regression/random_forest.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Saving Random Forest Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/regression/random_forest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_evaluator_cv.setLabelCol(CLASSIFICATION_LABEL_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.9, 0.1], seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing `macro_place` distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    df.toPandas(), \n",
    "    [CLASSIFICATION_LABEL_COL],\n",
    "    color=\"teal\",\n",
    "    n_cols=1,\n",
    "    figsize=(8,4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class **balancement** is pretty **good**.\n",
    "\n",
    "Imbalancement would've been a problem because it could've been source of bias.\n",
    "In fact, a model that sees a class too many times, may be biased towards that class, damaging the classification accuracy on the less frequent classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note about the first two models, **SVM** and **Logistic Regression**:\n",
    "\n",
    "* PySpark treats these models as **binary** classifiers.\n",
    "* In order to use them as **multiclass** classifiers, they can be used as estimators of a **OneVsRest** model, which automatically transforms the problem from binary to multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import OneVsRestModel\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_classification_svm_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"SVM\"]\n",
    "):\n",
    "    print(\"SVM Classification model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = OneVsRest(\n",
    "        classifier=LinearSVC(\n",
    "            featuresCol=FEATURES_COL,\n",
    "            labelCol=CLASSIFICATION_LABEL_COL,\n",
    "            predictionCol=PREDICTION_COL,\n",
    "        ),\n",
    "        featuresCol=FEATURES_COL,\n",
    "        labelCol=CLASSIFICATION_LABEL_COL,\n",
    "        predictionCol=PREDICTION_COL\n",
    "    )\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"SVM Classification model found in disk, loading...\")\n",
    "    model = OneVsRestModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"SVM\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"SVM\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating SVM Classifier model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/classification/svm.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/classification/svm.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"SVM\"]\n",
    "):\n",
    "    print(\"Saving SVM Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/classification/svm\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import OneVsRestModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_classification_logistic_regression_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Logistic Regression\"]\n",
    "):\n",
    "    print(\"Logistic Regression Classification model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = OneVsRest(\n",
    "        classifier=LogisticRegression(\n",
    "            featuresCol=FEATURES_COL,\n",
    "            labelCol=CLASSIFICATION_LABEL_COL,\n",
    "            predictionCol=PREDICTION_COL,\n",
    "        ),\n",
    "        featuresCol=FEATURES_COL,\n",
    "        labelCol=CLASSIFICATION_LABEL_COL,\n",
    "        predictionCol=PREDICTION_COL\n",
    "    )\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Logistic Regression Classification model found in disk, loading...\")\n",
    "    model = OneVsRestModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Logistic Regression\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Logistic Regression\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Logistic Regression Classifier model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/classification/logistic_regression.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/classification/logistic_regression.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Logistic Regression\"]\n",
    "):\n",
    "    print(\"Saving Logistic Regression Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/classification/logistic_regression\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_classification_decision_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\"Classification Decision Tree model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = DecisionTreeClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification Decision Tree model found in disk, loading...\")\n",
    "    model = DecisionTreeClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Decision Tree\"]\n",
    "    )\n",
    "    evaluators = get_evaluators(\n",
    "        model, \n",
    "        label_col=CLASSIFICATION_LABEL_COL, \n",
    "        prediction_col=PREDICTION_COL, \n",
    "        evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Decision Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/classification/decision_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/classification/decision_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\"Saving Decision Tree model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/classification/decision_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_classification_random_forest_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Classification Random Forest model NOT found in disk, training...\")\n",
    "    \n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = RandomForestClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .addGrid(estimator.numTrees, [20, 40])\n",
    "        .addGrid(estimator.featureSubsetStrategy, [\"auto\", \"onethird\", \"all\", \"log2\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification Random Forest model found in disk, loading...\")\n",
    "    model = RandomForestClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Random Forest\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Random Forest model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/classification/random_forest.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/classification/random_forest.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Saving Random Forest Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/classification/random_forest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_1_classification_mlp_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"MLP\"]\n",
    "):\n",
    "    print(\"Classification MLP model NOT found in disk, training...\")\n",
    "    \n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = MultilayerPerceptronClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.layers, [[24, NUM_MACRO_PLACES]])\n",
    "        .addGrid(estimator.solver, [\"gd\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification MLP model found in disk, loading...\")\n",
    "    model = MultilayerPerceptronClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"MLP\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"MLP\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating MLP model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_1/classification/mlp.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_1/classification/mlp.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 1\"][\"Classification\"][\"MLP\"]\n",
    "):\n",
    "    print(\"Saving MLP Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_1/classification/mlp\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2: \"less is more\"\n",
    "\n",
    "Considering the knowledge gathered from the previous attempt, it can be useful to perform **dimensionality reduction** and to perform learning on the lower-dimensional data.\n",
    "\n",
    "It is useful for:\n",
    "\n",
    "* Reducing the **curse of dimensionality**.\n",
    "* Addressing **feature correlation**.\n",
    "\n",
    "The following dimensionality reduction methods have been used:\n",
    "\n",
    "* PCA.\n",
    "* t-SNE.\n",
    "* Univariate Feature Selection.\n",
    "* Feature selection by manually picking the player features `overall` and `value`.\n",
    "\n",
    "An initial consideration is that:\n",
    "\n",
    "* Log scaling reduces skewedness of `avg(mentality_penalties)` feature but it **increases the skewedness** of all the other features.\n",
    "* The other scalings (z-score and min-max) do NOT appear to be different than the \"raw\" data distribution.\n",
    "\n",
    "For this reason the **min-max scaling** has been used from there on. In facts, the min-max scaling places \"for free\" all the features in the same scale, which is a very important consideration for SVM, which will be used in the upcoming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import UnivariateFeatureSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Univariate Feature Selection up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = UnivariateFeatureSelector(\n",
    "    featuresCol=\"all_vec_min_max\",\n",
    "    labelCol=TARGET_VARIABLE, \n",
    "    selectionMode=\"percentile\"\n",
    ").setFeatureType(\"continuous\").setLabelType(\n",
    "    \"categorical\"\n",
    ").setSelectionThreshold(0.08)\n",
    "\n",
    "# Number of wanted features\n",
    "NUM_FEATURES = [2, 6, 12]\n",
    "\n",
    "# UFS does not accepts directly the number of desired features, but rather it\n",
    "# works with thresholds.\n",
    "# So, thresholds are computed in order to get the desired number of features \n",
    "THRESHOLDS = [num_features/24 for num_features in NUM_FEATURES]\n",
    "\n",
    "# The result of every feature selection is placed in its own column, in order to\n",
    "# being able to use them in learning of attempt 2\n",
    "UFS_FEATURES = [\n",
    "    f\"all_vec_min_max_ufs_{num_features}\" for num_features in NUM_FEATURES\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Univariate Feature Selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result = dict()\n",
    "\n",
    "for num_features, thr, ufs_features in zip(\n",
    "    NUM_FEATURES, THRESHOLDS, UFS_FEATURES\n",
    "):\n",
    "    selector.setSelectionThreshold(thr)\n",
    "    selector.setOutputCol(ufs_features),\n",
    "    fit_result[str(num_features)] = selector.fit(df)\n",
    "    df = fit_result[str(num_features)].transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the names of the features that have been selected, for every threshold.\n",
    "\n",
    "Their visualizations (feature-target correlation, feature distribution and correlation matrix) will be skipped, since they are exactly the same as in attempt 1.<br>\n",
    "In facts, UFS does not touch/modify features, but rather just selects them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = dict()\n",
    "for num_features in NUM_FEATURES:\n",
    "    selected_features[str(num_features)] = list(\n",
    "        map(\n",
    "            lambda i: ALL_FEATURES[i], fit_result[str(num_features)].selectedFeatures\n",
    "        )\n",
    "    )\n",
    "    print(f\"Univariate Feature Selection selected these {num_features} features:\\n{selected_features[str(num_features)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data space resulting from UFS (2 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()\n",
    "\n",
    "scatter_plot(\n",
    "    x=pdf[\"all_vec_min_max_ufs_2\"].map(lambda x: x[0]),\n",
    "    y=pdf[\"all_vec_min_max_ufs_2\"].map(lambda x: x[1]),\n",
    "    c=pdf[\"points\"],\n",
    "    x_label=\"Feature 0\",\n",
    "    y_label=\"Feature 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, this data space is **not linearly separable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall as feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features are highly correlated and yield bad learning performances, the `overall` player ability has been tried as a **feature**, in the hypotesis that it captures some hidden characteristics that may improve the performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark steps (assembling into vector, min-max scaling and decomposition into Pandas DF for plotting reasons) are the same as previous cases, so will not be commented once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERALL = [\"avg(overall)\"]\n",
    "OVERALL_MIN_MAX = [\"avg(overall)_min_max\"]\n",
    "\n",
    "COLOR_OVERALL_MIN_MAX = \"green\"\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=OVERALL, outputCol=\"overall_vec\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"overall_vec\", \n",
    "    outputCol=\"overall_vec_min_max\"\n",
    ")\n",
    "\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()\n",
    "\n",
    "pdf = feature_vec_to_cols(\n",
    "    pdf=pdf,\n",
    "    vec=\"overall_vec_min_max\",\n",
    "    columns=OVERALL_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf,\n",
    "    OVERALL_MIN_MAX,\n",
    "    TARGET_VARIABLE,\n",
    "    figsize=(10,4),\n",
    "    color=\"chocolate\",\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing new here: `overall` suffers from **high variance** as well.\n",
    "\n",
    "Our **hypotheses** of `overall` capturing something non-explicitly expressed by the other features, has been **confuted**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    OVERALL_MIN_MAX,\n",
    "    color=\"chocolate\",\n",
    "    figsize=(10,4),\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of overall is in line with all the other features, it follows an almost-perfectly centered in the mean Normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value as feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar reasoning has been applied to the `value` feature.\n",
    "\n",
    "Theoretically:\n",
    "* the stronger the players in a team, the higher their value, the better the team, the higher the points.\n",
    "* \"Money talks\": whilst the overall feature may be biased (for example by the way it is computed by FIFA), the economic value of a player shouldn't, because football clubs would never overpay for a player.<br>\n",
    "\n",
    "So, the economic value could potentially be a more accurate estimate of the player quality.\n",
    "\n",
    "Let's see whether this is true or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE = [\"avg(value)\"]\n",
    "\n",
    "COLOR_VALUE_MIN_MAX = \"coral\"\n",
    "VALUE_MIN_MAX = [\"avg(value)_min_max\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=VALUE, outputCol=\"value_vec\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"value_vec\", \n",
    "    outputCol=\"value_vec_min_max\"\n",
    ")\n",
    "\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.where(col(\"season\").isin(seasons_modern)).toPandas()\n",
    "pdf = feature_vec_to_cols(\n",
    "    pdf=pdf,\n",
    "    vec=\"value_vec_min_max\",\n",
    "    columns=VALUE_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf,\n",
    "    VALUE_MIN_MAX,\n",
    "    TARGET_VARIABLE,\n",
    "    figsize=(10,4),\n",
    "    color=COLOR_VALUE_MIN_MAX,\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no tangible improvements w.r.t. previous attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    [\"avg(value)\"],\n",
    "    color=COLOR_VALUE_MIN_MAX,\n",
    "    figsize=(10,4),\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per previous plot, feature distribution is very skewed towards left, meaning that there are a lot of players with low economic value.\n",
    "\n",
    "A min-max normalization did not solve the skewness, so a $log_2$ transformation has been tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"value_log\", to_log_UDF(\"avg(value)\"))\n",
    "pdf = df.filter(col(\"season\").isin(seasons_modern)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    [\"value_log\"],\n",
    "    color=COLOR_VALUE_MIN_MAX,\n",
    "    figsize=(10,4),\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logarithmic transformation solved the skewness.<br>\n",
    "Log-transformated values will min-max normalized, in order to have the **same scale** as in previous attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"value_log\"], outputCol=\"value_log_vec\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"value_log_vec\", \n",
    "    outputCol=\"value_log_vec_min_max\"\n",
    ")\n",
    "\n",
    "df = scaler.fit(df).transform(df)\n",
    "pdf = df.filter(col(\"season\").isin(seasons_modern)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = feature_vec_to_cols(\n",
    "    pdf=pdf,\n",
    "    vec=\"value_log_vec_min_max\",\n",
    "    columns=[\"value_log_min_max\"]\n",
    ")\n",
    "\n",
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    [\"value_log_min_max\"],\n",
    "    color=COLOR_VALUE_MIN_MAX,\n",
    "    figsize=(10,4),\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = feature_vec_to_cols(\n",
    "    pdf=pdf,\n",
    "    vec=\"overall_vec_min_max\",\n",
    "    columns=OVERALL_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall-value correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domain knowledge** tells us that, usually, high amount of money are spent on supposedly very good players, whilst few money is spent on average players.\n",
    "\n",
    "Statistically speaking, this would translate into a (positive) correlation between the two variables.\n",
    "\n",
    "Let's see whether that's true or not..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(\n",
    "    pdf, \n",
    "    [\"avg(overall)_min_max\", \"value_log_min_max\"],\n",
    "    figsize=(8,4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `overall` and `value` are highly positively correlated.\n",
    "\n",
    "For this reason, `overall` will not be considered, since learning results would be very similar to the case of using value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning for attempt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same learning configuration and logic of the first attmempt has been used; only novelties will be explained.\n",
    "\n",
    "`featuresCol` will be set via the hyperparameter search grid, in order to train the various models on the results of the various feature selection techniques.<br>\n",
    "Specifically, the following feature-sets will be tested:\n",
    "* `value` (log transformed)\n",
    "* `Principal components` \n",
    "* The results of `UFS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.9, 0.1], seed=random_seed)\n",
    "\n",
    "FEATURES_COL = [\n",
    "    \"value_log_vec_min_max\", \"all_vec_min_max_pcs\"\n",
    "] + UFS_FEATURES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_evaluator_cv.setLabelCol(REGRESSION_LABEL_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_regression_linear_regression_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Linear Regression model NOT found in disk, training...\")\n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = LinearRegression()\n",
    "\n",
    "    linear_regression_param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "        .addGrid(estimator.solver, [\"auto\", \"normal\"])\n",
    "        .addGrid(estimator.fitIntercept, [True, False])\n",
    "        .addGrid(estimator.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=linear_regression_param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Linear Regression model found in disk, loading...\")\n",
    "    model = LinearRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Linear Regression\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Evaluating Linear Regression model trained in previous cell...\")\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/regression/linear_regression.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/regression/linear_regression.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Saving Linear Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/regression/linear_regression\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import DecisionTreeRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_regression_decision_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\"Prediction Tree model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = DecisionTreeRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Decision Tree model found in disk, loading...\")\n",
    "    model = DecisionTreeRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Prediction Tree\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Prediction Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/regression/prediction_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/regression/prediction_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\"Saving Decision Tree Regressor model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/regression/prediction_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import GBTRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_regression_gradient_boosted_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\"Regression Gradient Boosted Tree model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = GBTRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [2, 5, 10])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Gradient Boosted Tree model found in disk, loading...\")\n",
    "    model = GBTRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Gradient Boosted Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/regression/gradient_boosted_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/regression/gradient_boosted_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\"Saving Gradient Boosted Tree Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/regression/gradient_boosted_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_regression_random_forest_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Regression Random Forest model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = RandomForestRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .addGrid(estimator.numTrees, [20, 40])\n",
    "        .addGrid(\n",
    "            estimator.featureSubsetStrategy, \n",
    "            [\"auto\", \"onethird\", \"all\", \"log2\"]\n",
    "        )\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Random Forest model found in disk, loading...\")\n",
    "    model = RandomForestRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Random Forest\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Random Forest model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/regression/random_forest.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/regression/random_forest.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Saving Random Forest Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/regression/random_forest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_evaluator_cv.setLabelCol(CLASSIFICATION_LABEL_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import OneVsRestModel\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_classification_svm_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"SVM\"]\n",
    "):\n",
    "    print(\"SVM Classification model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = OneVsRest(\n",
    "        classifier=LinearSVC(\n",
    "            featuresCol=FEATURES_COL,\n",
    "            labelCol=CLASSIFICATION_LABEL_COL,\n",
    "            predictionCol=PREDICTION_COL,\n",
    "        ),\n",
    "        featuresCol=FEATURES_COL,\n",
    "        labelCol=CLASSIFICATION_LABEL_COL,\n",
    "        predictionCol=PREDICTION_COL\n",
    "    )\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"SVM Classification model found in disk, loading...\")\n",
    "    model = OneVsRestModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"SVM\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"SVM\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating SVM Classifier model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/classification/svm.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/classification/svm.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"SVM\"]\n",
    "):\n",
    "    print(\"Saving SVM Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/classification/svm\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import OneVsRestModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_classification_logistic_regression_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Logistic Regression\"]\n",
    "):\n",
    "    print(\"Logistic Regression Classification model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = OneVsRest(\n",
    "        classifier=LinearSVC(\n",
    "            featuresCol=FEATURES_COL,\n",
    "            labelCol=CLASSIFICATION_LABEL_COL,\n",
    "            predictionCol=PREDICTION_COL,\n",
    "        ),\n",
    "        featuresCol=FEATURES_COL,\n",
    "        labelCol=CLASSIFICATION_LABEL_COL,\n",
    "        predictionCol=PREDICTION_COL\n",
    "    )\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Logistic Regression Classification model found in disk, loading...\")\n",
    "    model = OneVsRestModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Logistic Regression\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Logistic Regression\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Logistic Regression Classifier model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/classification/logistic_regression.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/classification/logistic_regression.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Logistic Regression\"]\n",
    "):\n",
    "    print(\"Saving Logistic Regression Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/classification/logistic_regression\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_classification_decision_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\"Classification Decision Tree model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = DecisionTreeClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification Decision Tree model found in disk, loading...\")\n",
    "    model = DecisionTreeClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Decision Tree\"]\n",
    "    )\n",
    "    evaluators = get_evaluators(\n",
    "        model, \n",
    "        label_col=CLASSIFICATION_LABEL_COL, \n",
    "        prediction_col=PREDICTION_COL, \n",
    "        evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Decision Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/classification/decision_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/classification/decision_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\"Saving Decision Tree model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/classification/decision_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_2_classification_random_forest_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Classification Random Forest model NOT found in disk, training...\")\n",
    "    \n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = RandomForestClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .addGrid(estimator.numTrees, [20, 40])\n",
    "        .addGrid(estimator.featureSubsetStrategy, [\"auto\", \"onethird\", \"all\", \"log2\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification Random Forest model found in disk, loading...\")\n",
    "    model = RandomForestClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Random Forest\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Random Forest model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_2/classification/random_forest.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_2/classification/random_forest.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 2\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Saving Random Forest Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_2/classification/random_forest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 3: clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the two previous attempts did not yield to good results, what if learning is performed on **learning-produced features**?\n",
    "\n",
    "The idea is to produce the feature-set using **K-Means clustering**.\n",
    "\n",
    "After clustering is performed, everything revolves around **distance** between a player and the centroid of the cluster the player has been assigned to.<br>\n",
    "These are the leveraged **assumptions**:\n",
    "* Cluster **centroid** represents the **average** player, a player that offers no pecularity, hence no tactical advantage\n",
    "* The **higher** the distance from the centroid $\\to$ the bigger the **pecularity** $\\to$ the bigger the **tactical advantage** $\\to$ the **better** the end of the season **results**\n",
    "\n",
    "Since players must then be assembled into teams and since it is important to differentiate teams across years, clustering will be performed **for every season**, rather than considering seasons all together.\n",
    "\n",
    "Players aggregation into football teams remains as explained in the previous attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commodity method to perform K-Means clustering, expanded from the one seen in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(\n",
    "    dataset,\n",
    "    n_clusters,\n",
    "    distance_measure=\"euclidean\",\n",
    "    max_iter=20,\n",
    "    features_col=\"features\",\n",
    "    prediction_col=\"cluster\",\n",
    "    verbose=False,\n",
    "    season=-1\n",
    "):\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"\"\"Training K-means clustering using the following parameters: \n",
    "            - K (n. of clusters) = {n_clusters}\n",
    "            - max_iter (max n. of iterations) = {max_iter}\n",
    "            - distance measure = {distance_measure}\n",
    "            - season = {season}\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    kmeans = KMeans(\n",
    "        featuresCol=features_col,\n",
    "        predictionCol=prediction_col,\n",
    "        k=n_clusters,\n",
    "        initMode=\"k-means||\",\n",
    "        initSteps=5,\n",
    "        tol=0.000001,\n",
    "        maxIter=max_iter,\n",
    "        distanceMeasure=distance_measure,\n",
    "    )\n",
    "\n",
    "    model = kmeans.fit(dataset)\n",
    "    clusters_df = model.transform(dataset)\n",
    "\n",
    "    return model, clusters_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commodity method to perform K-Means clustering using a **variable number of clusters** (`k`).\n",
    "\n",
    "Expanded from the code seen in class, it adds support for our specific use case, introduced in attempt 3 presentation.<br>\n",
    "Specifically, for every football player, the centroid of its cluster is stored as an additional field.<br>\n",
    "This allows us to easily batch-compute player-cluster centroid distances, whenever needed, enforcing **PySpark**'s strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_clustering(\n",
    "    k_range, \n",
    "    input_df, \n",
    "    max_iter,\n",
    "    featuresCol,\n",
    "    clusterCol,\n",
    "    verbose=False,\n",
    "    season=-1,\n",
    "    distance_measure=\"cosine\"\n",
    "):\n",
    "    clustering_results = {}\n",
    "\n",
    "    clusters_df = input_df\n",
    "\n",
    "    for k in k_range:\n",
    "        if verbose:\n",
    "            print(f\"Running K-means using K = {k}\")\n",
    "\n",
    "        model, clusters_df = k_means(\n",
    "            clusters_df, \n",
    "            k, \n",
    "            max_iter=max_iter, \n",
    "            distance_measure=distance_measure, \n",
    "            features_col=featuresCol,\n",
    "            prediction_col=clusterCol + \"_k_\" + str(k),\n",
    "            verbose=True,\n",
    "            season=season\n",
    "        )  \n",
    "        \n",
    "        silhouette_k = evaluate_k_means(\n",
    "            clusters_df, \n",
    "            distance_measure=distance_measure,\n",
    "            prediction_col=clusterCol + \"_k_\" + str(k),\n",
    "            featuresCol=featuresCol\n",
    "\n",
    "        ) \n",
    "        wssd_k = model.summary.trainingCost\n",
    "        \n",
    "        if verbose:\n",
    "\n",
    "            print(f\"Silhouette ({distance_measure} distance): {silhouette_k}\")\n",
    "            print(f\"WSSD ({distance_measure} distance): {wssd_k}\")\n",
    "\n",
    "        # Getting all the cluster centroids\n",
    "        l = list(enumerate(model.clusterCenters()))\n",
    "\n",
    "        # Enumerating all clustering centroids, in order to get the following\n",
    "        # list of pairs (cluster_id, cluster_id_centroid).\n",
    "        # Cluster Centroid must be stored in a DenseVector, in order to being\n",
    "        # able to compute distances, as explained in attempt 3 introduction\n",
    "        l = [(ind, DenseVector(c)) for ind, c in l]\n",
    "\n",
    "        # Defining the schema of the new DataFrame, which will contain the cluster\n",
    "        # centroid as a field of every player, as explained in attempt 3 intro.\n",
    "        schema = [\"cluster_id\"  + \"_k_\" + str(k), \"centroid\"  + \"_k_\" + str(k)]\n",
    "\n",
    "        schema = StructType([ \n",
    "            StructField(\"cluster_id\" + \"_k_\" + str(k),IntegerType(),True), \n",
    "            StructField(\"centroid\" + \"_k_\" + str(k),VectorUDT(),True), \n",
    "        ])\n",
    "\n",
    "        # Creating new DataFrame using aforementioned schema\n",
    "        centr_df = spark.createDataFrame(data=l, schema=schema)\n",
    "\n",
    "        # Adding centroids information on the original DataFrame\n",
    "        clusters_df = clusters_df.join(\n",
    "            centr_df, on=[\"cluster_id\" + \"_k_\" + str(k)]\n",
    "        )\n",
    "\n",
    "        # Packing evaluation metrics for future retreival\n",
    "        clustering_results[str(k)] = {\n",
    "            \"silhouette_k\": silhouette_k,\n",
    "            \"wssd_k\"      : wssd_k,\n",
    "        }\n",
    "\n",
    "    # Returning eval metrics and newly created DataFrame\n",
    "    return clustering_results, clusters_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commodity method to **evaluate K-Means** clustering, expanded from code seen in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_k_means(\n",
    "    clusters,\n",
    "    metric_name=\"silhouette\",\n",
    "    distance_measure=\"squaredEuclidean\",\n",
    "    prediction_col=\"cluster\",\n",
    "    featuresCol = \"features\"\n",
    "):\n",
    "\n",
    "    # only silhouette is supported by PySpark\n",
    "    evaluator = ClusteringEvaluator(\n",
    "        metricName=metric_name,\n",
    "        distanceMeasure=distance_measure,\n",
    "        predictionCol=prediction_col,\n",
    "        featuresCol=featuresCol\n",
    "    )\n",
    "\n",
    "    return evaluator.evaluate(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commodity method to **plot** K-means clustering **results**, expanded from code shown in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustering_results(\n",
    "    clustering_results, k_range, plot_title\n",
    "):\n",
    "\n",
    "    # Creating Pandas Dataframe for plotting reasons\n",
    "    k_col = [str(x) for x in k_range]\n",
    "    wssd_col = [\n",
    "        clustering_results[k][\"wssd_k\"] for k in k_col \n",
    "    ]\n",
    "    silhouette_col = [\n",
    "        clustering_results[k][\"silhouette_k\"] for k in k_col \n",
    "    ]\n",
    "\n",
    "    plot_df_temp = pd.DataFrame([k_col, wssd_col, silhouette_col]).transpose()\n",
    "    plot_df_temp.columns = [\"K\", \"WSSD\", \"Silhouette\"]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    _ = sns.pointplot(\n",
    "        data=plot_df_temp, x=\"K\", y=\"WSSD\", ax=ax, color=\"navy\"\n",
    "    )\n",
    "    _ = ax.set_xlabel(\"Number of clusters\")\n",
    "    _ = ax.set_ylabel(\"WSSD (lower is better)\")\n",
    "    _ = ax.set_title(plot_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assembling features into a vector, as required by PySpark.\n",
    "\n",
    "Clustering will be performed on all 24 player features, as in naive approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "CLUSTERING_FEATURES = copy.deepcopy(PLAYER_FEATURES)\n",
    "CLUSTERING_FEATURES.remove(\"value\")\n",
    "CLUSTERING_FEATURES.remove(\"overall\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=CLUSTERING_FEATURES,\n",
    "    outputCol=\"clustering_features_vec\"\n",
    ")\n",
    "\n",
    "pre_processed_df = assembler.transform(pre_processed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-max **scaling** is very important in Clustering, since **K-Means** heavily **relies** on computing and evaluating the **distance** between datapoints.<br>\n",
    "\n",
    "Computing distances between datapoints whose dimenions have dishomogeneous scales may yield to inaccurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"clustering_features_vec\",\n",
    "    outputCol=\"clustering_features_vec_min_max\"    \n",
    ")\n",
    "\n",
    "pre_processed_df = scaler.fit(pre_processed_df).transform(pre_processed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per attempt 3 introduction, clustering is performed **across** all the different season in a separate way, **not** on all the seasons together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\n",
    "    s: pre_processed_df.filter(col(\"season\") == s) for s in seasons\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per notebook philosopy, in order to speed future execution up, trained models and their evaluations are persisted in disk.\n",
    "\n",
    "For this reason, if no trained model is found on disk, then we proceed to training; otherwise, data is loaded from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(CLUSTERING_DF_PATH):\n",
    "    print(\"clustering NOT in disk, training...\")\n",
    "    clustering_results_dict = dict()\n",
    "    clustering_results_df = dict()\n",
    "\n",
    "    for s in seasons:\n",
    "        print(f\"Season: {s}\")\n",
    "        clustering_results_dict[s], clustering_results_df[s] = do_clustering(\n",
    "            k_range=k_range, \n",
    "            input_df=df_dict[s], \n",
    "            max_iter=MAX_ITER,\n",
    "            featuresCol=\"clustering_features_vec_min_max\",\n",
    "            clusterCol=\"cluster_id\",\n",
    "            verbose=False,\n",
    "            season=s\n",
    "        )\n",
    "else:\n",
    "    print(\n",
    "        \"clustering already done, data will be loaded from disk in the next cells\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(CLUSTERING_DF_PATH):\n",
    "\n",
    "    pre_processed_df = reduce(\n",
    "        DataFrame.unionAll, \n",
    "        [\n",
    "            clustering_results_df[s] for s in seasons\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"saving clustering data on disk...\")\n",
    "    pre_processed_df.write.parquet(CLUSTERING_DF_PATH)\n",
    "\n",
    "else:\n",
    "    print(\"clustering data found on disk, will be loaded in next cells...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(CLUSTERING_EVAL_PATH):\n",
    "\n",
    "    print(\"clustering evaluation found on disk, loading...\")\n",
    "    \n",
    "    with open(CLUSTERING_EVAL_PATH) as json_file:\n",
    "        clustering_results_dict = json.load(json_file)\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"clustering evaluation NOT in disk, exporting...\")\n",
    "\n",
    "    import json\n",
    "    with open(CLUSTERING_EVAL_PATH, 'w') as fp:\n",
    "        json.dump(clustering_results_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electing the best number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the **Elbow method** will be used to find the **best number of clusters**: its observations will be performed considering the seasons separatedly and all together, in order to have a **wider understanding** of what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow method plots for every season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in seasons:\n",
    "    plot_clustering_results(\n",
    "        clustering_results_dict[s], \n",
    "        k_range=K_RANGE, \n",
    "        plot_title=f\"Elbow method evaluation for season {s}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seasons all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow method plots for seasons all together.\n",
    "\n",
    "Basically, considering seasons all together amounts to computing the average **WSSD** across the seasons, as visible in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_results_dict[\"avg\"] = dict.fromkeys(K_RANGE)\n",
    "\n",
    "for k in clustering_results_dict[\"avg\"].keys():\n",
    "    clustering_results_dict[\"avg\"][k] = {\n",
    "        \"wssd_k\": 0,\n",
    "        \"silhouette_k\": 0\n",
    "    }\n",
    "\n",
    "sum_wssd = dict.fromkeys(K_RANGE, 0)\n",
    "sum_silhouette = dict.fromkeys(K_RANGE, 0)\n",
    "\n",
    "for s in seasons:\n",
    "    for k in K_RANGE:\n",
    "        sum_wssd[k] += clustering_results_dict[s][k][\"wssd_k\"]\n",
    "        sum_silhouette[k] += clustering_results_dict[s][k][\"silhouette_k\"]\n",
    "    \n",
    "avg_wssd = dict()\n",
    "avg_silhouette = dict()\n",
    "\n",
    "for k in K_RANGE:\n",
    "    avg_wssd[k] = sum_wssd[k] / len(k_range)\n",
    "    avg_silhouette[k] = sum_silhouette[k] / len(k_range)\n",
    "\n",
    "    clustering_results_dict[\"avg\"][k][\"wssd_k\"] = avg_wssd[k]\n",
    "    clustering_results_dict[\"avg\"][k][\"silhouette_k\"] = avg_silhouette[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_results(\n",
    "    clustering_results_dict[\"avg\"], \n",
    "    k_range=K_RANGE, \n",
    "    plot_title=\"Elbow method evaluation, average of all seasons\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elbow** method shows that $k = 6$ is the **best number of clusters**, considering seasons separatedly and all together both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation uses **Silhouette** coefficient, since it is the only evaluation metric supported by PySpark.\n",
    "\n",
    "Once again, two cases will be considered: single seasons and seasons all together.\n",
    "\n",
    "For simplicity, only the case with $k = 6$ number of clusters will be considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in seasons:\n",
    "    print(\n",
    "        f\"Silhouette coefficient for season {s}: \"\n",
    "        f\"{clustering_results_dict[s]['6']['silhouette_k']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasons all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Silhouette coefficient (avg of all seasons): \"\n",
    "    f\"{clustering_results_dict['avg']['6']['silhouette_k']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette coefficient aligns with discoveries of previous learning attempts.\n",
    "\n",
    "In fact, the value we get, $0.30$, is close to $0$, which indicates that **obtained clusters are not significant**.\n",
    "\n",
    "After composing the data, some visualizations will be performed, to give a graphical explanation of this result as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As hinted, for visualization (and learning) purposes, clustering results must be integrated into the main DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute distance between player and centroid of its cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing distance between player and the centroid of the cluster it has been assigned to (as explained in attempt 3 intro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_distance_from_centroid_UDF = udf(\n",
    "    lambda player, centroid: float(\n",
    "        Vectors.squared_distance(\n",
    "            player, centroid\n",
    "        )\n",
    "    ), FloatType()\n",
    ")\n",
    "\n",
    "if os.path.isdir(CLUSTERING_DF_PATH):\n",
    "    print(\"clustering data found on disk, loading...\")\n",
    "    pre_processed_df = spark.read.parquet(CLUSTERING_DF_PATH)\n",
    "\n",
    "for k in K_RANGE:\n",
    "    pre_processed_df = pre_processed_df.withColumn(\n",
    "        \"distance_from_centroid\" + \"_k_\" + k,\n",
    "        compute_distance_from_centroid_UDF(\n",
    "            col(\"clustering_features_vec_min_max\"),\n",
    "            col(\"centroid\" + \"_k_\" + k)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From players to teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling football teams as aggregation of its football players, as explained in project intro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = pre_processed_df.groupBy(\n",
    "    [\"season\", \"club_name\", \"macro_role\"]\n",
    ").agg(\n",
    "    {\n",
    "        \"distance_from_centroid\" + \"_k_\" + str(k): \"avg\" for k in K_RANGE \n",
    "    }\n",
    ")\n",
    "\n",
    "# K_RANGE stores the different number of clusters, tested with the Elbow method\n",
    "for k in K_RANGE:\n",
    "    teams_df = teams_df.withColumnRenamed(\n",
    "        \"avg(distance_from_centroid\" + \"_k_\" + str(k) + \")\",\n",
    "        \"avg_distance_from_centroid\" + \"_k_\" + str(k)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factory method to get the specific subquery, needed by every number of clusters\n",
    "def generate_subquery(macro_role, k):\n",
    "    return f\"\"\"(\n",
    "        case\n",
    "            when macro_role='{macro_role}' then avg_distance_from_centroid_k_{k} \n",
    "        else NULL\n",
    "        end\n",
    "    ) as avg_dist_macro_role_{int(macro_role)}_k_{k}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every player of every club, in every season and in every macro role (goalkeeper, defender, midfielder and attacker), get its distance from the centroid of the cluster the player has been assigned to.\n",
    "\n",
    "At the end of this query, these are the fileds of a record (with their explanation):\n",
    "* `club_name`: name of football club.\n",
    "* `season`: the season the record refers to.\n",
    "* ...\n",
    "* ... \n",
    "* `avg_dist_macro_role_1_k_2`: average distance between <span style=\"color:blue\">defenders</span> (that played for `club_name` in season `season`) and centroid of the cluster they have been assigned to, when using $k=2$ number of clusters.\n",
    "* ...\n",
    "* `avg_dist_macro_role_1_k_62`: average distance between <span style=\"color:green\">defenders</span> (that played for `club_name` in season `season`) and centroid of the cluster they have been assigned to, when using $k=62$ number of clusters.\n",
    "* ...\n",
    "* ...\n",
    "* `avg_dist_macro_role_3_k_2`: average distance between <span style=\"color:Purple\">attackers</span> (that played for `club_name` in season `season` ) and centroid of the cluster they have been assigned to, when using $k=2$ number of clusters.\n",
    "* ...\n",
    "* `avg_dist_macro_role_3_k_62`: average distance between <span style=\"color:Fuchsia\">attackers</span> (that played for `club_name` in season `season` ) and centroid of the cluster they have been assigned to, when using $k=62$ number of clusters. \n",
    "\n",
    "For example, starting from this data:\n",
    "\n",
    "| club_name | season | short_name | macro_role | dist_from_cluster_centroid_k_2 | ... | dist_from_cluster_centroid_k_62    |\n",
    "| --------- | ------ | ---------- | ---------- | -------------------- | --- | ------------------------ |\n",
    "| Napoli    | 2020   | Malcuit    | 1          | <span style=\"color:blue\">(0.4, 0.5, 0.2)</span>      | ... |  <span style=\"color:green\">(0.7, 0.6, 0.5)         |\n",
    "| Napoli    | 2020   | Mário Rui  | 1          | <span style=\"color:blue\">(0.6, 0.1, 0.1)</span>      | ... |  <span style=\"color:green\">(0.04, 0.13, 0.02)      |\n",
    "| Napoli    | 2020   | Rrahmani   | 1          | <span style=\"color:blue\">(0.6, 0.1, 0.1)</span>      | ... |  <span style=\"color:green\">(0.04, 0.13, 0.02)      |\n",
    "| Napoli    | 2020   | Koulibaly  | 1          | <span style=\"color:blue\">(0.9, 0.9, 0.3)</span>      | ... |  <span style=\"color:green\">(0.91, 0.49, 0.38)      |\n",
    "| Napoli    | 2020   | Petagna    | 3          | <span style=\"color:Purple\">(0.1, 0.1, 0.1)</span>      | ... |  <span style=\"color:Fuchsia\">(0.16, 0.51, 0.19)      |yellow\n",
    "| Napoli    | 2020   | Politano   | 3          | <span style=\"color:Purple\">(0.1, 0.1, 0.1)</span>      | ... |  <span style=\"color:Fuchsia\">(0.16, 0.51, 0.19)      |\n",
    "| Napoli    | 2020   | Lozano     | 3          | <span style=\"color:Purple\">(0.2, 0.3, 0.1)</span>      | ... |  <span style=\"color:Fuchsia\">(0.24, 0.63, 0.51)      |\n",
    "\n",
    "The result will be:\n",
    "\n",
    "| club_name | season | avg_dist_macro_role_1_k_2 | ... | avg_dist_macro_role_1_k_62 | ... | avg_dist_macro_role_3_k_2 | ... | avg_dist_macro_role_3_k_62 |\n",
    "| --------- | ------ | ------------------------- | --- | -------------------------- | --- | ------------------------- | --- | -------------------- |\n",
    "| Napoli | 2020 | <span style=\"color:blue\">(0.62, 0.4, 0.18)</span> | ... | <span style=\"color:green\">(0.42, 0.34, 0.23)</span> | ... | <span style=\"color:purple\">(0.13, 0.16, 0.1)</span> | ... | <span style=\"color:fuchsia\">(0.18, 0.55, 0.3)</span> |\n",
    "\n",
    "The average is computed by taking into consideration the **multidimensionality** of the data, as if we were operating with Pandas, NumPy or PyTorch:\n",
    "\n",
    "<span style=\"color:green\">$(0.42, 0.34, 0.23) = (avg(0.7, 0.04, 0.04, 0.91), avg(0.6, 0.13, 0.13, 0.49), avg(0.5, 0.02, 0.02, 0.38))$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells produce the just explained data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df.createOrReplaceTempView(\"t\")\n",
    "\n",
    "temp = dict()\n",
    "\n",
    "for k in K_RANGE:\n",
    "    temp[k] = (\n",
    "        spark.sql(\n",
    "            f\"\"\"\n",
    "                select season, club_name, {generate_subquery(0.0, k)}, {generate_subquery(1.0, k)}, {generate_subquery(2.0, k)}, {generate_subquery(3.0, k)}, {generate_subquery(4.0, k)}, {generate_subquery(5.0, k)}, {generate_subquery(6.0, k)}, {generate_subquery(7.0, k)}\n",
    "                from t\n",
    "            \"\"\"\n",
    "        )\n",
    "        .groupBy(\"season\", \"club_name\")\n",
    "        .agg(\n",
    "            avg(f\"avg_dist_macro_role_0_k_{k}\").alias(f\"avg_dist_macro_role_0_k_{k}\"),\n",
    "            avg(f\"avg_dist_macro_role_1_k_{k}\").alias(f\"avg_dist_macro_role_1_k_{k}\"),\n",
    "            avg(f\"avg_dist_macro_role_2_k_{k}\").alias(f\"avg_dist_macro_role_2_k_{k}\"),\n",
    "            avg(f\"avg_dist_macro_role_3_k_{k}\").alias(f\"avg_dist_macro_role_3_k_{k}\"),\n",
    "            avg(f\"avg_dist_macro_role_4_k_{k}\").alias(f\"avg_dist_macro_role_4_k_{k}\"),\n",
    "            avg(f\"avg_dist_macro_role_5_k_{k}\").alias(f\"avg_dist_macro_role_5_k_{k}\"),\n",
    "            avg(f\"avg_dist_macro_role_6_k_{k}\").alias(f\"avg_dist_macro_role_6_k_{k}\"),\n",
    "            avg(f\"avg_dist_macro_role_7_k_{k}\").alias(f\"avg_dist_macro_role_7_k_{k}\"),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = temp[K_RANGE[0]]\n",
    "\n",
    "for i in range(1, len(K_RANGE)):\n",
    "    teams_df = teams_df.join(\n",
    "        temp[K_RANGE[i]],\n",
    "        on=[\"season\", \"club_name\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_distances_dict = dict()\n",
    "avg_distances_vec_dict = dict()\n",
    "\n",
    "for k in K_RANGE:\n",
    "    avg_distances_dict[k] = [\n",
    "        f\"avg_dist_macro_role_{i}_k_{k}\" for i in range(0, NUM_MACRO_ROLES)\n",
    "    ]\n",
    "    avg_distances_vec_dict[k] = f\"avg_dist_vec_k_{k}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filling `NaN`**/`null` values with the minimum value in the entire DataFrame.\n",
    "\n",
    "As a consequence of what has been explained in the clustering introduction, the minimum value basically represents \"**averageness**\".\n",
    "So, if a team does not have data for a `macro_role`, we give to it the minimum possible value, i.e. the less peculariar possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_min = teams_df.toPandas()[\n",
    "    [ x for x in avg_distances_dict.values() for x in x ]\n",
    "].to_numpy().reshape(-1).min()\n",
    "\n",
    "teams_df = teams_df.fillna(global_min * 1.5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K_RANGE:\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=avg_distances_dict[k], outputCol=avg_distances_vec_dict[k]\n",
    "    )\n",
    "\n",
    "    teams_df = assembler.transform(teams_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, visualization of the clustering result will be provided.\n",
    "\n",
    "Since 24-dimensional datapoints have been used, points will be fed to PCA and its result will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_df, pca_model = perform_pca(\n",
    "    df=pre_processed_df,\n",
    "    num_components=2,\n",
    "    input_col=\"clustering_features_vec_min_max\",\n",
    "    output_col=\"clustering_features_vec_min_max_pcs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = feature_vec_to_cols(\n",
    "    pre_processed_df.toPandas(),\n",
    "    \"clustering_features_vec_min_max_pcs\",\n",
    "    [\"clustering_features_vec_min_max_pc_0\", \"clustering_features_vec_min_max_pc_1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in seasons:\n",
    "    scatter_plot(\n",
    "        x = pdf[pdf[\"season\"] == s][\"clustering_features_vec_min_max_pc_0\"],\n",
    "        y = pdf[pdf[\"season\"] == s][\"clustering_features_vec_min_max_pc_1\"],\n",
    "        x_label=\"Clustering Features Principal Component 0\",\n",
    "        y_label=\"Clustering Features Principal Component 1\",\n",
    "        title=f\"Clustering result for season {s}\",\n",
    "        c=pdf[pdf[\"season\"] == s][\"cluster_id_k_6\"],\n",
    "        c_map=plt.cm.get_cmap(\"tab10\"),\n",
    "        figsize=(12,8),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some visualization comments:\n",
    "\n",
    "* x and y axis are the two principal components, computed in order to being able to actually plot data.\n",
    "* **Color** of the point **encodes** the **cluster** assigned to the datapoint.\n",
    "* Huge **difference** in data space between **legacy** and **modern** seasons.\n",
    "* In **modern** seasons, **galkeepers** are very well **separated** from the rest of the players, whilst in **legacy** seasons they tend to be **closer** to other players.\n",
    "* In all seasons, **offensive** players tend to be **more mixed up**, whilst **central** and **defensive** players tend to be better **separated**.\n",
    "\n",
    "Colors are not preserved across different seasons, due to how utility libraries work, but, during the experimentation stages, we made plots interactive, in order to see details about the single datapoints and be able to actually state the previous sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(\n",
    "    teams_df,\n",
    "    on=[\"club_name\", \"season\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.9, 0.1], seed=random_seed)\n",
    "\n",
    "FEATURES_COL = list(avg_distances_vec_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_evaluator_cv.setLabelCol(REGRESSION_LABEL_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_3_regression_linear_regression_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Linear Regression model NOT found in disk, training...\")\n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = LinearRegression()\n",
    "\n",
    "    linear_regression_param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "        .addGrid(estimator.solver, [\"auto\", \"normal\"])\n",
    "        .addGrid(estimator.fitIntercept, [True, False])\n",
    "        .addGrid(estimator.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=linear_regression_param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Linear Regression model found in disk, loading...\")\n",
    "    model = LinearRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Linear Regression\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Evaluating Linear Regression model trained in previous cell...\")\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_3/regression/linear_regression.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_3/regression/linear_regression.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Saving Linear Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_3/regression/linear_regression\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression with multiple `k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evals_dict = dict()\n",
    "cross_validated_models_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "for k in K_RANGE:\n",
    "\n",
    "    PREDICTION_COL = f\"attempt_3_regression_linear_regression_multiple_k_{k}_predictions\"\n",
    "\n",
    "    if not model_exists(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Linear Regression multiple k\"][k]\n",
    "    ):\n",
    "        print(f\"Linear Regression multiple k with k={k} model NOT found in disk, training...\")\n",
    "        regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "        \n",
    "        estimator = LinearRegression()\n",
    "\n",
    "        param_grid = (\n",
    "            ParamGridBuilder()\n",
    "            .addGrid(estimator.featuresCol, [f\"avg_dist_vec_k_{k}\"])\n",
    "            .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "            .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "            .addGrid(estimator.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "            .addGrid(estimator.solver, [\"auto\", \"normal\"])\n",
    "            .addGrid(estimator.fitIntercept, [True, False])\n",
    "            .addGrid(estimator.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "            .build()\n",
    "        )\n",
    "\n",
    "        cross_validated_models_dict[k] = learn_best_model(\n",
    "            estimator=estimator,\n",
    "            param_grid=param_grid,\n",
    "            evaluator_cv=regression_evaluator_cv\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Linear Regression multiple k with k={k} model found in disk, loading...\")\n",
    "        model = LinearRegressionModel.load(\n",
    "            TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Linear Regression multiple k\"][k]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K_RANGE:\n",
    "    if not model_exists(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Linear Regression multiple k\"][k]\n",
    "    ):\n",
    "        print(\n",
    "            f\"Evaluating Linear Regression multiple k with k={k} model trained in previous cell...\")\n",
    "        \n",
    "        PREDICTION_COL = f\"attempt_3_regression_linear_regression_multiple_k_{k}_predictions\"\n",
    "        evaluate_learning_models(\n",
    "            best_model=cross_validated_models_dict[k].bestModel,\n",
    "            evaluators=get_evaluators(\n",
    "                cross_validated_models_dict[k].bestModel, \n",
    "                label_col=REGRESSION_LABEL_COL, \n",
    "                prediction_col=PREDICTION_COL, \n",
    "                evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "            ),\n",
    "            save_training_result_path=f\"./evaluation_results/attempt_3/regression/linear_regression_multiple_k/linear_regression_multiple_k_{k}.json\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Printing evaluation loaded from disk...\")\n",
    "        model_evals_dict[k] = print_model_evaluation(\n",
    "            model_evaluation_path=f\"./evaluation_results/attempt_3/regression/linear_regression_multiple_k/linear_regression_multiple_k_{k}.json\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_dict_pretty = {\n",
    "    k : v[\"test_set_evaluation\"][\"r2\"] for (k, v) in zip(model_evals_dict.keys(), model_evals_dict.values())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K_RANGE:    \n",
    "    if not model_exists(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Linear Regression multiple k\"][k]\n",
    "    ):\n",
    "        print(\n",
    "            f\"Saving Linear Regression multiple k with k={k} model on disk...\"\n",
    "        )\n",
    "        cross_validated_models_dict[k].bestModel.save(\n",
    "            f\"./trained_models/attempt_3/regression/linear_regression_multiple_k/{k}_clusters\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import DecisionTreeRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_3_regression_decision_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\"Prediction Tree model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = DecisionTreeRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Decision Tree model found in disk, loading...\")\n",
    "    model = DecisionTreeRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Prediction Tree\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Prediction Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_3/regression/prediction_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_3/regression/prediction_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\"Saving Decision Tree Regressor model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_3/regression/prediction_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import GBTRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_3_regression_gradient_boosted_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\"Regression Gradient Boosted Tree model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = GBTRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [2, 5, 10])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Gradient Boosted Tree model found in disk, loading...\")\n",
    "    model = GBTRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Gradient Boosted Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_3/regression/gradient_boosted_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_3/regression/gradient_boosted_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\"Saving Gradient Boosted Tree Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_3/regression/gradient_boosted_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_3_regression_random_forest_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Regression Random Forest model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = RandomForestRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .addGrid(estimator.numTrees, [20, 40])\n",
    "        .addGrid(\n",
    "            estimator.featureSubsetStrategy, \n",
    "            [\"auto\", \"onethird\", \"all\", \"log2\"]\n",
    "        )\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Random Forest model found in disk, loading...\")\n",
    "    model = RandomForestRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Random Forest\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Random Forest model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_3/regression/random_forest.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_3/regression/random_forest.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Saving Random Forest Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_3/regression/random_forest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_evaluator_cv.setLabelCol(CLASSIFICATION_LABEL_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.9, 0.1], seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import OneVsRestModel\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "PREDICTION_COL = \"attempt_3_classification_svm_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"SVM\"]\n",
    "):\n",
    "    print(\"SVM Classification model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = OneVsRest(\n",
    "        classifier=LinearSVC(\n",
    "            featuresCol=FEATURES_COL[0],\n",
    "            labelCol=CLASSIFICATION_LABEL_COL,\n",
    "            predictionCol=PREDICTION_COL,\n",
    "        ),\n",
    "        featuresCol=FEATURES_COL[0],\n",
    "        labelCol=CLASSIFICATION_LABEL_COL,\n",
    "        predictionCol=PREDICTION_COL\n",
    "    )\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"SVM Classification model found in disk, loading...\")\n",
    "    model = OneVsRestModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"SVM\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"SVM\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating SVM Classifier model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_3/classification/svm.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_3/classification/svm.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"SVM\"]\n",
    "):\n",
    "    print(\"Saving SVM Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_3/classification/svm\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import OneVsRestModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "PREDICTION_COL = \"attempt_3_classification_logistic_regression_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"Logistic Regression\"]\n",
    "):\n",
    "    print(\"Logistic Regression Classification model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = OneVsRest(\n",
    "        classifier=LogisticRegression(\n",
    "            featuresCol=FEATURES_COL,\n",
    "            labelCol=CLASSIFICATION_LABEL_COL,\n",
    "            predictionCol=PREDICTION_COL,\n",
    "        ),\n",
    "        featuresCol=FEATURES_COL,\n",
    "        labelCol=CLASSIFICATION_LABEL_COL,\n",
    "        predictionCol=PREDICTION_COL\n",
    "    )\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Logistic Regression Classification model found in disk, loading...\")\n",
    "    model = OneVsRestModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"Logistic Regression\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"Logistic Regression\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Logistic Regression Classifier model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_3/classification/logistic_regression.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_3/classification/logistic_regression.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"Logistic Regression\"]\n",
    "):\n",
    "    print(\"Saving Logistic Regression Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_3/classification/logistic_regression\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_3_classification_decision_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\"Classification Decision Tree model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = DecisionTreeClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification Decision Tree model found in disk, loading...\")\n",
    "    model = DecisionTreeClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"Decision Tree\"]\n",
    "    )\n",
    "    evaluators = get_evaluators(\n",
    "        model, \n",
    "        label_col=CLASSIFICATION_LABEL_COL, \n",
    "        prediction_col=PREDICTION_COL, \n",
    "        evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Decision Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_3/classification/decision_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_3/classification/decision_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\"Saving Decision Tree model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_3/classification/decision_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_3_classification_random_forest_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Classification Random Forest model NOT found in disk, training...\")\n",
    "    \n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = RandomForestClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .addGrid(estimator.numTrees, [20, 40])\n",
    "        .addGrid(estimator.featureSubsetStrategy, [\"auto\", \"onethird\", \"all\", \"log2\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification Random Forest model found in disk, loading...\")\n",
    "    model = RandomForestClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"Random Forest\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Random Forest model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_3/classification/random_forest.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_3/classification/random_forest.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Saving Random Forest Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_3/classification/random_forest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_3_classification_mlp_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"MLP\"]\n",
    "):\n",
    "    print(\"Classification MLP model NOT found in disk, training...\")\n",
    "    \n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = MultilayerPerceptronClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.layers, [[NUM_MACRO_ROLES, NUM_MACRO_PLACES]])\n",
    "        .addGrid(estimator.solver, [\"gd\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification MLP model found in disk, loading...\")\n",
    "    model = MultilayerPerceptronClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"MLP\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"MLP\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating MLP model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_3/classification/mlp.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_3/classification/mlp.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 3\"][\"Classification\"][\"MLP\"]\n",
    "):\n",
    "    print(\"Saving MLP Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_3/classification/mlp\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering-Learning cross evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since clustering represents an additional layer of learning, it's interesting to cross-link clustering and learning evaluations, in order to try to discover whether there is some kind of relation between the two.\n",
    "\n",
    "Specifically, since clustering is heavily influenced by the `k` number of clusters, we are interested in observing how learning performances behave, upon changing `k` values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting data ready for plotting:\n",
    "* x-axis: numbers of clusters\n",
    "* y-axis on the left: WSSD\n",
    "* y-axis on the right: $r^2$ score, computed on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(model_evals_dict.keys())\n",
    "wssd_k = [\n",
    "    x[\"wssd_k\"] for x in clustering_results_dict[\"avg\"].values()\n",
    "]\n",
    "r2 = [\n",
    "    x[\"test_set_evaluation\"][\"r2\"] for x in model_evals_dict.values()\n",
    "]\n",
    "\n",
    "plot_df_temp = pd.DataFrame([x, wssd_k, r2]).transpose()\n",
    "plot_df_temp.columns = [\"x\", \"wssd_k\", \"r2\"]\n",
    "\n",
    "ax = plot_df_temp.plot(\n",
    "    x=\"x\", \n",
    "    y=\"wssd_k\", \n",
    "    color=\"#208AAE\"\n",
    ")\n",
    "ax.set_xlabel(\"Number of clusters\")\n",
    "ax.set_ylabel(\"WSSD (lower is better)\")\n",
    "ax.set_ylim(15, 65)\n",
    "ax.legend(bbox_to_anchor=(1,0.9))\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "plot_df_temp.plot(\n",
    "    x=\"x\", \n",
    "    y=\"r2\", \n",
    "    ax=ax2, \n",
    "    color=\"#FF88DC\",\n",
    ")\n",
    "ax2.set_ylabel(\"r2 (higher is better)\")\n",
    "ax2.set_ylim(0.15, 0.65)\n",
    "ax2.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "# ax.figure.legend()\n",
    "plt.title(\"Elbow method vs. r2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$r^2$ test score is always the same, floating around $0.5$.\n",
    "\n",
    "A thourough investigation of the actual values $r^2$ revealed that they are pretty much equal up to the third decimal digit, meaning that the fluctuations are negligible, hence the plot appears as a straight line.\n",
    "\n",
    "Originally, we hypothesized that there could've been some interesting behavious, but data proved us wrong: **$r^2$ test score** is **not affected** by `k` value (the **number of clusters** used in clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 4: prior-based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous three attempts opened us to the hypothesis that data captured at the start of the season may not be sufficient to predict the end-of-the-season ranking of football teams.\n",
    "\n",
    "In fact, during the entire season, there are multiple **difficult to measure factors** that influence results, like, for example:\n",
    "\n",
    "* **Fans support**.\n",
    "* **Coach** competence.\n",
    "* Technical staff competence.\n",
    "* Training style.\n",
    "* **Training pitch** quality.\n",
    "* **Fixture schedule**.\n",
    "* International breaks.\n",
    "* Players' **mood**.\n",
    "* Societary involvement.\n",
    "* \"Winning **mentality**\".\n",
    "\n",
    "\n",
    "What if all of these non-trivially measurable components are actually captured somewhere in the data?<br>\n",
    "A valid **statistical prior** could be the average seasonal ranking of the football team, since it would include all aforementioned influence factors.\n",
    "\n",
    "This prior has been named **Reward-Penalty** (RP) coefficient and it has been computed as follows:<br>\n",
    "\n",
    "* Given a football team $ft$ and a season $s$: $rp(ft, s) = 21 - avg\\_rank(ft, s)$.\n",
    "* $avg\\_rank(ft, s)$ is a function that computes the average ranking for the given football team $ft$ in seasons up to $s$ (excluded).\n",
    "\n",
    "For example, given the following data:\n",
    "\n",
    "| team name | season | ranking |\n",
    "| --------- | ------ | ------- |\n",
    "| Napoli    | 2021   | 3       |\n",
    "| Napoli    | 2020   | 5       |\n",
    "| Napoli    | 2019   | <span style=\"color:RoyalBlue\">7</span>       |\n",
    "| Napoli    | 2018   | <span style=\"color:Purple\">2</span>       |\n",
    "| Napoli    | 2017   | <span style=\"color:green\">2</span>       |\n",
    "\n",
    "$rp(Napoli,\\ 2020) = 21 - int(avg($<span style=\"color:RoyalBlue\">7</span>$,$<span style=\"color:Purple\">2</span>$,$<span style=\"color:green\">2</span>$)) = 21 - int(3.66) = 21 - 3 = 18$\n",
    "\n",
    "The goal of the `rp_coefficient` is to counter variance, by means of rewarding better performing teams and penalizing lower-performers.\n",
    "\n",
    "`rp_coefficient` may be a source of bias in the models, because human bias is injected into the data.<br>\n",
    "For this reason, `rp_coefficient` is used together with `rp_tradeoff`, a **tradeoff** parameter controlling how much importance is given to `rp_coefficient`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing `rp_coefficient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = df\n",
    "\n",
    "MAX_PLACE = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`t_rp_coeff` is a temporary table created from `rp_df`, containing only `season`, `club_name`, `place`.<br>\n",
    "\n",
    "It will come in handy in the subsequent advanced SQL manipulations, needed to compute the average ranking for every football team, for every year previous to the year taken into consideration (see example in the introduction paragraph for further details about rp_coefficient computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df.select(\n",
    "    \"season\", \"club_name\", \"place\"\n",
    ").createOrReplaceTempView(\"t_rp_coeff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next SQL query joins `t_rp_coeff` against itself:\n",
    "\n",
    "* For every football team, the join produces all possible year pairs, $(y_1,\\ y_2)$.\n",
    "* The join condition checks whether $(y_1\\ <\\ y_2)$ is true or not, in order to compute the average placement only considering the previous years (as explained in attempt 4 introduction).\n",
    "* The average place is computed using SQL aggregation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = spark.sql(\n",
    "    f\"\"\"\n",
    "    select t_rp_coeff.season, t_rp_coeff.club_name,\n",
    "        avg(\n",
    "            (\n",
    "                select sub.place\n",
    "                where sub.season < t_rp_coeff.season and sub.club_name == t_rp_coeff.club_name\n",
    "            )\n",
    "        ) as rp_coeff\n",
    "    from t_rp_coeff, t_rp_coeff as sub\n",
    "    group by t_rp_coeff.season, t_rp_coeff.club_name\n",
    "    order by t_rp_coeff.season desc\n",
    "    \"\"\"\n",
    ").fillna(MAX_PLACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = rp_df.join(\n",
    "    df, on=[\"club_name\", \"season\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once `rp_coeff` is computed, it is then applied, as per formula presented in attempt 4 introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = rp_df.withColumn(\"rp_coeff\", MAX_PLACE - col(\"rp_coeff\"))\n",
    "\n",
    "add_normalize_by_rp_UDF = udf(\n",
    "    lambda points, rp, tradeoff: points + tradeoff * rp, DoubleType()\n",
    ")\n",
    "\n",
    "RP_NORMALIZED_COLS = [\n",
    "    f\"avg(overall)_rp_normalized_tradeoff_{tradeoff}\".replace(\n",
    "        \".\", \"-\"\n",
    "    ) for tradeoff in ADD_RP_TRADEOFF\n",
    "]\n",
    "\n",
    "for tradeoff, col_name in zip(ADD_RP_TRADEOFF, RP_NORMALIZED_COLS):\n",
    "    rp_df = rp_df.withColumn(\n",
    "        col_name, add_normalize_by_rp_UDF(\n",
    "            col(\"avg(overall)\"), col(\"rp_coeff\"), lit(tradeoff)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[col_name], \n",
    "        outputCol=col_name + \"_vec\"\n",
    "    )\n",
    "\n",
    "    rp_df = assembler.transform(rp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = rp_df.toPandas()\n",
    "\n",
    "for rp_norm_col in RP_NORMALIZED_COLS:\n",
    "    plot_feature_target_relation(\n",
    "        pdf, [\"avg(overall)\"], rp_norm_col, color=\"Violet\", n_cols=1, figsize=(8,8)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is not quite as expected:\n",
    "\n",
    "* `rp_tradeoff` = $0.5$ seems to reduce variance a little bit.\n",
    "* `rp_tradeoff` $\\ge\\ 1$ seems to actually mimick the behaviour of all previous attempts.\n",
    "\n",
    "Nevertheless, in the hyperparameter grid, we'll set `featureCol` to all columns storing the different `rp_coefficient`-scaled features, so as Cross Validation will tell us what is the best `rp_coefficient`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning for attempt 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = rp_df.randomSplit([0.9, 0.1], seed=random_seed)\n",
    "\n",
    "FEATURES_COL = [\n",
    "    rp_normalized_col + \"_vec\" for rp_normalized_col in RP_NORMALIZED_COLS\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_evaluator_cv.setLabelCol(REGRESSION_LABEL_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_4_regression_linear_regression_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Linear Regression model NOT found in disk, training...\")\n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = LinearRegression()\n",
    "\n",
    "    linear_regression_param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "        .addGrid(estimator.solver, [\"auto\", \"normal\"])\n",
    "        .addGrid(estimator.fitIntercept, [True, False])\n",
    "        .addGrid(estimator.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=linear_regression_param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Linear Regression model found in disk, loading...\")\n",
    "    model = LinearRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Linear Regression\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Evaluating Linear Regression model trained in previous cell...\")\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_4/regression/linear_regression.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_4/regression/linear_regression.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Linear Regression\"]\n",
    "):\n",
    "    print(\"Saving Linear Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_4/regression/linear_regression\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with multiple `rp_tradeoff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validated_models_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "for tradeoff, col_name in zip(ADD_RP_TRADEOFF, FEATURES_COL):\n",
    "    PREDICTION_COL = \"attempt_4_regression_linear_regression_multiple_tradeoffs_{tradeoff}_predictions\"\n",
    "\n",
    "    if not model_exists(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Linear Regression multiple tradeoffs\"][tradeoff]\n",
    "    ):\n",
    "        print(\n",
    "            f\"Linear Regression multiple tradeoffs with tradeoff={tradeoff} model NOT found in disk, training...\"\n",
    "        )\n",
    "        regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "        \n",
    "        estimator = LinearRegression()\n",
    "\n",
    "        linear_regression_param_grid = (\n",
    "            ParamGridBuilder()\n",
    "            .addGrid(estimator.featuresCol, [col_name])\n",
    "            .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "            .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "            .addGrid(estimator.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "            .addGrid(estimator.solver, [\"auto\", \"normal\"])\n",
    "            .addGrid(estimator.fitIntercept, [True, False])\n",
    "            .addGrid(estimator.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "            .build()\n",
    "        )\n",
    "\n",
    "        cross_validated_models_dict[tradeoff] = learn_best_model(\n",
    "            estimator=estimator,\n",
    "            param_grid=linear_regression_param_grid,\n",
    "            evaluator_cv=regression_evaluator_cv\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Linear Regression multiple tradeoffs with tradeoff={tradeoff} model found in disk, loading...\")\n",
    "        model = LinearRegressionModel.load(\n",
    "            TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Linear Regression multiple tradeoffs\"][tradeoff]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evals_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tradeoff, col_name in zip(ADD_RP_TRADEOFF, FEATURES_COL):\n",
    "    if not model_exists(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Linear Regression multiple tradeoffs\"][tradeoff]\n",
    "    ):\n",
    "        print(\n",
    "            f\"Evaluating Linear Regression multiple tradeoffs with tradeoff={tradeoff} model trained in previous cell...\")\n",
    "        evaluate_learning_models(\n",
    "            best_model=cross_validated_models_dict[tradeoff].bestModel,\n",
    "            evaluators=get_evaluators(\n",
    "                cross_validated_models_dict[tradeoff].bestModel, \n",
    "                label_col=REGRESSION_LABEL_COL, \n",
    "                prediction_col=PREDICTION_COL, \n",
    "                evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "            ),\n",
    "            save_training_result_path=f\"./evaluation_results/attempt_4/regression/linear_regression_multiple_tradeoffs/tradeoff_{tradeoff}.json\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Printing evaluation loaded from disk...\")\n",
    "        model_evals_dict[tradeoff] = print_model_evaluation(\n",
    "            model_evaluation_path=f\"./evaluation_results/attempt_4/regression/linear_regression_multiple_tradeoffs/tradeoff_{tradeoff}.json\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tradeoff, col_name in zip(ADD_RP_TRADEOFF, FEATURES_COL):\n",
    "    if not model_exists(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Linear Regression multiple tradeoffs\"][tradeoff]\n",
    "    ):\n",
    "        print(\n",
    "            f\"Saving Linear Regression multiple tradeoffs with tradeoff={tradeoff} model on disk...\"\n",
    "        )\n",
    "        cross_validated_models_dict[tradeoff].bestModel.save(\n",
    "            f\"./trained_models/attempt_4/regression/linear_regression_multiple_tradeoffs/tradeoff_{tradeoff}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import DecisionTreeRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_4_regression_decision_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\"Prediction Tree model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = DecisionTreeRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Decision Tree model found in disk, loading...\")\n",
    "    model = DecisionTreeRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Prediction Tree\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Prediction Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_4/regression/prediction_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_4/regression/prediction_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Prediction Tree\"]\n",
    "):\n",
    "    print(\"Saving Decision Tree Regressor model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_4/regression/prediction_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import GBTRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_4_regression_gradient_boosted_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\"Regression Gradient Boosted Tree model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = GBTRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [2, 5, 10])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Gradient Boosted Tree model found in disk, loading...\")\n",
    "    model = GBTRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Gradient Boosted Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_4/regression/gradient_boosted_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_4/regression/gradient_boosted_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Gradient Boosted Tree\"]\n",
    "):\n",
    "    print(\"Saving Gradient Boosted Tree Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_4/regression/gradient_boosted_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_4_regression_random_forest_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Regression Random Forest model NOT found in disk, training...\")\n",
    "    \n",
    "    regression_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = RandomForestRegressor()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [REGRESSION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .addGrid(estimator.numTrees, [20, 40])\n",
    "        .addGrid(\n",
    "            estimator.featureSubsetStrategy, \n",
    "            [\"auto\", \"onethird\", \"all\", \"log2\"]\n",
    "        )\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=regression_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Regression Random Forest model found in disk, loading...\")\n",
    "    model = RandomForestRegressionModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Random Forest\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Random Forest model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=REGRESSION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=REGRESSION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_4/regression/random_forest.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_4/regression/random_forest.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Regression\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Saving Random Forest Regression model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_4/regression/random_forest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_evaluator_cv.setLabelCol(CLASSIFICATION_LABEL_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import OneVsRestModel\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "PREDICTION_COL = \"attempt_4_classification_svm_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"SVM\"]\n",
    "):\n",
    "    print(\"SVM Classification model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = OneVsRest(\n",
    "        classifier=LinearSVC(\n",
    "            featuresCol=FEATURES_COL,\n",
    "            labelCol=CLASSIFICATION_LABEL_COL,\n",
    "            predictionCol=PREDICTION_COL,\n",
    "        ),\n",
    "        featuresCol=FEATURES_COL[0],\n",
    "        labelCol=CLASSIFICATION_LABEL_COL,\n",
    "        predictionCol=PREDICTION_COL\n",
    "    )\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"SVM Classification model found in disk, loading...\")\n",
    "    model = OneVsRestModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"SVM\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"SVM\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating SVM Classifier model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_4/classification/svm.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_4/classification/svm.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"SVM\"]\n",
    "):\n",
    "    print(\"Saving SVM Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_4/classification/svm\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import OneVsRestModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "PREDICTION_COL = \"attempt_4_classification_logistic_regression_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"Logistic Regression\"]\n",
    "):\n",
    "    print(\"Logistic Regression Classification model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = OneVsRest(\n",
    "        classifier=LogisticRegression(\n",
    "            featuresCol=FEATURES_COL,\n",
    "            labelCol=CLASSIFICATION_LABEL_COL,\n",
    "            predictionCol=PREDICTION_COL,\n",
    "        ),\n",
    "        featuresCol=FEATURES_COL,\n",
    "        labelCol=CLASSIFICATION_LABEL_COL,\n",
    "        predictionCol=PREDICTION_COL\n",
    "    )\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Logistic Regression Classification model found in disk, loading...\")\n",
    "    model = OneVsRestModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"Logistic Regression\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"Logistic Regression\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Logistic Regression Classifier model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_4/classification/logistic_regression.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_4/classification/logistic_regression.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"Logistic Regression\"]\n",
    "):\n",
    "    print(\"Saving Logistic Regression Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_4/classification/logistic_regression\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_4_classification_decision_tree_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\"Classification Decision Tree model NOT found in disk, training...\")\n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = DecisionTreeClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification Decision Tree model found in disk, loading...\")\n",
    "    model = DecisionTreeClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"Decision Tree\"]\n",
    "    )\n",
    "    evaluators = get_evaluators(\n",
    "        model, \n",
    "        label_col=CLASSIFICATION_LABEL_COL, \n",
    "        prediction_col=PREDICTION_COL, \n",
    "        evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Decision Tree model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_4/classification/decision_tree.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_4/classification/decision_tree.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"Decision Tree\"]\n",
    "):\n",
    "    print(\"Saving Decision Tree model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_4/classification/decision_tree\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_4_classification_random_forest_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Classification Random Forest model NOT found in disk, training...\")\n",
    "    \n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = RandomForestClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.maxDepth, [5, 24])\n",
    "        .addGrid(estimator.maxBins, [32, 64])\n",
    "        .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "        .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "        .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "        .addGrid(estimator.numTrees, [20, 40])\n",
    "        .addGrid(estimator.featureSubsetStrategy, [\"auto\", \"onethird\", \"all\", \"log2\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification Random Forest model found in disk, loading...\")\n",
    "    model = RandomForestClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"Random Forest\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating Random Forest model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_4/classification/random_forest.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_4/classification/random_forest.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"Random Forest\"]\n",
    "):\n",
    "    print(\"Saving Random Forest Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_4/classification/random_forest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationModel\n",
    "\n",
    "PREDICTION_COL = \"attempt_4_classification_mlp_predictions\"\n",
    "\n",
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"MLP\"]\n",
    "):\n",
    "    print(\"Classification MLP model NOT found in disk, training...\")\n",
    "    \n",
    "    classification_evaluator_cv.setPredictionCol(PREDICTION_COL)\n",
    "    \n",
    "    estimator = MultilayerPerceptronClassifier()\n",
    "\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "        .addGrid(estimator.labelCol, [CLASSIFICATION_LABEL_COL])\n",
    "        .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "        .addGrid(estimator.layers, [[1, NUM_MACRO_PLACES]])\n",
    "        .addGrid(estimator.solver, [\"gd\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    cross_validated_models = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        evaluator_cv=classification_evaluator_cv\n",
    "    )\n",
    "else:\n",
    "    print(\"Classification MLP model found in disk, loading...\")\n",
    "    model = MultilayerPerceptronClassificationModel.load(\n",
    "        TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"MLP\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"MLP\"]\n",
    "):\n",
    "    print(\n",
    "        \"Evaluating MLP model trained in previous cell...\"\n",
    "    )\n",
    "    evaluate_learning_models(\n",
    "        best_model=cross_validated_models.bestModel,\n",
    "        evaluators=get_evaluators(\n",
    "            cross_validated_models.bestModel, \n",
    "            label_col=CLASSIFICATION_LABEL_COL, \n",
    "            prediction_col=PREDICTION_COL, \n",
    "            evaluation_metrics=CLASSIFICATION_EVALUATION_METRICS\n",
    "        ),\n",
    "        save_training_result_path=\"./evaluation_results/attempt_4/classification/mlp.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Printing evaluation loaded from disk...\")\n",
    "    print_model_evaluation(\n",
    "        model_evaluation_path=\"./evaluation_results/attempt_4/classification/mlp.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(\n",
    "    TRAINED_MODELS_DIRS[\"Attempt 4\"][\"Classification\"][\"MLP\"]\n",
    "):\n",
    "    print(\"Saving MLP Classification model on disk...\")\n",
    "    cross_validated_models.bestModel.save(\n",
    "        \"./trained_models/attempt_4/classification/mlp\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of `rp_tradeoff` on learning\n",
    "\n",
    "This is a more focused view showcasing how `rp_tradeoff` impacts learning performances.\n",
    "\n",
    "The assumption is as follows: the bigger `rp_tradeoff`, the more performances should increase, because the system is \"helped\" more and more.\n",
    "\n",
    "For this reason, the produced plot will will have:\n",
    "\n",
    "* `rp_tradeoff` on the $x$ axis\n",
    "* $r^2$ on the $y$ axis\n",
    "\n",
    "The following code prepares the axes, places them into a Pandas DataFrame and plots them, using Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_dict_r2_only = {\n",
    "    k : v[\"train_set_evaluation\"][\"r2\"] for (k, v) in zip(model_evals_dict.keys(), model_evals_dict.values())\n",
    "}\n",
    "\n",
    "plot_df_temp = pd.DataFrame(\n",
    "    [\n",
    "        list(evals_dict_r2_only.keys()), \n",
    "        list(evals_dict_r2_only.values())\n",
    "    ]\n",
    ").transpose()\n",
    "plot_df_temp.columns = [\"rp_tradeoff\", \"r2\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "_ = sns.pointplot(\n",
    "    data=plot_df_temp, x=\"rp_tradeoff\", y=\"r2\", ax=ax, color=\"HotPink\"\n",
    ")\n",
    "\n",
    "_ = ax.set_xlabel(\"rp_tradeoff\")\n",
    "_ = ax.set_ylabel(\"r2 (higher is better)\")\n",
    "_ = ax.set_title(\"rp_tradeoff vs. r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, our **assumption** has been **confuted**.\n",
    "\n",
    "Actually, the more importance is given to `rp_coefficient` the worse the system gets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with the idea of **predicting end-of-the-season points for European football clubs of the six major leagues** (English Premier League, Spanish Primera Division, Italian Serie A, French Ligue 1, German 1. Bundesliga and Dutch Eredivise).\n",
    "\n",
    "First we had to define how to **model** a **football team**: we decided to treat a football team as the **aggregation** of the **features** of its football **players**.\n",
    "\n",
    "For this reason, we got all the needed **data**:\n",
    "* **Player features**: dataset on [Kaggle](https://www.kaggle.com/datasets/stefanoleone992/fifa-21-complete-player-dataset ) + [**custom-made** scraping](https://github.com/Big-Data-FC/scraper).\n",
    "* **Teams**' end-of-the-season points: dataset on [Kaggle](https://www.kaggle.com/datasets/josephvm/european-club-football-dataset).\n",
    "* **Cross-linking** of the two datasets: custom, hand-made dataset.\n",
    "\n",
    "We treated the problem as **regression** and **classification**, using a total of **nine** different Learning **models**.\n",
    "\n",
    "After the typical data **visualizations**, we started with a naive attempt, in which we considered all possible features.<br>\n",
    "This resulted in sub-optimal performances, most likely due to **high variance** in the data curse of dimensionality.\n",
    "\n",
    "To counter these two issues, we opted for **feature selection**, performed by the following methods:\n",
    "* **P**rincipal **C**omponent **A**nalysis\n",
    "* t-SNE\n",
    "* **U**nivariate **F**eature **S**election\n",
    "* `overall` as a feature\n",
    "* `value` as a feature\n",
    "\n",
    "Results did not change that much, but offered us the motivation for looking at the **scientific** literature, in which we found two **papers** that confirmed our **observations** and reached similar results:\n",
    "* [\"Who wins the championship?\"](https://www.researchgate.net/publication/312418756_Who_wins_the_championship_Market_value_and_team_composition_as_predictors_of_success_in_the_top_European_football_leagues)\n",
    "* [\"Causes of Success in the La Liga and How to Predict Them\"](https://ieeexplore.ieee.org/document/8796732)\n",
    "\n",
    "Not happy, we decided to dig deeper and moved from \"plain\" features to learning-produced features, by means of performing **K-Means clustering**.\n",
    "\n",
    "After seeing results of **Elbow method** and **Silhouette coefficients**, we proceeded to perform **learning**, which once again did not produce results so different from the previous attempts and the papers.\n",
    "\n",
    "Furthermore, papers observations have been found in Clustering as well.\n",
    "\n",
    "Finally, taking inspiration from Deep Learning theory, we tried a **prior-based approach**, using rankings of previous years as **rewards** or **penalties** (RP), in attempt to reduce variance.<br>\n",
    "After computing the prior and applying it, learning has been performed but, unfortunately, it produced pretty much the **same results** of previous attempts.<br>\n",
    "In some cases, RP coefficient even worsened the situation.\n",
    "\n",
    "In general, the different data visualizations presented us data that appeared to be **non-linearly separable**.<br>\n",
    "Thus, non-linear approaches may be needed, asking the intervention of Deep Learning.<br>\n",
    "However, this path has not been explored, because of data constraints: using Deep Neural Networks on less than 2k entires requires very advanced models and techniques, which we do not master (yet)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bdc-scraper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dbee12c48506d0ea52066729cdbd5c841c92d7e5282187c802887604930b19f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
