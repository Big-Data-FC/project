{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the **Big Data FC** project is to **predict** how many **points** a **football team** belonging to the main European football leagues will end the season with, according to the **characteristics of its players**.\n",
    "\n",
    "To reach the goal, data relative to the **football players** will first be loaded, in order to then compose the **football teams**.\n",
    "After that, a second dataset will be used to gather seasonal **rankings**, for every football team.\n",
    "\n",
    "The project as a whole is composed of:\n",
    "\n",
    "* This **notebook**, containing all steps of:\n",
    "  * Data loading.\n",
    "  * Data cleaning and pre-processing\n",
    "  * Data visualization.\n",
    "  * Data analysis.\n",
    "  * Learning and evaluation.\n",
    "* A custom [**scraper**](https://github.com/Big-Data-FC/scraper), to gather further players data.\n",
    "* A set of **REST APIs** to query the loaded data and the prediction model.\n",
    "* The collection of [scraped datasets](https://github.com/Big-Data-FC/datasets).\n",
    "\n",
    "During the project, multiple approaches and techniques were explored, all described in this notebook.\n",
    "\n",
    "_By [Daniele Solombrino](https://github.com/dansolombrino) and [Davide Quaranta](https://github.com/fortym2)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, a series of components that are used across the entire project are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PySpark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "import gc\n",
    "\n",
    "import builtins\n",
    "import operator\n",
    "import json\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "%matplotlib widget\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# import itertools\n",
    "\n",
    "\n",
    "# import copy\n",
    "\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "# from pyspark.ml.feature import StandardScaler\n",
    "# from pyspark.ml.feature import PCA\n",
    "\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from pyspark.ml.clustering import KMeans\n",
    "# from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# RANDOM_SEED = None\n",
    "\n",
    "# from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "# from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# from pyspark_dist_explore import hist\n",
    "\n",
    "# from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# from pyspark.ml import Pipeline\n",
    "\n",
    "# from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# from pyspark.ml.tuning import CrossValidator\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# from operator import itemgetter\n",
    "\n",
    "# from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# from pyspark.ml.stat import Summarizer\n",
    "\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# from pyspark.ml.feature import UnivariateFeatureSelector\n",
    "# from pyspark.ml.linalg import DenseVector\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt \n",
    "# import seaborn as sns\n",
    "# import re\n",
    "\n",
    "# import sklearn\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.preprocessing import scale\n",
    "# from sklearn.feature_selection import RFE\n",
    "# # from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.linear_model import Lasso\n",
    "# # from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# # from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "# import warnings # supress warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell initializes the Spark context, with a given configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/21 08:33:51 WARN Utils: Your hostname, RTX-2070-Rig resolves to a loopback address: 127.0.1.1; using 192.168.1.189 instead (on interface wlp7s0)\n",
      "22/06/21 08:33:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/21 08:33:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.ui.port\", \"4050\")\n",
    "    .set(\"spark.executor.memory\", \"4G\")\n",
    "    .set(\"spark.driver.memory\", \"20G\")\n",
    "    .set(\"spark.driver.maxResultSize\", \"10G\")\n",
    ")\n",
    "# .set(\"spark.master\", \"spark://192.168.1.189:4050\")\n",
    "\n",
    "\n",
    "# create the context\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading football players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to load football players data, which comes from to two different sources:\n",
    "\n",
    "* For seasons between 2015 and 2020 (called \"modern\"): [FIFA 15-21 complete dataset](https://www.kaggle.com/datasets/stefanoleone992/fifa-21-complete-player-dataset)\n",
    "\n",
    "* For season between 2007 and 2014 (called \"legacy\"): scraped data from [sofifa.com](https://sofifa.com), a website specialized in storing data taken from EA Sports FIFA games.\n",
    "\n",
    "As introduced before, scraped datasets are committed in a [GitHub repository](https://github.com/Big-Data-FC/datasets).\n",
    "\n",
    "From now on, the terms \"modern\" and \"legacy\" will be used to refer to the two kinds of datasets.\n",
    "\n",
    "Initially, modern and legacy data will be splitted in two different dataframes, since there are some differences in the structure of their data, among these different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "modern_df = spark.read.csv(\n",
    "    \"data/players_*.csv\", sep=\",\", inferSchema=True, header=True, multiLine=True\n",
    ")\n",
    "\n",
    "legacy_df = spark.read.csv(\n",
    "    \"data/scraped_players_*.csv\", sep=\",\", inferSchema=True, header=True, \n",
    "    multiLine=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing football players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to focus the project on the major European leagues, it is useful to define a list of leagues to filter, and also to define a list of season to easily discriminate between modern an legacy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the European Leagues supported by Big-Data-FC\n",
    "leagues = [\n",
    "    \"Italian Serie A\",\n",
    "    # \"Spain Primera Division\",\n",
    "    # \"German 1. Bundesliga\",\n",
    "    # \"French Ligue 1\",\n",
    "    # \"English Premier League\",\n",
    "    # \"Holland Eredivisie\",\n",
    "]\n",
    "\n",
    "# These are the seasons supported by Big-Data-FC\n",
    "# seasons_modern = [\"20\", \"19\", \"18\", \"17\", \"16\", \"15\", \"14\"] \n",
    "seasons_modern = [\"20\"] \n",
    "# seasons_legacy = [\"13\", \"12\", \"11\", \"10\", \"09\", \"08\", \"07\"]\n",
    "seasons_legacy = [\"13\"]\n",
    "seasons = seasons_legacy + seasons_modern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next definition is about **macro roles**, which is a custom-defined abstration to aggregate affine football roles.\n",
    "\n",
    "For example, all the midfield roles such as \"central midfielder\", \"advanced midfielder\", \"left|right wing\" can be **grouped together** in the same macro role \"midfielder\".\n",
    "\n",
    "Macro roles will be used later on, in a subsequent learning phase. For this reason, much more about them will be touched in future points.\n",
    "\n",
    "The following cell defines the actual aggregation from FIFA roles abbreviations into macro roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_roles = [\"0.0\", \"1.0\", \"2.0\", \"3.0\"]\n",
    "\n",
    "roles_to_macro_roles_dict = {\n",
    "    \"GK\": \"0\",\n",
    "    \"LB\": \"1\",\n",
    "    \"RB\": \"1\",\n",
    "    \"RWB\": \"1\",\n",
    "    \"LWB\": \"1\",\n",
    "    \"CB\": \"1\",\n",
    "    \"CDM\": \"2\",\n",
    "    \"CM\": \"2\",\n",
    "    \"RM\": \"2\",\n",
    "    \"LM\": \"2\",\n",
    "    \"CAM\": \"2\",\n",
    "    \"RW\": \"3\",\n",
    "    \"LW\": \"3\",\n",
    "    \"ST\": \"3\",\n",
    "    \"LF\": \"3\",\n",
    "    \"RF\": \"3\",\n",
    "    \"CF\": \"3\",\n",
    "}\n",
    "\n",
    "NUM_MACRO_ROLES = 4\n",
    "\n",
    "roles_to_macro_role_UDF = udf(\n",
    "    lambda roles: float(\n",
    "        roles_to_macro_roles_dict[roles.split(\",\")[0]]\n",
    "    ), \n",
    "    StringType()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the columns associeted to players also contain graphical or data that is generally not informative for the project's purpose (such as shirt number, celebration moves, etc), a list of meaningful columns has been defined, on which the actual working dataframes will be based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"short_name\",\n",
    "    \"club_name\",\n",
    "    \"league_name\",\n",
    "    \"season\",\n",
    "    \"player_positions\",\n",
    "    \"macro_role\",\n",
    "    \"overall\",\n",
    "    \"value\",\n",
    "    \"pace\",\n",
    "    \"shooting\",\n",
    "    \"passing\",\n",
    "    \"dribbling\",\n",
    "    \"defending\",\n",
    "    \"physic\",\n",
    "    \"attacking_crossing\",\n",
    "    \"attacking_finishing\",\n",
    "    \"attacking_heading_accuracy\",\n",
    "    \"attacking_short_passing\",\n",
    "    \"skill_dribbling\",\n",
    "    \"skill_fk_accuracy\",\n",
    "    \"skill_long_passing\",\n",
    "    \"skill_ball_control\",\n",
    "    \"movement_acceleration\",\n",
    "    \"movement_sprint_speed\",\n",
    "    \"movement_reactions\",\n",
    "    \"power_shot_power\",\n",
    "    \"power_stamina\",\n",
    "    \"power_strength\",\n",
    "    \"power_long_shots\",\n",
    "    \"mentality_aggression\",\n",
    "    \"mentality_penalties\",\n",
    "    \"defending_standing_tackle\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets do **not** explicitly include the **year** (season) to which the record refers to.\n",
    "\n",
    "Rather, this information is implicitly stored in a URL (also for the non-scraped ones, which still originate from the same source), which has its own field.\n",
    "For this reason, a function to extract such information from aforementioned field is needed.\n",
    "\n",
    "As an example, a if the URL is `/player/41236/zlatan-ibrahimovic/130034/`, the corresponding season is `13` (from `/13xxxx/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(url):\n",
    "    url_split = url.split(\"/\")\n",
    "\n",
    "    # FIFA years must be scaled by a negative factor of one (i.e. 2021 has to be 2020, etc.)\n",
    "    # This is needed to ensure compatibility with the seasonal score dataset\n",
    "    return str(\n",
    "        (int(url_split[-2 if url_split[-1] == \"\" else -1][0:2]) - 1)\n",
    "    ).zfill(2)\n",
    "\n",
    "get_season_UDF = udf(lambda url: get_season(url), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the **monetary value** of players is different among the modern and legacy dataset.\n",
    "\n",
    "Specifically, the legacy one abbreviates the values into the form `€10M` to represent `€10000000`.\n",
    "\n",
    "The following function is used to convert it into the extended one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def expand_value_UDF(value):\n",
    "    value = value.replace(\"€\", \"\")\n",
    "    if value[-1] not in (\"K\", \"M\"):\n",
    "        # no abbreviation at the end\n",
    "        return value\n",
    "\n",
    "    # extract the number and the unit\n",
    "    num = value[:-1]\n",
    "    unit = value[-1]\n",
    "\n",
    "    # decide based on the unit\n",
    "    if unit == \"M\":\n",
    "        return float(num) * 1000000\n",
    "    if unit == \"K\":\n",
    "        return float(num) * 1000\n",
    "\n",
    "    return \"ERROR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the actual pre-process.\n",
    "\n",
    "Some actions are needed by legacy and modern both, whilst other are exclusive to either one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting season from the player URL, as per previous cell\n",
    "pre_processed_modern_df = modern_df.withColumn(\n",
    "    \"season\", get_season_UDF(col(\"player_url\"))\n",
    ")\n",
    "pre_processed_legacy_df = legacy_df.withColumn(\n",
    "    \"season\", get_season_UDF(col(\"player_url\"))\n",
    ")\n",
    "\n",
    "# Taking only the players playing for teams in supported Leagues, \n",
    "# in the supported seasons\n",
    "pre_processed_modern_df = pre_processed_modern_df.where(\n",
    "    (pre_processed_modern_df.league_name.isin(leagues))\n",
    "    &\n",
    "    (pre_processed_modern_df.season.isin(seasons_modern))\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.where(\n",
    "    (pre_processed_legacy_df.league_name.isin(leagues))\n",
    "    &\n",
    "    (pre_processed_legacy_df.season.isin(seasons_legacy))\n",
    ")\n",
    "\n",
    "# Dropping duplicate players\n",
    "pre_processed_modern_df = pre_processed_modern_df.dropDuplicates([\"player_url\"])\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.dropDuplicates([\"player_url\"])\n",
    "\n",
    "# Selected columns have been checked for absence of null/missing data.\n",
    "# Nevertheless, to ensure compatibility and reusability with other datasets, \n",
    "# a null-filling sweep is done\n",
    "pre_processed_modern_df = pre_processed_modern_df.na.fill(0)\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.na.fill(0)\n",
    "\n",
    "# Getting the macro role of the player, according to its field position\n",
    "pre_processed_modern_df = pre_processed_modern_df.withColumn(\n",
    "    \"macro_role\", roles_to_macro_role_UDF(col(\"player_positions\"))\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumn(\n",
    "    \"macro_role\", roles_to_macro_role_UDF(col(\"player_positions\"))\n",
    ")\n",
    "\n",
    "# Renaming the \"value_eur\" field to \"value\" to have compatiblity with legacy\n",
    "pre_processed_modern_df = pre_processed_modern_df.withColumnRenamed(\n",
    "    \"value_eur\", \"value\"\n",
    ")\n",
    "\n",
    "# Convert the monetary value to have compatibility with modern\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumn(\n",
    "    \"value\", expand_value_UDF(col(\"value\"))\n",
    ")\n",
    "\n",
    "# Renaming some legacy columns, so as they have the same name as in the modern\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"pas\", \"passing\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"dri\", \"dribbling\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.drop(col(\"defending\"))\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"def\", \"defending\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"phy\", \"physic\"\n",
    ")\n",
    "\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"sho\", \"shooting\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"pac\", \"pace\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"bov\", \"overall\"\n",
    ")\n",
    "\n",
    "# Keeping only the needed columns.\n",
    "pre_processed_modern_df = pre_processed_modern_df.select(columns)\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.select(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking whether some monetary values have not been successfully converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if pre_processed_legacy_df.select(\"value\").where(col(\"value\") == \"ERROR\").count() > 0:\n",
    "    print(\"WARN: some abbreviated monetary values were not correctly expanded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing, both dataframes have the same set of columns, so they can be concatenated together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_df = pre_processed_modern_df.unionByName(\n",
    "    pre_processed_legacy_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del legacy_df\n",
    "del modern_df\n",
    "del pre_processed_legacy_df\n",
    "del pre_processed_modern_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result of this section.\n",
    "\n",
    "For graphical reasons, only a selection of the columns will be shown, just to give an idea of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+---------------+------+----------------+----------+-------+--------+----+--------+\n",
      "|     short_name| club_name|    league_name|season|player_positions|macro_role|overall|   value|pace|shooting|\n",
      "+---------------+----------+---------------+------+----------------+----------+-------+--------+----+--------+\n",
      "|       C. Terzi|    Spezia|Italian Serie A|    20|              CB|       1.0|     68|  180000|  47|      41|\n",
      "|      G. Pegolo|  Sassuolo|Italian Serie A|    20|              GK|       0.0|     70|  180000|   0|       0|\n",
      "|      G. Buffon|  Juventus|Italian Serie A|    20|              GK|       0.0|     82| 2200000|   0|       0|\n",
      "|        P. Gori| Benevento|Italian Serie A|    20|              GK|       0.0|     62|   30000|   0|       0|\n",
      "|      G. Pandev|     Genoa|Italian Serie A|    20|          ST, RW|       3.0|     72| 1100000|  44|      74|\n",
      "|    Bruno Alves|     Parma|Italian Serie A|    20|              CB|       1.0|     72|  625000|  35|      67|\n",
      "|   G. Chiellini|  Juventus|Italian Serie A|    20|              CB|       1.0|     87|15500000|  66|      46|\n",
      "|   F. Marchetti|     Genoa|Italian Serie A|    20|              GK|       0.0|     71|  240000|   0|       0|\n",
      "|         Rafael|    Spezia|Italian Serie A|    20|              GK|       0.0|     73|  400000|   0|       0|\n",
      "|     A. Mirante|      Roma|Italian Serie A|    20|              GK|       0.0|     78| 1700000|   0|       0|\n",
      "|      R. Klavan|  Cagliari|Italian Serie A|    20|              CB|       1.0|     74| 2100000|  38|      48|\n",
      "|       A. Gómez|  Atalanta|Italian Serie A|    20|     CAM, CF, ST|       2.0|     86|34500000|  90|      79|\n",
      "|      C. Maggio| Benevento|Italian Serie A|    20|              RB|       1.0|     74| 1000000|  69|      69|\n",
      "|  F. Magnanelli|  Sassuolo|Italian Serie A|    20|             CDM|       2.0|     75| 2100000|  56|      55|\n",
      "|      A. Cordaz|   Crotone|Italian Serie A|    20|              GK|       0.0|     72|  325000|   0|       0|\n",
      "|      A. Rosati|    Torino|Italian Serie A|    20|              GK|       0.0|     69|  110000|   0|       0|\n",
      "|       A. Young|     Inter|Italian Serie A|    20|      LM, RM, RB|       2.0|     77| 4500000|  69|      67|\n",
      "|     R. Palacio|   Bologna|Italian Serie A|    20|         ST, CAM|       3.0|     75| 2400000|  72|      71|\n",
      "|      F. Ribéry|Fiorentina|Italian Serie A|    20|      CF, LM, LW|       3.0|     81| 6500000|  74|      74|\n",
      "|F. Quagliarella| Sampdoria|Italian Serie A|    20|              ST|       3.0|     80| 5500000|  64|      85|\n",
      "+---------------+----------+---------------+------+----------------+----------+-------+--------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pre_processed_df.select(*columns[0:10]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete list of columns is (again for graphical reasons) printed here in an horizontal format, where each item is a tuple of the form `(field_name, type)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('short_name', 'string'), ('club_name', 'string'), ('league_name', 'string'), ('season', 'string'), ('player_positions', 'string'), ('macro_role', 'string'), ('overall', 'int'), ('value', 'string'), ('pace', 'int'), ('shooting', 'int'), ('passing', 'int'), ('dribbling', 'int'), ('defending', 'int'), ('physic', 'int'), ('attacking_crossing', 'int'), ('attacking_finishing', 'int'), ('attacking_heading_accuracy', 'int'), ('attacking_short_passing', 'int'), ('skill_dribbling', 'int'), ('skill_fk_accuracy', 'int'), ('skill_long_passing', 'int'), ('skill_ball_control', 'int'), ('movement_acceleration', 'int'), ('movement_sprint_speed', 'int'), ('movement_reactions', 'int'), ('power_shot_power', 'int'), ('power_stamina', 'int'), ('power_strength', 'int'), ('power_long_shots', 'int'), ('mentality_aggression', 'int'), ('mentality_penalties', 'int'), ('defending_standing_tackle', 'int')]\n"
     ]
    }
   ],
   "source": [
    "print(pre_processed_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building football teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_teams_df = pre_processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having pre-processed the football players, it's time to build the football teams.\n",
    "\n",
    "For the sake of the learning stage of the project, **teams are differentiated across different seasons**; for example, Real Madrid of 2020 is **different** than Real Madrid of 2018.\n",
    "\n",
    "A football team is then modeled as **the set of the average of the features of all of its football players**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that are considered as features\n",
    "PLAYER_FEATURES = [\n",
    "    \"overall\",\n",
    "    \"value\",\n",
    "    \"pace\",\n",
    "    \"shooting\",\n",
    "    \"passing\",\n",
    "    \"dribbling\",\n",
    "    \"defending\",\n",
    "    \"physic\",\n",
    "    \"attacking_crossing\",\n",
    "    \"attacking_finishing\",\n",
    "    \"attacking_heading_accuracy\",\n",
    "    \"attacking_short_passing\",\n",
    "    \"skill_dribbling\",\n",
    "    \"skill_fk_accuracy\",\n",
    "    \"skill_long_passing\",\n",
    "    \"skill_ball_control\",\n",
    "    \"movement_acceleration\",\n",
    "    \"movement_sprint_speed\",\n",
    "    \"movement_reactions\",\n",
    "    \"power_shot_power\",\n",
    "    \"power_stamina\",\n",
    "    \"power_strength\",\n",
    "    \"power_long_shots\",\n",
    "    \"mentality_aggression\",\n",
    "    \"mentality_penalties\",\n",
    "    \"defending_standing_tackle\"\n",
    "]\n",
    "\n",
    "# Apposing the avg pre-fix to features\n",
    "PLAYER_FEATURES_AVG = [\n",
    "    \"avg(\" + player_feature + \")\" for player_feature in PLAYER_FEATURES\n",
    "]\n",
    "\n",
    "# Target variable of the learning stage\n",
    "TARGET_VARIABLE = \"points\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composing the football team, as introduced before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_teams_df = football_teams_df.select(\n",
    "    \"season\", \"club_name\", *PLAYER_FEATURES\n",
    ").groupBy(\n",
    "    [\"season\", \"club_name\"]\n",
    ").agg(\n",
    "    { player_feature: \"avg\" for player_feature in PLAYER_FEATURES }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result of this section.\n",
    "\n",
    "For graphical reasons, only a selection of the columns will be shown, just to give an idea of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------------+--------------------+------------------+------------------+------------------+\n",
      "|season|    club_name|     avg(overall)|          avg(value)|         avg(pace)|     avg(shooting)|      avg(passing)|\n",
      "+------+-------------+-----------------+--------------------+------------------+------------------+------------------+\n",
      "|    20|      Udinese|70.15151515151516|  3373636.3636363638|55.484848484848484| 47.54545454545455| 51.60606060606061|\n",
      "|    20|     Cagliari|           69.625|          3688281.25|           58.4375|          51.90625|          55.46875|\n",
      "|    20|     Juventus|77.48387096774194| 1.691774193548387E7|  64.6774193548387|55.064516129032256| 62.32258064516129|\n",
      "|    20|        Genoa|70.45454545454545|   3045151.515151515| 58.03030303030303|51.333333333333336| 56.90909090909091|\n",
      "|    20|        Lazio|75.51515151515152|1.0902272727272727E7| 65.15151515151516|              55.0| 60.09090909090909|\n",
      "|    20|     Sassuolo| 70.6969696969697|   4298030.303030303|60.666666666666664|51.333333333333336| 56.36363636363637|\n",
      "|    20|       Torino|71.06060606060606|   5381060.606060606| 56.54545454545455| 45.27272727272727| 52.06060606060606|\n",
      "|    20|       Spezia|65.48387096774194|  1036290.3225806452|52.806451612903224| 42.41935483870968|47.096774193548384|\n",
      "|    20|   Fiorentina| 70.6969696969697|   4969393.939393939| 59.27272727272727|49.303030303030305|55.303030303030305|\n",
      "|    20|        Milan|             73.8|   9405666.666666666|              63.9| 53.86666666666667|              58.3|\n",
      "|    20|    Sampdoria|69.76666666666667|  3354833.3333333335|              58.3| 49.36666666666667|54.666666666666664|\n",
      "|    20|        Inter|             78.0|1.5434090909090908E7| 63.06060606060606| 58.27272727272727| 63.60606060606061|\n",
      "|    20|Hellas Verona|69.78787878787878|   2802878.787878788| 61.27272727272727| 50.45454545454545| 56.57575757575758|\n",
      "|    20|      Bologna| 70.0909090909091|  3600303.0303030303|58.303030303030305|48.666666666666664| 55.09090909090909|\n",
      "|    20|     Atalanta|             74.0|   9148484.848484848|59.666666666666664| 50.54545454545455|54.878787878787875|\n",
      "|    20|      Crotone|64.60606060606061|  1056666.6666666667| 58.72727272727273| 47.90909090909091| 51.36363636363637|\n",
      "|    20|         Roma|73.64516129032258|  7688870.9677419355|              61.0| 52.70967741935484|58.064516129032256|\n",
      "|    20|        Parma|70.41935483870968|   3093225.806451613|58.806451612903224| 50.16129032258065|52.645161290322584|\n",
      "|    20|       Napoli|76.66666666666667|1.2853030303030303E7| 65.03030303030303| 56.75757575757576| 62.18181818181818|\n",
      "|    20|    Benevento|67.48484848484848|  1720151.5151515151| 61.03030303030303| 46.96969696969697| 53.96969696969697|\n",
      "+------+-------------+-----------------+--------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "football_teams_df.select(\n",
    "    \"season\", \"club_name\", *PLAYER_FEATURES_AVG[0:5]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete list of columns with their type is (in a compact horizontal form):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('season', 'string'), ('club_name', 'string'), ('avg(attacking_short_passing)', 'double'), ('avg(shooting)', 'double'), ('avg(power_stamina)', 'double'), ('avg(skill_long_passing)', 'double'), ('avg(power_strength)', 'double'), ('avg(defending_standing_tackle)', 'double'), ('avg(skill_fk_accuracy)', 'double'), ('avg(skill_dribbling)', 'double'), ('avg(dribbling)', 'double'), ('avg(pace)', 'double'), ('avg(mentality_aggression)', 'double'), ('avg(movement_reactions)', 'double'), ('avg(movement_sprint_speed)', 'double'), ('avg(passing)', 'double'), ('avg(movement_acceleration)', 'double'), ('avg(attacking_heading_accuracy)', 'double'), ('avg(attacking_finishing)', 'double'), ('avg(defending)', 'double'), ('avg(attacking_crossing)', 'double'), ('avg(power_long_shots)', 'double'), ('avg(mentality_penalties)', 'double'), ('avg(overall)', 'double'), ('avg(power_shot_power)', 'double'), ('avg(value)', 'double'), ('avg(physic)', 'double'), ('avg(skill_ball_control)', 'double')]\n"
     ]
    }
   ],
   "source": [
    "print(football_teams_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading football teams seasonal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having dealt with the football players and composed them into football clubs, it's time to get **\"target\" data** for the learning stage: the final ranking, for every team of every year.\n",
    "\n",
    "First step is to load the data from disk, which has been taken from the [European Football Dataset](https://www.kaggle.com/datasets/josephvm/european-club-football-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_scores_df = (\n",
    "    spark.read.csv(\n",
    "        \"data/all_tables_fixed_renamed_leagues.csv\",\n",
    "        sep=\",\",\n",
    "        inferSchema=True,\n",
    "        header=True,\n",
    "        multiLine=True,\n",
    "    )\n",
    "    .withColumnRenamed(\"Year\", \"season\")\n",
    "    .withColumnRenamed(\"Team\", \"club_name\")\n",
    "    .withColumnRenamed(\"P\", \"points\")\n",
    "    .withColumnRenamed(\"Place\", \"place\")\n",
    "    .withColumnRenamed(\"League\", \"league\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing football teams seasonal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all the available columns, only the ones in `seasonal_scores_columns` will be taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_scores_columns = [\n",
    "    \"season\", \"league\", \"club_name\", \"points\", \"place\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seasonal scores dataset uses **abbreviated version of football team names** (for example: `BAR` for Barcelona, `LEI` for Leicester, etc.), which would cause **incompatibility** with modern and legacy FIFA datasets, which instead uses complete names.\n",
    "\n",
    "Furthermore, the abbreviations are **not standard**, so there are **conflicts** such as, among others:\n",
    "\n",
    "* `BAR` both for Barcelona and Bari (different leagues).\n",
    "* `HUE` both for Huelva and Huesca (same league).\n",
    "\n",
    "For this reason, a **custom** hand-made **mapping** procedure has been developed in order to resolve such conflicts.\n",
    "\n",
    "Another source of incompatibility originated from **inconsistencies** within FIFA datasets, from year to year; for example, some teams had slight variations in their names (e.g. Torino and Torino FC).\n",
    "\n",
    "All said inconsistencies have been **manually solved** and disambiguated at the dataset-level.\n",
    "\n",
    "Furthermore, the final **mapping** between abbreviations and FIFA names resulted into a **custom-made dataset** (also available in the linked dataset GitHub repo), which is the following form:\n",
    "\n",
    "|abbr|league|club_name|fifa_club_name|\n",
    "|---|---|---|---|\n",
    "|AAC|German Bundesliga|Aachen|Alemannia Aachen|\n",
    "|ADO|Dutch Eredivisie|Ado Den Haag|ADO Den Haag|\n",
    "|AJA|Dutch Eredivisie|Ajax|Ajax|\n",
    "|AJC|French Ligue 1|Ajaccio|AC Ajaccio|\n",
    "|ALB|Spanish La Liga|Albacete|Albacete BP|\n",
    "|...|...|...|...|\n",
    "\n",
    "Aforementioned mapping is then converted to **JSON** with an utility script in the same repository, and loaded here in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/clubs_map.json\")\n",
    "club_name_abbr_to_ext = json.load(f)\n",
    "f.close()\n",
    "\n",
    "ABBREVIATED_CLUB_NAME_NOT_FOUD = \"ABBREVIATED_CLUB_NAME_NOT_FOUD\"\n",
    "GENERAL_EXCEPTION = \"GENERAL_EXCEPTION\"\n",
    "\n",
    "def extend_club_name(club_name_abbr):\n",
    "    try:\n",
    "        return club_name_abbr_to_ext[club_name_abbr]\n",
    "    except KeyError as e:\n",
    "        return ABBREVIATED_CLUB_NAME_NOT_FOUD\n",
    "    except Exception as e:\n",
    "        return GENERAL_EXCEPTION\n",
    "\n",
    "extend_club_name_UDF = udf(\n",
    "    lambda club_name_abbr: extend_club_name(str(club_name_abbr)),\n",
    "    StringType(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rankings dataset the seasons are expressed as `YYYY`, whilst FIFA uses the `YY` encoding.\n",
    "\n",
    "For this reason, to guarantee compatibility, season values in the rankings dataset is abbreviated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviate_season_UDF = udf(\n",
    "    lambda season: str(season)[-2:],\n",
    "    StringType(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the actual **data pre-processing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_seasonal_scores_df = seasonal_scores_df\n",
    "\n",
    "# Abbreviating season, as per previous cell\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"season\", abbreviate_season_UDF(col(\"season\"))\n",
    ")\n",
    "\n",
    "# Keeping only supported leagues in supported seasons\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.where(\n",
    "    (pre_processed_seasonal_scores_df.season.isin(seasons))\n",
    "    & \n",
    "    (pre_processed_seasonal_scores_df.league.isin(leagues))\n",
    ")\n",
    "\n",
    "# Selecting only the desired columns\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.select(\n",
    "    seasonal_scores_columns\n",
    ")\n",
    "\n",
    "# Although data has been checked for duplicates and missing value, to ensure \n",
    "# operabiloty with other datasets, the pre-processing steps are still performed\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.dropDuplicates(\n",
    "    seasonal_scores_columns\n",
    ")\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.na.fill(0)\n",
    "\n",
    "# Extending club names, as per previous cell\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"club_name\", extend_club_name_UDF(col(\"club_name\"))\n",
    ")\n",
    "# Checking whether club name expansions went all good or not\n",
    "if pre_processed_seasonal_scores_df.filter(\n",
    "    col(\"club_name\") == ABBREVIATED_CLUB_NAME_NOT_FOUD\n",
    ").count() > 0:\n",
    "    print(\"WARN: some clubs have NOT been found\")\n",
    "    print(\"Please check your data\")\n",
    "\n",
    "# Casting points to float, as required by learning procedures\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"points\", pre_processed_seasonal_scores_df.points.cast(DoubleType())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result of this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------------+------+-----+\n",
      "|season|         league|    club_name|points|place|\n",
      "+------+---------------+-------------+------+-----+\n",
      "|    20|Italian Serie A|    Sampdoria|  52.0|    9|\n",
      "|    20|Italian Serie A|Hellas Verona|  45.0|   10|\n",
      "|    13|Italian Serie A|      Udinese|  44.0|   13|\n",
      "|    13|Italian Serie A|Hellas Verona|  54.0|   10|\n",
      "|    13|Italian Serie A|       Torino|  57.0|    7|\n",
      "|    13|Italian Serie A|    Sampdoria|  45.0|   12|\n",
      "|    20|Italian Serie A|      Bologna|  41.0|   12|\n",
      "|    13|Italian Serie A|   Fiorentina|  65.0|    4|\n",
      "|    20|Italian Serie A|        Genoa|  42.0|   11|\n",
      "|    20|Italian Serie A|    Benevento|  33.0|   18|\n",
      "|    13|Italian Serie A|         Roma|  85.0|    2|\n",
      "|    13|Italian Serie A|        Lazio|  56.0|    9|\n",
      "|    20|Italian Serie A|      Udinese|  40.0|   14|\n",
      "|    20|Italian Serie A|        Inter|  91.0|    1|\n",
      "|    13|Italian Serie A|      Bologna|  29.0|   19|\n",
      "|    20|Italian Serie A|         Roma|  62.0|    7|\n",
      "|    13|Italian Serie A|        Milan|  57.0|    8|\n",
      "|    13|Italian Serie A|        Inter|  60.0|    5|\n",
      "|    13|Italian Serie A|      Catania|  32.0|   18|\n",
      "|    13|Italian Serie A|        Parma|  58.0|    6|\n",
      "+------+---------------+-------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_processed_seasonal_scores_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining football teams features with their seasonal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two dataframes (players and seasonal scores) need to be merged together to form a single dataframe.\n",
    "This can easily be done by joining on the key (`season`, `club_name`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = football_teams_df.join(\n",
    "    pre_processed_seasonal_scores_df,\n",
    "    on = [\"season\", \"club_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check whether some clubs were left out by the aforementioned join operation, two differences are computed:\n",
    "\n",
    "* Seasonal scores dataframe - joined dataframe.\n",
    "* Football teams dataframe - joined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = pre_processed_seasonal_scores_df.select(\"club_name\").subtract(df.select(\"club_name\")).distinct()\n",
    "\n",
    "if diff.count() > 0:\n",
    "    print(\"WARN: Some football teams have been left out the join (pre_processed_seasonal_scores_df)\")\n",
    "    diff.show()\n",
    "\n",
    "diff = football_teams_df.select(\"club_name\").subtract(df.select(\"club_name\")).distinct()\n",
    "if diff.count() > 0:\n",
    "    print(\"WARN: Some football teams have been left out the join (football_teams_df)\")\n",
    "    diff.show()\n",
    "\n",
    "del diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result of this section.\n",
    "\n",
    "For graphical reasons, only a selection of the columns will be shown, just to give an idea of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------------+-----------------+------------------+------+-----+\n",
      "|season|         league|    club_name|     avg(overall)|         avg(pace)|points|place|\n",
      "+------+---------------+-------------+-----------------+------------------+------+-----+\n",
      "|    20|Italian Serie A|      Udinese|70.15151515151516|55.484848484848484|  40.0|   14|\n",
      "|    20|Italian Serie A|     Cagliari|           69.625|           58.4375|  37.0|   16|\n",
      "|    20|Italian Serie A|     Juventus|77.48387096774194|  64.6774193548387|  78.0|    4|\n",
      "|    20|Italian Serie A|        Genoa|70.45454545454545| 58.03030303030303|  42.0|   11|\n",
      "|    20|Italian Serie A|        Lazio|75.51515151515152| 65.15151515151516|  68.0|    6|\n",
      "|    20|Italian Serie A|     Sassuolo| 70.6969696969697|60.666666666666664|  62.0|    8|\n",
      "|    20|Italian Serie A|       Torino|71.06060606060606| 56.54545454545455|  37.0|   17|\n",
      "|    20|Italian Serie A|       Spezia|65.48387096774194|52.806451612903224|  39.0|   15|\n",
      "|    20|Italian Serie A|   Fiorentina| 70.6969696969697| 59.27272727272727|  40.0|   13|\n",
      "|    20|Italian Serie A|        Milan|             73.8|              63.9|  79.0|    2|\n",
      "|    20|Italian Serie A|    Sampdoria|69.76666666666667|              58.3|  52.0|    9|\n",
      "|    20|Italian Serie A|        Inter|             78.0| 63.06060606060606|  91.0|    1|\n",
      "|    20|Italian Serie A|Hellas Verona|69.78787878787878| 61.27272727272727|  45.0|   10|\n",
      "|    20|Italian Serie A|      Bologna| 70.0909090909091|58.303030303030305|  41.0|   12|\n",
      "|    20|Italian Serie A|     Atalanta|             74.0|59.666666666666664|  78.0|    3|\n",
      "|    20|Italian Serie A|      Crotone|64.60606060606061| 58.72727272727273|  23.0|   19|\n",
      "|    20|Italian Serie A|         Roma|73.64516129032258|              61.0|  62.0|    7|\n",
      "|    20|Italian Serie A|        Parma|70.41935483870968|58.806451612903224|  20.0|   20|\n",
      "|    20|Italian Serie A|       Napoli|76.66666666666667| 65.03030303030303|  77.0|    5|\n",
      "|    20|Italian Serie A|    Benevento|67.48484848484848| 61.03030303030303|  33.0|   18|\n",
      "+------+---------------+-------------+-----------------+------------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"season\", \"league\", \"club_name\", \"avg(overall)\", \"avg(pace)\", \"points\", \"place\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full list of columns with their type is (in a compact horizontal form):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('season', 'string'), ('club_name', 'string'), ('avg(attacking_short_passing)', 'double'), ('avg(shooting)', 'double'), ('avg(power_stamina)', 'double'), ('avg(skill_long_passing)', 'double'), ('avg(power_strength)', 'double'), ('avg(defending_standing_tackle)', 'double'), ('avg(skill_fk_accuracy)', 'double'), ('avg(skill_dribbling)', 'double'), ('avg(dribbling)', 'double'), ('avg(pace)', 'double'), ('avg(mentality_aggression)', 'double'), ('avg(movement_reactions)', 'double'), ('avg(movement_sprint_speed)', 'double'), ('avg(passing)', 'double'), ('avg(movement_acceleration)', 'double'), ('avg(attacking_heading_accuracy)', 'double'), ('avg(attacking_finishing)', 'double'), ('avg(defending)', 'double'), ('avg(attacking_crossing)', 'double'), ('avg(power_long_shots)', 'double'), ('avg(mentality_penalties)', 'double'), ('avg(overall)', 'double'), ('avg(power_shot_power)', 'double'), ('avg(value)', 'double'), ('avg(physic)', 'double'), ('avg(skill_ball_control)', 'double'), ('league', 'string'), ('points', 'double'), ('place', 'int')]\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del seasonal_scores_df\n",
    "del pre_processed_seasonal_scores_df\n",
    "del football_teams_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good practice whenever a data-driven task is being taclked is to first visualize the data.\n",
    "\n",
    "In fact, even just by simply looking at data we may find some useful information which may have a direct impact on the subsequent learning phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points distribution\n",
    "\n",
    "Kicking the visualization stage off with data distirbutions for the end-of-season points, across all supported seasons, for all supported leagues.\n",
    "\n",
    "Data distribution is very important to monitor: exposing a model too many or too few time to a value may be source of bias, which we want to avoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.displot(df.select(\"points\").toPandas(), kde=True)\n",
    "p.fig.tight_layout(pad=1.5)\n",
    "plt.title(\"Points distribution across all leagues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations can be done:\n",
    "\n",
    "1. Data tends to approximatively follow a Gaussian/Normal distribution\n",
    "2. Said distribution appears to be slighlty skewed towards left\n",
    "\n",
    "These observations perfectly coincide with domain knowledge.\n",
    "1. In every league, there are just a few very strong teams (Champions League qualifiers), a limited number of very bad teams (fighting for relegation) and then a multitude of middle-table teams\n",
    "2. Some leagues do not have much talent in the middle part of the table, resulting in a general \"equilibrium\" between such team.\n",
    "As a result, scores of such teams not be high, but rather close to the mean value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following density plot instead focuses on each league, showing again the points distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.displot(df.toPandas(), x=\"points\", hue=\"league\", kind=\"kde\", aspect=1.7, palette=\"tab10\")\n",
    "p.fig.tight_layout(pad=1.5)\n",
    "plt.title(\"Points distribution per league\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations can be made:\n",
    "\n",
    "* The **French** and **Spanish** leagues are the less skewed among all leagues, possibly meaning that they are characterized by a majority of \"average\" teams.\n",
    "* The **Italian** and **English** leagues are very similar on low and middle points, but start to behave slightly different on the middle-high and high parts of the table.\n",
    "* The **Dutch** league is the most left-skewed, possibly hinting at the lower quality of the league, as suggested by the domain knowledge.\n",
    "* The **Spanish** league has a boost on high points, hinting at a consistent dominancy of one or more teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall player quality distribution\n",
    "\n",
    "Similar visualizations can be done to analyze the overall player quality.\n",
    "\n",
    "The following plot shows the distribution of the player quality (`avg(overall)`) across all leagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.displot(df.select(\"avg(overall)\").toPandas(), kde=True)\n",
    "p.fig.tight_layout(pad=1.5)\n",
    "plt.title(\"Player quality across all leagues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarily to what observed for the points, the plot shows that:\n",
    "\n",
    "* A vast majority of the players are of average quality.\n",
    "* A minority of the players are below-average.\n",
    "* A minority of the players are above-average.\n",
    "\n",
    "The following plot focuses on the single leagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.displot(df.toPandas(), x=\"avg(overall)\", hue=\"league\", kind=\"kde\", aspect=1.7, palette=\"tab10\")\n",
    "p.fig.tight_layout(pad=1.5)\n",
    "plt.title(\"Player quality per league\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot it is possible to observe that:\n",
    "\n",
    "* The Dutch league hosts the majority of below-average players.\n",
    "* The French league follows the Dutch one in below-average players.\n",
    "* The German league has the highest amount of average players.\n",
    "* The German, Engligh, Italian and Spanish leagues have a peak of average players.\n",
    "* The same leagues have a considerable amount of above-average players.\n",
    "* The Spanish leagues dominates on the above-average players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1: \"naive\" player features\n",
    "\n",
    "The following section represents the first attempt at building a model able to predict the points of a team, considering the features of its players.\n",
    "\n",
    "This attempt has been renamed as \"naive\", meaning that it simply consist of an analysis based on **all the features** that are related to a **player's technical abilities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that do not inherently represent players abilities\n",
    "ALL_FEATURES = PLAYER_FEATURES_AVG\n",
    "ALL_FEATURES.remove(\"avg(overall)\")\n",
    "ALL_FEATURES.remove(\"avg(value)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning procedures require that the considered features are assembled in a vector.\n",
    "\n",
    "The following cell performs this operation by adding a new \"assembled\" column to the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=ALL_FEATURES, outputCol=\"all_vec\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to define a some custom utility functions to plot multiple figures, that will be called multiple times along the course of the project.\n",
    "\n",
    "The following cells contain the definition of said functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_target_relation(\n",
    "    data, x, y, n_cols = 2, figsize = (15, 30), color = \"#000000\"\n",
    "):\n",
    "\n",
    "    n_rows = int(len(x) / n_cols) if len(x) >= n_cols else n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "    for x_ind, x_value in enumerate(x):\n",
    "        ax = sns.regplot(\n",
    "            data=data,\n",
    "            x=x_value,\n",
    "            y=y,\n",
    "            color = color,\n",
    "            ax=axes[x_ind // n_cols, x_ind % n_cols] if n_rows > 1 else axes,\n",
    "        )\n",
    "\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distribution(\n",
    "    data,\n",
    "    features,\n",
    "    figsize=(4,4), \n",
    "    color=\"#000000\",\n",
    "    n_cols=2\n",
    "):\n",
    "\n",
    "    n_rows = int(len(features) / n_cols) if len(features) >= n_cols else n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "    for feature_ind, feature in enumerate(features):\n",
    "        _ = sns.histplot(\n",
    "            data[feature],\n",
    "            kde=True,\n",
    "            color=color,\n",
    "            facecolor=color,\n",
    "            ax=axes[feature_ind // n_cols, feature_ind % n_cols] if n_rows > 1 else axes,\n",
    "        )\n",
    "\n",
    "    fig.tight_layout(pad=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(\n",
    "    data, features, title=\"Pearson Correlation Matrix\", figsize=(16,12)\n",
    "):\n",
    "\n",
    "    mask = np.zeros_like(data[features].corr(), dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    with sns.axes_style(\"white\"):  # Temporarily set the background to white\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        plt.title(title, fontsize=24)\n",
    "\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "        _ = sns.heatmap(\n",
    "            data[features].corr(),\n",
    "            linewidths=0.25,\n",
    "            vmax=0.7,\n",
    "            square=True,\n",
    "            ax=ax,\n",
    "            cmap=cmap,\n",
    "            linecolor=\"w\",\n",
    "            annot=True,\n",
    "            annot_kws={\"size\": 8},\n",
    "            mask=mask,\n",
    "            cbar_kws={\"shrink\": 0.9},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before learning, it is good practice to first gather in-depth information on the structure of the data, as well as obtaining useful visualizations.\n",
    "\n",
    "In the following sections, multiple approaches at data visualization will be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section a first analysis is performed on _raw_ data, meaning that no normalization or standardization is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color to be used in the plots for raw data\n",
    "COLOR_RAW = \"#332FD0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Pandas for easy plotting with Seaborn\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **feature-target correlation** shows the correlation between each feature and the target variable (point).\n",
    "\n",
    "The idea is to immediately visualize whether there are specific features that are particularly correlated with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES, TARGET_VARIABLE, color=COLOR_RAW\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are too many datapoints, it is very hard to look at the plots. Let's see how visualizations change when focusing on some leagues or years, thus restricting the data scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf[pdf[\"season\"] == \"20\"], ALL_FEATURES, TARGET_VARIABLE, color=COLOR_RAW\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that:\n",
    "\n",
    "1. There is a general phenomenon of linear relation between each feature and the final points.\n",
    "2. There is a generalized high variance across all features.\n",
    "   1. E.g. taking the very last feature (`avg(defending_standing_tackle`), players with a value close to `50` result to a wide range of points.\n",
    "   2. On the contrary, in extreme values (close to `40` or `90`), the resulting variable have a narrower range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the distribution of the player features in the analyzed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES, color = COLOR_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot it is possible to observe that:\n",
    "\n",
    "1. Most features have a symmetrical distribution.\n",
    "2. Most features are centered around the middle.\n",
    "3. Some features are more skewed towards left, hinting that they may be more \"rare\".\n",
    "4. Some features don't properly follow a Gaussian distribution (`avg(pace)`, `avg(physic)`).\n",
    "\n",
    "Furthermore, observation `2` is linked to the observation `2.2` of the previous plot:\n",
    "\n",
    "* A lot of features have an average value.\n",
    "* Around the average values, there is high variance.\n",
    "\n",
    "Hence, most likely, the system will be exposed multiple times to this issue, affecting performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Pearson's Correlation Matrix** is a graphical tool to show a numerical value indicating the correlation between each variable.\n",
    "\n",
    "It is employed in the project, to find whether some features are correlated to each other, which can either be a problem for Linear Regression, but not so much for Tree-based models, since they by design perform a feature selection step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(pdf, ALL_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are:\n",
    "\n",
    "* A lot of positive correlations (red).\n",
    "* Some negative correlations (teal).\n",
    "* Some feature are tendentially not correlated to others (white).\n",
    "\n",
    "Using some domain knowledge, it is possible to further comment some correlations, like (among others):\n",
    "\n",
    "* The more a defender is good, the more aggressive they are.\n",
    "* The more a defender is good, the more able to perform standing tackles they are.\n",
    "* The more a player has good physical strenght, the more fast (pace) they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have some features with skewed distributions, it is needed to standardize, to see whether it helps to center feature distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is needed to define a scaler which takes feature from `all_vec` and places scaled versions in `all_vec_std`.\n",
    "\n",
    "The configuration is of a standard z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"all_vec\", \n",
    "    outputCol=\"all_vec_std\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_STD = \"#9254C8\"\n",
    "\n",
    "ALL_FEATURES_STD = [\n",
    "    player_feature + \"_std\" for player_feature in ALL_FEATURES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the scaler grouped all features in a single field (feature vector), but plotting requires using different fields (one per features), it is needed to define a function to extract each feature and place it in a separate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vec_to_cols(pdf, vec, columns):\n",
    "    tmp = pdf.reindex(\n",
    "        columns=list(pdf.columns) + columns\n",
    "    )\n",
    "\n",
    "    tmp[columns] = tmp[\n",
    "        vec\n",
    "    ].transform(\n",
    "        {\n",
    "            columns[i]: operator.itemgetter(i) for i, p in enumerate(columns)\n",
    "        }\n",
    "    )\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = feature_vec_to_cols(pdf, \"all_vec_std\", ALL_FEATURES_STD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see the effects of the function by looking at the columns list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As done in the _raw data_ section, a feature-target correlation analysis is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_STD, TARGET_VARIABLE, color=COLOR_STD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again zoom on a single season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf[pdf[\"season\"] == \"20\"], ALL_FEATURES_STD, TARGET_VARIABLE, color=COLOR_STD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization does not improve the variance issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf, ALL_FEATURES_STD, color = COLOR_STD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization did not change the feature distribution.\n",
    "\n",
    "It is not needed to plot the correlation matrix again, since it is not affected by a change of scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further attempt consists of apply Logarithmic Transformation, still trying to combat skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_LOG = \"#E15FED\"\n",
    "ALL_FEATURES_LOG = [\n",
    "    player_feature + \"_log\" for player_feature in ALL_FEATURES\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function translates a value to its $log_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_log_UDF = udf(\n",
    "    lambda value: float(np.log2(value)), DoubleType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, feature_log in zip(ALL_FEATURES, ALL_FEATURES_LOG):\n",
    "    df = df.withColumn(feature_log, to_log_UDF(col(feature)))\n",
    "\n",
    "df = df.withColumn(\"points_log\", to_log_UDF(col(\"points\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_LOG, \"points_log\", color=COLOR_LOG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also Logarithmic transformation did not help with the variance problem.\n",
    "\n",
    "Let's again see a zommed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf[pdf[\"season\"] == \"20\"], ALL_FEATURES_LOG, \"points_log\", color=COLOR_LOG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithmic transformation did not help with variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES_LOG, color = COLOR_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithmic transformation slightly moved the skewdness of all features towards right, still not solving it; as a consequence, centered or right-skewed features are not slightly worsened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-max transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_MIN_MAX = \"#6EDCD9\"\n",
    "ALL_FEATURES_MIN_MAX = [\n",
    "    player_feature + \"_min_max\" for player_feature in ALL_FEATURES\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"all_vec\", \n",
    "    outputCol=\"all_vec_min_max\"\n",
    ")\n",
    "\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()\n",
    "pdf = feature_vec_to_cols(pdf, vec=\"all_vec_min_max\", columns=ALL_FEATURES_MIN_MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_MIN_MAX, TARGET_VARIABLE, color=COLOR_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoomed view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf[pdf[\"season\"] == \"20\"], ALL_FEATURES_MIN_MAX, TARGET_VARIABLE, color=COLOR_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES_MIN_MAX, color=COLOR_MIN_MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following GIF compares the previously plotted feature distributions.\n",
    "\n",
    "![GIF changes illustration](https://s8.gifyu.com/images/ezgif.com-gif-maker4847ef0cb3270edd.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before learning, some further useful visualizations are given.\n",
    "\n",
    "**From now on, Min-Max transofrmed data will be used.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (on min-max normalized data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "PCA_NUM_COMPONENTS = 2\n",
    "\n",
    "def perform_pca(df, num_components, input_col, output_col):\n",
    "    pca = PCA(\n",
    "        k=num_components, \n",
    "        inputCol=input_col, \n",
    "        outputCol=output_col\n",
    "    )\n",
    "    pca_model = pca.fit(df)\n",
    "\n",
    "    return pca_model.transform(df), pca_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_explained_variance(pca_model, num_components_to_plot=2, figsize=(8,6)):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    _ = sns.barplot(\n",
    "        x=[i for i in range(num_components_to_plot)],\n",
    "        y=pca_model.explainedVariance.values[0:num_components_to_plot],\n",
    "        ax=ax,\n",
    "        palette=\"summer\"\n",
    "    )\n",
    "\n",
    "    _ = ax.set_xlabel(\"Eigenvalues\", labelpad=16, fontsize=16)\n",
    "    _ = ax.set_ylabel(\"Proportion of Variance\", fontsize=16)\n",
    "    _ = ax.set_xticklabels(\n",
    "        [f\"Principal Component {i}\" for i in range(num_components_to_plot)], \n",
    "        rotation=0\n",
    "    )\n",
    "    _ = ax.set_title(\"Explained variance of each Principal Component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, pca_model = perform_pca(\n",
    "    df=df,\n",
    "    num_components=PCA_NUM_COMPONENTS,\n",
    "    input_col=\"all_vec_min_max\",\n",
    "    output_col=\"all_vec_min_max_pcs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_explained_variance(\n",
    "    pca_model=pca_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(\n",
    "    x,\n",
    "    y,\n",
    "    x_label,\n",
    "    y_label,\n",
    "    title=\"\",\n",
    "    c=None,\n",
    "    c_map=plt.cm.get_cmap(\"tab10\"),\n",
    "    figsize=(12,8),\n",
    "    ):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    _ = plt.scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        c=y if c is None else c,\n",
    "        edgecolor=\"none\",\n",
    "        #alpha=1,\n",
    "        cmap=c_map,\n",
    "        axes=ax,\n",
    "    )\n",
    "\n",
    "    _ = ax.set_xlabel(x_label, labelpad=20, fontsize=16)\n",
    "    _ = ax.set_ylabel(y_label, fontsize=16)\n",
    "    _ = ax.set_title(title)\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=pdf.all_vec_min_max_pcs.map(lambda x: x[0]),\n",
    "    y=pdf.all_vec_min_max_pcs.map(lambda x: x[1]),\n",
    "    c=pdf.points,\n",
    "    x_label=\"Principal Component 0\",\n",
    "    y_label=\"Principal Component 1\",\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA result is quite underwhelming: teams with widely different end-of-the-season placement tend to be mixed-up.\n",
    "\n",
    "Notice that Principal Component Analysis is a linear model, so it is bounded to produce linear representations: usually, non-trivial problems present non-linear data.\n",
    "\n",
    "For this reason, adding some non-linearity may possibly lead to a better lower-dimensional embedding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=pdf.all_vec_min_max_pcs.map(lambda x: x[0]),\n",
    "    y=pdf.points,\n",
    "    x_label=\"Principal Component 0\",\n",
    "    y_label=\"Points\",\n",
    ")\n",
    "scatter_plot(\n",
    "    x=pdf.all_vec_min_max_pcs.map(lambda x: x[1]),\n",
    "    y=pdf.points,\n",
    "    x_label=\"Principal Component 1\",\n",
    "    y_label=\"Points\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is one of the most used tools, when assuming non-linearity in the data space.\n",
    "\n",
    "Implementation from SciKit Learn framework will be used, since PySpark does not provide any version of this tool.\n",
    "\n",
    "SciKit Learn works with NumPy structures, so the feature vector in PySpark DataFrame must be converted to a NDArray structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vec_min_max_np = np.array(\n",
    "    list(\n",
    "        map(\n",
    "            lambda v: v[\"all_vec_min_max\"].toArray(), \n",
    "            df.select(\"all_vec_min_max\").collect()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "points_np = np.array(\n",
    "    list(\n",
    "        map(\n",
    "            lambda v: v[\"points\"], \n",
    "            df.collect()\n",
    "        )\n",
    "    )\n",
    ").reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data is ready, TNSE is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_embedding = TSNE(\n",
    "    n_components=2, learning_rate='auto', init='random', method=\"barnes_hut\"\n",
    ").fit_transform(all_vec_min_max_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, once the embedding space is populated, the result is plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=tsne_embedding[:,0],\n",
    "    y=tsne_embedding[:,1],\n",
    "    c=points_np,\n",
    "    x_label=\"TSNE Embedding Dimension 0\",\n",
    "    y_label=\"TSNE Embedding Dimension 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=tsne_embedding[:,0],\n",
    "    y=points_np,\n",
    "    x_label=\"TSNE Embedding Dimension 0\",\n",
    "    y_label=\"Points\",\n",
    ")\n",
    "scatter_plot(\n",
    "    x=tsne_embedding[:,1],\n",
    "    y=points_np,\n",
    "    x_label=\"TSNE Embedding Dimension 1\",\n",
    "    y_label=\"Points\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, adding non-linearity does not seem to improve the situation.\n",
    "\n",
    "Not much progress has been made, w.r.t. PCA \n",
    "\n",
    "[TODO] comment on variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on visualizations\n",
    "\n",
    "Data seems to be:\n",
    "\n",
    "* Not linearly separable.\n",
    "* Suffering from high variance.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "The problem has been treated in two different forms:\n",
    "\n",
    "* As **regression** on the **points**.\n",
    "* As **classification** on the table/ranking areas (macro places).\n",
    "\n",
    "In both cases, for all the different models will be used:\n",
    "\n",
    "* **K-Fold Cross Validation**, to perform validation tests on the model performances.\n",
    "* **Grid-search approach**, to perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.ml.regression import Regressor\n",
    "from pyspark.ml.regression import LinearRegressionTrainingSummary\n",
    "from pyspark.ml.classification import Classifier\n",
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import RandomForestClassificationTrainingSummary\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationSummary\n",
    "\n",
    "from pyspark.ml.regression import RegressionModel\n",
    "from pyspark.ml.classification import ClassificationModel\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_best_model(\n",
    "    estimator, \n",
    "    param_grid, \n",
    "    feature_vec, \n",
    "    label_col, \n",
    "    prediction_col,\n",
    "    evaluation_metrics,\n",
    "    evaluation_metrics_cv\n",
    "):\n",
    "    evaluators = dict()\n",
    "\n",
    "    if isinstance(estimator, Regressor):\n",
    "        evaluators = {\n",
    "            metric: RegressionEvaluator(\n",
    "                labelCol=label_col,\n",
    "                predictionCol=prediction_col,\n",
    "                metricName=metric,\n",
    "            )\n",
    "            for metric in evaluation_metrics\n",
    "        }\n",
    "    elif isinstance(estimator, Classifier) or isinstance(estimator, OneVsRest):\n",
    "        evaluators = {\n",
    "            metric: MulticlassClassificationEvaluator(\n",
    "                labelCol=label_col,\n",
    "                predictionCol=prediction_col,\n",
    "                metricName=metric,\n",
    "            )\n",
    "            for metric in evaluation_metrics\n",
    "        }\n",
    "    else:\n",
    "        raise Exception(\"Unexpected estimator, got\" + str(type(estimator)))\n",
    "\n",
    "    cross_validations = {\n",
    "        metric: CrossValidator(\n",
    "            estimator=estimator,\n",
    "            estimatorParamMaps=param_grid,\n",
    "            evaluator=evaluators[metric],\n",
    "            numFolds=5,\n",
    "            collectSubModels=True\n",
    "        )\n",
    "        for metric in evaluation_metrics_cv\n",
    "    }\n",
    "\n",
    "    cross_validated_models = dict()\n",
    "\n",
    "    for metric in evaluation_metrics_cv:\n",
    "        cross_validated_models[metric] = cross_validations[\n",
    "            metric\n",
    "        ].fit(train_df)\n",
    "\n",
    "    return cross_validated_models, evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_learning_models(\n",
    "    cross_validated_models, \n",
    "    evaluators,\n",
    "    evaluation_metrics_cv\n",
    "):\n",
    "    for metric in evaluation_metrics_cv:\n",
    "        best_model = cross_validated_models[metric].bestModel\n",
    "        has_summary = True\n",
    "        \n",
    "        try:\n",
    "            has_summary = best_model.hasSummary\n",
    "        except AttributeError as e:\n",
    "            # since the only models that have summary\n",
    "            # have the hasSummary field,\n",
    "            # it is needed to catch this exception\n",
    "            has_summary = False\n",
    "        \n",
    "        if has_summary:\n",
    "            training_result = best_model.summary\n",
    "            \n",
    "            if isinstance(training_result, LinearRegressionTrainingSummary):\n",
    "                print(\n",
    "                    f\"\"\"\n",
    "                    Training set evaluation\n",
    "                    {type(best_model).__name__}\n",
    "                    Best model according to metric: {metric}\n",
    "                    \\tRMSE:        {builtins.round(training_result.rootMeanSquaredError, 2)}\n",
    "                    \\tR2:          {builtins.round(training_result.r2, 2)}\n",
    "                    \\tAdjusted R2: {builtins.round(training_result.r2adj, 2)}\n",
    "                    \"\"\"\n",
    "                )\n",
    "            elif isinstance(training_result, RandomForestClassificationTrainingSummary):\n",
    "                print(\n",
    "                    f\"\"\"\n",
    "                    Training set evaluation\n",
    "                    {type(best_model).__name__}\n",
    "                    Best model according to metric: {metric}\n",
    "                    \\tAccuracy:                  {builtins.round(training_result.accuracy, 2)}\n",
    "                    \\tfalsePositiveRateByLabel:  {training_result.falsePositiveRateByLabel}\n",
    "                    \\tprecisionByLabel:          {training_result.precisionByLabel}\n",
    "                    \\trecallByLabel:             {training_result.recallByLabel}\n",
    "                    \\ttruePositiveRateByLabel:   {training_result.truePositiveRateByLabel}\n",
    "                    \\tweightedFalsePositiveRate: {training_result.weightedFalsePositiveRate}\n",
    "                    \\tweightedTruePositiveRate:  {training_result.weightedTruePositiveRate}\n",
    "                    \\tweightedPrecision:         {training_result.weightedPrecision}\n",
    "                    \\tweightedRecall:            {training_result.weightedRecall}\n",
    "                    \"\"\"\n",
    "                )\n",
    "            elif isinstance(best_model, MultilayerPerceptronClassificationModel):\n",
    "                training_result = best_model.summary()\n",
    "\n",
    "                print(\n",
    "                    f\"\"\"\n",
    "                    Training set evaluation\n",
    "                    {type(best_model).__name__}\n",
    "                    Best model according to metric: {metric}\n",
    "                    \\tAccuracy:                  {builtins.round(training_result.accuracy, 2)}\n",
    "                    \\tfalsePositiveRateByLabel:  {training_result.falsePositiveRateByLabel}\n",
    "                    \\tprecisionByLabel:          {training_result.precisionByLabel}\n",
    "                    \\trecallByLabel:             {training_result.recallByLabel}\n",
    "                    \\ttruePositiveRateByLabel:   {training_result.truePositiveRateByLabel}\n",
    "                    \\tweightedFalsePositiveRate: {training_result.weightedFalsePositiveRate}\n",
    "                    \\tweightedTruePositiveRate:  {training_result.weightedTruePositiveRate}\n",
    "                    \\tweightedPrecision:         {training_result.weightedPrecision}\n",
    "                    \\tweightedRecall:            {training_result.weightedRecall}\n",
    "                    \"\"\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"\"\"\n",
    "                    Training set evaluation\n",
    "                    {type(best_model).__name__}\n",
    "                    Best model according to metric: {metric}\n",
    "                    \\tAccuracy: {builtins.round(training_result.accuracy, 2)}\n",
    "                    \\tF1:       {builtins.round(training_result.f1, 2)}\n",
    "                    \"\"\"\n",
    "                )\n",
    "\n",
    "            predictions = {\n",
    "                metric: cross_validated_models[\n",
    "                    metric\n",
    "                ].bestModel.transform(test_df)\n",
    "                for metric in evaluation_metrics_cv\n",
    "            }\n",
    "\n",
    "            for m, model in cross_validated_models.items():\n",
    "                print(\n",
    "                f\"\"\"\n",
    "                Test set evaluation\n",
    "                {type(best_model).__name__}\n",
    "                Best model according to metric {m}\n",
    "                \"\"\"\n",
    "                )\n",
    "\n",
    "                for e, evaluator in evaluators.items():\n",
    "                    print(f\"{evaluator.getMetricName()}: {evaluator.evaluate(predictions[m])}\")\n",
    "\n",
    "        else: # no summary available\n",
    "            for stage_name, stage_df in zip([\"Train\", \"Test\"], [train_df, test_df]):\n",
    "                predictions = {\n",
    "                    metric: best_model.transform(stage_df)\n",
    "                    for metric in evaluation_metrics_cv\n",
    "                }\n",
    "\n",
    "                for m, model in cross_validated_models.items():\n",
    "                    print(\n",
    "                        f\"\"\"{stage_name} set evaluation\n",
    "                        {type(model).__name__}\n",
    "                        Best model elected by {m}\"\"\"\n",
    "                    )\n",
    "\n",
    "                    for e, evaluator in evaluators.items():\n",
    "                        print(f\"{evaluator.getMetricName()}: {evaluator.evaluate(predictions[m])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# TODO make upper case\n",
    "evaluation_metrics = [\"r2\", \"mse\"]\n",
    "evaluation_metrics_cv = [\"r2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "FEATURES_COL = \"all_vec_min_max\"\n",
    "LABEL_COL = \"points\"\n",
    "PREDICTION_COL = \"lr_predictions\"\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    # .addGrid(lr_raw.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "    # .addGrid(lr_raw.solver, [\"auto\", \"normal\", \"l-bfgs\"])\n",
    "    # .addGrid(lr_raw.standardization, [True, False])\n",
    "    # .addGrid(lr_raw.loss, [\"squaredError\", \"huber\"])\n",
    "    # huber loss does NOT support ElasticNet Regularization :(\n",
    "    # .addGrid(lr_raw.fitIntercept, [True, False])\n",
    "    # .addGrid(lr_raw.elasticNetParam, [0.0, 0.25, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics = evaluation_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(cross_validated_models, evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "PREDICTION_COL = \"dtr_predictions\"\n",
    "\n",
    "estimator = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    # .addGrid(estimator.maxDepth, [2, 5, 10])\n",
    "    # .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "    # .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics = evaluation_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(cross_validated_models, evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "PREDICTION_COL = \"gbtr_predictions\"\n",
    "\n",
    "estimator = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    # .addGrid(estimator.maxDepth, [2, 5, 10])\n",
    "    # .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "    # .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "    # .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "    # .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics = evaluation_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(cross_validated_models, evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "PREDICTION_COL = \"rfr_predictions\"\n",
    "\n",
    "estimator = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    # .addGrid(estimator.maxDepth, [2, 5, 10])\n",
    "    # .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "    # .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "    # .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "    # .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "    # .addGrid(estimator.numTrees, [10, 20, 40])\n",
    "    # .addGrid(estimator.featureSubsetStrategy, [\"auto\", \"onethird\", \"all\", \"log2\"])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics = evaluation_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(cross_validated_models, evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_COL = \"all_vec_min_max\"\n",
    "LABEL_COLS = [\"macro_place_naive\", \"macro_place_complex\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define macro places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea of a macro place, is of a set of table partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MACRO_PLACES = 5\n",
    "\n",
    "def get_macro_place(place, league, complex=False):\n",
    "    def __naive(place, league):\n",
    "        if league in [\"Dutch Eredivisie\", \"German Bundesliga\"]:\n",
    "            if 1 <= place <= 3:\n",
    "                return 0.0\n",
    "            if 4 <= place <= 6:\n",
    "                return 1.0\n",
    "            if 7 <= place <= 10:\n",
    "                return 2.0\n",
    "            if 11 <= place <= 15:\n",
    "                return 3.0\n",
    "            if 16 <= place <= 18:\n",
    "                return 4.0\n",
    "        else:\n",
    "            if 1 <= place <= 4:\n",
    "                return 0.0\n",
    "            if 5 <= place <= 8:\n",
    "                return 1.0\n",
    "            if 9 <= place <= 12:\n",
    "                return 2.0\n",
    "            if 13 <= place <= 16:\n",
    "                return 3.0\n",
    "            if 17 <= place <= 20:\n",
    "                return 4.0\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __complex(place, league):\n",
    "        if league == \"German Bundesliga\":\n",
    "            if 1 <= place <= 4:\n",
    "                return 0.0\n",
    "            if 5 <= place <= 7:\n",
    "                return 1.0\n",
    "            if 8 <= place <= 10:\n",
    "                return 2.0\n",
    "            if 11 <= place <= 15:\n",
    "                return 3.0\n",
    "            if 16 <= place <= 18:\n",
    "                return 4.0\n",
    "\n",
    "        if league == \"Holland Eredivise\":\n",
    "            if place == 1:\n",
    "                return 0.0\n",
    "            if 2 <= place <= 3:\n",
    "                return 1.0\n",
    "            if 4 <= place <= 9:\n",
    "                return 2.0\n",
    "            if 10 <= place <= 15:\n",
    "                return 3.0\n",
    "            if 16 <= place <= 18:\n",
    "                return 4.0\n",
    "\n",
    "        if league == \"French League 1\":\n",
    "            if 1 <= place <= 2:\n",
    "                return 0.0\n",
    "            if 3 <= place <= 5:\n",
    "                return 1.0\n",
    "            if 6 <= place <= 11:\n",
    "                return 2.0\n",
    "            if 12 <= place <= 17:\n",
    "                return 3.0\n",
    "            if 18 <= place <= 20:\n",
    "                return  4.0\n",
    "        \n",
    "        else: #It, Sp, En\n",
    "            if 1 <= place <= 4:\n",
    "                return 0.0\n",
    "            if 5 <= place <= 7:\n",
    "                return 1.0\n",
    "            if 8 <= place <= 12:\n",
    "                return 2.0\n",
    "            if 13 <= place <= 17:\n",
    "                return 3.0\n",
    "            if 18 <= place <= 20:\n",
    "                return 4.0\n",
    "\n",
    "        return None\n",
    "\n",
    "    if complex:\n",
    "        return __complex(place, league)\n",
    "\n",
    "    return __naive(place, league)\n",
    "\n",
    "get_macro_place_UDF = udf(\n",
    "    lambda place, league, complex: get_macro_place(float(place), league, complex),\n",
    "    DoubleType(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"macro_place_naive\", get_macro_place_UDF(col(\"place\"), col(\"league\"), lit(False))\n",
    ").withColumn(\n",
    "    \"macro_place_complex\", get_macro_place_UDF(col(\"place\"), col(\"league\"), lit(True))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = [\"accuracy\", \"f1\"]\n",
    "evaluation_metrics_cv = [\"accuracy\"]\n",
    "\n",
    "train_df, test_df = df.randomSplit(\n",
    "    [0.9,0.1]\n",
    "    # [0.7, 0.3]\n",
    "    # NOTE reactive 90/10 split, keep 70/30 just when using one league\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    df.toPandas(), \n",
    "    [\"macro_place_naive\", \"macro_place_complex\"],\n",
    "    color=\"teal\",\n",
    "    n_cols=2,\n",
    "    figsize=(8,4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows comments:\n",
    "\n",
    "* **Naive approach**: sligh imbalancement against the low partition of the table.\n",
    "* **Compelx approach**: imbalancement in favor of the mid-table teams, confirming of course the visualizations in the beginning of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_COL = \"svmc_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LinearSVC(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=LABEL_COL,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics = evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models[label_col], \n",
    "        evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_COL = \"all_vec_min_max\"\n",
    "LABEL_COLS = [\"macro_place_naive\", \"macro_place_complex\"]\n",
    "PREDICTION_COL = \"lrc_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LogisticRegression(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "PREDICTION_COL = \"dtc_predictions\"\n",
    "\n",
    "estimator = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics = evaluation_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "PREDICTION_COL = \"rfc_predictions\"\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics = [\n",
    "            \"accuracy\",\n",
    "            \"falsePositiveRateByLabel\",\n",
    "            \"precisionByLabel\",\n",
    "            \"recallByLabel\",\n",
    "            \"truePositiveRateByLabel\",\n",
    "            \"weightedFalsePositiveRate\",\n",
    "            \"weightedTruePositiveRate\",\n",
    "            \"weightedPrecision\",\n",
    "            \"weightedRecall\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2: \"less is more\"\n",
    "\n",
    "Ok, considering all features gives trash results.\n",
    "\n",
    "What if we embrace the \"less is more idea\" and try to improve the results by means of using less features?\n",
    "\n",
    "Nevertheless, feature correlation is very high, so, intrinsicly, it already did NOT make much sense to consider them all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log scaling reduces skewedness of \"avg(mentality_penalties)\" feature BUT it increases the skewedness of all the other features.\n",
    "The other scalings (z-score and min-max) do NOT appear to be different than the \"raw\" data distribution.\n",
    "\n",
    "For this reason and due to the limited amount of resources available on Google Colab, we decided to stick with the min-max scaled data.\n",
    "In fact, the min-max scaling places \"for free\" all the features in the same scale, which is a very important consideration for SVM, which will be used in the upcoming sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import UnivariateFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = UnivariateFeatureSelector(\n",
    "    featuresCol=\"all_vec_min_max\",\n",
    "    labelCol=TARGET_VARIABLE, \n",
    "    selectionMode=\"percentile\"\n",
    ").setFeatureType(\"continuous\").setLabelType(\"categorical\").setSelectionThreshold(0.08)\n",
    "\n",
    "NUM_FEATURES = [2, 4, 6, 8]\n",
    "THRESHOLDS = [num_features/24 for num_features in NUM_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(\"all_vec_min_max_ufs_2\",\"all_vec_min_max_ufs_4\",\"all_vec_min_max_ufs_6\",\"all_vec_min_max_ufs_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result = dict()\n",
    "\n",
    "for num_features, thr in zip(NUM_FEATURES, THRESHOLDS):\n",
    "    selector.setSelectionThreshold(thr)\n",
    "    selector.setOutputCol(f\"all_vec_min_max_ufs_{num_features}\"),\n",
    "    fit_result[str(num_features)] = selector.fit(df)\n",
    "    df = fit_result[str(num_features)].transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = dict()\n",
    "for num_features in NUM_FEATURES:\n",
    "    selected_features[str(num_features)] = list(\n",
    "        map(\n",
    "            lambda i: ALL_FEATURES[i], fit_result[str(num_features)].selectedFeatures\n",
    "        )\n",
    "    )\n",
    "    print(f\"Univariate Feature Selection selected these {num_features} features:\\n{selected_features[str(num_features)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    x=pdf[\"all_vec_min_max_ufs_2\"].map(lambda x: x[0]),\n",
    "    y=pdf[\"all_vec_min_max_ufs_2\"].map(lambda x: x[1]),\n",
    "    c=pdf[\"points\"],\n",
    "    x_label=\"Feature 0\",\n",
    "    y_label=\"Feature 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(selected_features.keys())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_key = list(selected_features.keys())[-1]\n",
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    selected_features[max_key], # last dictionary entry\n",
    "    figsize=(10,4),\n",
    "    color=\"orange\",\n",
    "    n_cols=(int(max_key) // 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf,\n",
    "    selected_features[max_key],\n",
    "    TARGET_VARIABLE,\n",
    "    figsize=(10,8), \n",
    "    color=COLOR_MIN_MAX,\n",
    "    n_cols=(int(max_key) // 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at it from the original pearson matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state that these data distribs are trash.\n",
    "\n",
    "So, for this reason, talk about overall and value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall as feature (min-max normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features are so correlated and performances of attempt 1 are trash, why not considering just the overall as a feature?\n",
    "\n",
    "Maybe, we're lucky and the overall captures some other characteristics thay may steer the prediction a little bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERALL = [\"avg(overall)\"]\n",
    "\n",
    "COLOR_OVERALL_MIN_MAX = \"green\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=OVERALL, outputCol=\"overall_vec\"\n",
    ")\n",
    "\n",
    "# min_max_df = assembler.transform(min_max_df)\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERALL_MIN_MAX = [\"avg(overall)_min_max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"overall_vec\", \n",
    "    outputCol=\"overall_vec_min_max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_df = scaler.fit(min_max_df).transform(min_max_df)\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = min_max_df.toPandas()\n",
    "pdf = df.toPandas()\n",
    "\n",
    "pdf = feature_vec_to_cols(\n",
    "    pdf=pdf,\n",
    "    vec=\"overall_vec_min_max\",\n",
    "    columns=OVERALL_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf,\n",
    "    OVERALL_MIN_MAX,\n",
    "    TARGET_VARIABLE,\n",
    "    figsize=(10,4),\n",
    "    color=\"chocolate\",\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    OVERALL_MIN_MAX,\n",
    "    color=\"chocolate\",\n",
    "    figsize=(10,4),\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value as feature (min-max normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, we found this paper: \n",
    "\n",
    "So, why not try their approach as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE = [\"avg(value)\"]\n",
    "\n",
    "COLOR_VALUE_MIN_MAX = \"coral\"\n",
    "VALUE_MIN_MAX = [\"avg(value)_min_max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=VALUE, outputCol=\"value_vec\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"value_vec\", \n",
    "    outputCol=\"value_vec_min_max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()\n",
    "pdf = feature_vec_to_cols(\n",
    "    pdf=pdf,\n",
    "    vec=\"value_vec_min_max\",\n",
    "    columns=VALUE_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_target_relation(\n",
    "    pdf,\n",
    "    VALUE_MIN_MAX,\n",
    "    TARGET_VARIABLE,\n",
    "    figsize=(10,4),\n",
    "    color=COLOR_VALUE_MIN_MAX,\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look, they are basically the same. In fact, if we check their correlation, we get... [high correlation on pearson]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    [\"avg(value)\"],\n",
    "    color=COLOR_VALUE_MIN_MAX,\n",
    "    figsize=(10,4),\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that there is a lot of skewness. We already know that the min_max normalization has the same exact curve.\n",
    "\n",
    "Let's try to see what happens by applying a $log_2$ transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"value_log\", to_log_UDF(\"avg(value)\"))\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    [\"value_log\"],\n",
    "    color=COLOR_VALUE_MIN_MAX,\n",
    "    figsize=(10,4),\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"value_log\"], outputCol=\"value_log_vec\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"value_log_vec\", \n",
    "    outputCol=\"value_log_vec_min_max\"\n",
    ")\n",
    "\n",
    "df = scaler.fit(df).transform(df)\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = feature_vec_to_cols(\n",
    "    pdf=pdf,\n",
    "    vec=\"value_log_vec_min_max\",\n",
    "    columns=[\"value_log_min_max\"]\n",
    ")\n",
    "\n",
    "plot_feature_distribution(\n",
    "    pdf,\n",
    "    [\"value_log_min_max\"],\n",
    "    color=COLOR_VALUE_MIN_MAX,\n",
    "    figsize=(10,4),\n",
    "    n_cols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint at a very high correlation, then show it with pearson matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = feature_vec_to_cols(\n",
    "    pdf=pdf,\n",
    "    vec=\"overall_vec_min_max\",\n",
    "    columns=OVERALL_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(\n",
    "    pdf, \n",
    "    [\"avg(overall)_min_max\", \"value_log_min_max\"],\n",
    "    figsize=(8,4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment this correlation, and move on with life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning for attempt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[\n",
    "    state that via hyperparam grid we'll set the feature column, meaning that we'll try to train the models on all of the attemps\n",
    "\n",
    "[state expected results, according to correlations and similar stuff]\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make upper case\n",
    "evaluation_metrics = [\"r2\", \"mse\"]\n",
    "evaluation_metrics_cv = [\"r2\"]\n",
    "\n",
    "FEATURES_COL = \"value_log_vec_min_max\"\n",
    "LABEL_COL = \"points\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_train_df, lr_test_df = feature_selection_train_df, feature_selection_test_df\n",
    "lr_train_df, lr_test_df = train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_COL = \"lr_attempt2_predictions\"\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    # .addGrid(lr_raw.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "    # .addGrid(lr_raw.solver, [\"auto\", \"normal\", \"l-bfgs\"])\n",
    "    # .addGrid(lr_raw.standardization, [True, False])\n",
    "    # .addGrid(lr_raw.loss, [\"squaredError\", \"huber\"])\n",
    "    # huber loss does NOT support ElasticNet Regularization :(\n",
    "    # .addGrid(lr_raw.fitIntercept, [True, False])\n",
    "    # .addGrid(lr_raw.elasticNetParam, [0.0, 0.25, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "PREDICTION_COL = \"dtr_attempt2_predictions\"\n",
    "\n",
    "estimator = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    # .addGrid(estimator.maxDepth, [2, 5, 10])\n",
    "    # .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "    # .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "PREDICTION_COL = \"rfr_attempt2_predictions\"\n",
    "\n",
    "estimator = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    # .addGrid(estimator.maxDepth, [2, 5, 10])\n",
    "    # .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "    # .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "    # .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "    # .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "    # .addGrid(estimator.numTrees, [10, 20, 40])\n",
    "    # .addGrid(estimator.featureSubsetStrategy, [\"auto\", \"onethird\", \"all\", \"log2\"])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "PREDICTION_COL = \"gbtr_attempt2_predictions\"\n",
    "\n",
    "estimator = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    # .addGrid(estimator.maxDepth, [2, 5, 10])\n",
    "    # .addGrid(estimator.maxBins, [16, 32, 64])\n",
    "    # .addGrid(estimator.minInfoGain, [0, 0.1])\n",
    "    # .addGrid(estimator.subsamplingRate, [0.5, 1])\n",
    "    # .addGrid(estimator.lossType, [\"squared\", \"absolute\"])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = [\"accuracy\", \"f1\"]\n",
    "evaluation_metrics_cv = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "FEATURES_COL = \"all_vec_min_max\"\n",
    "LABEL_COLS = [\"macro_place_naive\", \"macro_place_complex\"]\n",
    "PREDICTION_COL = \"svmc_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LinearSVC(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=LABEL_COL,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics = evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models[label_col], \n",
    "        evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "PREDICTION_COL = \"lrc_attempt2_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LogisticRegression(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Evaluating with label {label_col}\")\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "PREDICTION_COL = \"dtc_attempt2_predictions\"\n",
    "\n",
    "estimator = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Evaluating with label {label_col}\")\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "PREDICTION_COL = \"rfc_attempt2_predictions\"\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics = [\n",
    "            \"accuracy\",\n",
    "            \"falsePositiveRateByLabel\",\n",
    "            \"precisionByLabel\",\n",
    "            \"recallByLabel\",\n",
    "            \"truePositiveRateByLabel\",\n",
    "            \"weightedFalsePositiveRate\",\n",
    "            \"weightedTruePositiveRate\",\n",
    "            \"weightedPrecision\",\n",
    "            \"weightedRecall\",\n",
    "        ],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Evaluating with label {label_col}\")\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "PREDICTION_COL = \"mlpc_attempt2_predictions\"\n",
    "\n",
    "estimator = MultilayerPerceptronClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [\"value_log_vec_min_max\"])\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .addGrid(estimator.layers, [[1, NUM_MACRO_PLACES]])\n",
    "    .addGrid(estimator.solver, [\"gd\"])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Evaluating with label {label_col}\")\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 3: clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import DenseVector, Vectors, VectorUDT\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(\n",
    "    dataset,\n",
    "    n_clusters,\n",
    "    distance_measure=\"euclidean\",\n",
    "    max_iter=20,\n",
    "    features_col=\"features\",\n",
    "    prediction_col=\"cluster\",\n",
    "):\n",
    "\n",
    "    print(\n",
    "        f\"\"\"Training K-means clustering using the following parameters: \n",
    "        - K (n. of clusters) = {n_clusters}\n",
    "        - max_iter (max n. of iterations) = {max_iter}\n",
    "        - distance measure = {distance_measure}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # if distance_measure == \"cosine\":\n",
    "\n",
    "        # NOTE we already start from normalized/std data, so, NO need to do it\n",
    "\n",
    "        # Normalize inputs to unit-length vectors\n",
    "        # dataset = Normalizer(\n",
    "        #     inputCol=features_col, outputCol=features_col + \"_norm\", p=1\n",
    "        # ).transform(dataset)\n",
    "\n",
    "        # features_col = features_col + \"_norm\"\n",
    "\n",
    "    # Train a K-means model\n",
    "    kmeans = KMeans(\n",
    "        featuresCol=features_col,\n",
    "        predictionCol=prediction_col,\n",
    "        k=n_clusters,\n",
    "        initMode=\"k-means||\",\n",
    "        initSteps=5,\n",
    "        tol=0.000001,\n",
    "        maxIter=max_iter,\n",
    "        distanceMeasure=distance_measure,\n",
    "    )\n",
    "\n",
    "    model = kmeans.fit(dataset)\n",
    "    # here there are all the relevant clustering information\n",
    "\n",
    "    # Make clusters\n",
    "    clusters_df = model.transform(dataset)\n",
    "\n",
    "    return model, clusters_df\n",
    "\n",
    "\n",
    "def evaluate_k_means(\n",
    "    clusters,\n",
    "    metric_name=\"silhouette\",\n",
    "    distance_measure=\"squaredEuclidean\",  # cosine\n",
    "    prediction_col=\"cluster\",\n",
    "    featuresCol = \"features\"\n",
    "):\n",
    "\n",
    "    # Evaluate clustering by computing Silhouette score\n",
    "    evaluator = ClusteringEvaluator(\n",
    "        metricName=metric_name,\n",
    "        distanceMeasure=distance_measure,\n",
    "        predictionCol=prediction_col,\n",
    "        featuresCol=featuresCol\n",
    "    )\n",
    "\n",
    "    return evaluator.evaluate(clusters)\n",
    "\n",
    "\n",
    "def do_clustering(\n",
    "    k_range, \n",
    "    input_df, \n",
    "    max_iter,\n",
    "    featuresCol,\n",
    "    clusterCol\n",
    "):\n",
    "    clustering_results = {}\n",
    "\n",
    "    clusters_df = input_df\n",
    "\n",
    "    # for k in tqdm( range(5, max_k_clusters + 1, 5), desc = \"Performing clustering\" ):\n",
    "    # for k in tqdm(range(2, max_k_clusters + 1, 4), desc=\"Performing clustering\"):\n",
    "    for k in k_range:\n",
    "        print(f\"Running K-means using K = {k}\")\n",
    "\n",
    "        # model, clusters_df = k_means(tf_idf_df, k, max_iter=50, distance_measure=\"cosine\") # Alternatively, distance_measure=\"euclidean\"\n",
    "        # model, clusters_df = k_means(input_df, k, max_iter=max_iter, distance_measure=\"cosine\") # Alternatively, distance_measure=\"euclidean\"\n",
    "        model, clusters_df = k_means(\n",
    "            # input_df,\n",
    "            clusters_df, \n",
    "            k, \n",
    "            max_iter=max_iter, \n",
    "            distance_measure=\"cosine\", \n",
    "            features_col=featuresCol,\n",
    "            prediction_col=clusterCol + \"_k_\" + str(k),\n",
    "        )  # Alternatively, distance_measure=\"euclidean\"\n",
    "        # silhouette_k = evaluate_k_means(clusters_df, distance_measure=\"cosine\") # Alternatively, distance_measure=\"squaredEuclidean\"\n",
    "        silhouette_k = evaluate_k_means(\n",
    "            clusters_df, \n",
    "            distance_measure=\"cosine\",\n",
    "            prediction_col=clusterCol + \"_k_\" + str(k),\n",
    "            featuresCol=featuresCol\n",
    "\n",
    "        )  # Alternatively, distance_measure=\"squaredEuclidean\"\n",
    "        wssd_k = model.summary.trainingCost\n",
    "        # wssd_k = model.summary\n",
    "\n",
    "        print(\n",
    "            \"Silhouette coefficient computed with cosine distance: {:.3f}\".format(\n",
    "                silhouette_k\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"Within-cluster Sum of Squared Distances (using cosine distance): {:.3f}\".format(\n",
    "                wssd_k\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"--------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "\n",
    "        l = list(\n",
    "            enumerate(\n",
    "                model.clusterCenters()\n",
    "            )\n",
    "        )\n",
    "        l = [(ind, DenseVector(c)) for ind, c in l]\n",
    "        # print(l)\n",
    "        schema = [\"cluster_id\"  + \"_k_\" + str(k), \"centroid\"  + \"_k_\" + str(k)]\n",
    "\n",
    "        schema = StructType([ \n",
    "            StructField(\"cluster_id\" + \"_k_\" + str(k),IntegerType(),True), \n",
    "            StructField(\"centroid\" + \"_k_\" + str(k),VectorUDT(),True), \n",
    "        ])\n",
    "        centr_df = spark.createDataFrame(data=l, schema=schema)\n",
    "\n",
    "        # centr_df.show()\n",
    "\n",
    "        # df_with_centroids = clusters_df.join(\n",
    "        #     centr_df, on=[\"cluster_id\"]\n",
    "        # )\n",
    "\n",
    "        clusters_df = clusters_df.join(\n",
    "            centr_df, on=[\"cluster_id\" + \"_k_\" + str(k)]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        clustering_results[str(k)] = {\n",
    "            \"silhouette_k\"      : silhouette_k,\n",
    "            \"wssd_k\"            : wssd_k,\n",
    "            \"model\"             : model,\n",
    "            \"df\"                : clusters_df,\n",
    "            # \"cluster_centroids\" : model.clusterCenters(),\n",
    "            # \"centr_df\" : centr_df,\n",
    "            # \"df_with_centroids\" : df_with_centroids\n",
    "        }\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Free up memory space at the end of each iteration\n",
    "        # del model\n",
    "        # del clusters_df\n",
    "        # gc.collect() # garbage collector\n",
    "    \n",
    "    clustering_results[\"df_with_centroid_full\"] = clusters_df\n",
    "\n",
    "    return clustering_results\n",
    "\n",
    "\n",
    "def plot_clustering_results(clustering_results, k_range):\n",
    "\n",
    "    # print(clustering_results)\n",
    "\n",
    "    # k_col = list(clustering_results.keys())[:-1]\n",
    "    k_col = [str(x) for x in k_range]\n",
    "    wssd_col = [\n",
    "        clustering_results[k][\"wssd_k\"] for k in k_col \n",
    "    ]\n",
    "    silhouette_col = [\n",
    "        clustering_results[k][\"silhouette_k\"] for k in k_col \n",
    "    ]\n",
    "\n",
    "    # print(f\"k_col: {k_col}\")\n",
    "    # print(wssd_col)\n",
    "    # print(silhouette_col)\n",
    "\n",
    "    plot_df_temp = pd.DataFrame([k_col, wssd_col, silhouette_col]).transpose()\n",
    "    plot_df_temp.columns = [\"K\", \"WSSD\", \"Silhouette\"]\n",
    "    # print(plot_df_temp)\n",
    "\n",
    "    # Create a 1x1 figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    _ = sns.pointplot(\n",
    "        data=plot_df_temp, x=\"K\", y=\"WSSD\", ax=ax, color=\"orangered\"\n",
    "    )\n",
    "    _ = ax.set_xlabel(\"K\")\n",
    "    _ = ax.set_ylabel(\"WSSD\")\n",
    "    \n",
    "    # Create a 1x1 figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    _ = sns.pointplot(\n",
    "        data=plot_df_temp, x=\"K\", y=\"Silhouette\", ax=ax, color=\"orangered\"\n",
    "    )\n",
    "    _ = ax.set_xlabel(\"K\")\n",
    "    _ = ax.set_ylabel(\"Silhouette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "CLUSTERING_FEATURES = copy.deepcopy(PLAYER_FEATURES)\n",
    "CLUSTERING_FEATURES.remove(\"value\")\n",
    "CLUSTERING_FEATURES.remove(\"overall\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=CLUSTERING_FEATURES,\n",
    "    outputCol=\"clustering_features\"\n",
    ")\n",
    "\n",
    "pre_processed_df = assembler.transform(pre_processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"clustering_features\",\n",
    "    outputCol=\"clustering_features_min_max\"    \n",
    ")\n",
    "\n",
    "pre_processed_df = scaler.fit(pre_processed_df).transform(pre_processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\n",
    "    s: pre_processed_df.filter(col(\"season\") == s) for s in seasons\n",
    "}\n",
    "\n",
    "MAX_K_CLUSTERS = 8\n",
    "MAX_ITER = 20\n",
    "\n",
    "k_range = [2]\n",
    "# k_range = range(2, MAX_K_CLUSTERS, 4)\n",
    "K_RANGE = [str(k) for k in k_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running K-means using K = 2\n",
      "Training K-means clustering using the following parameters: \n",
      "        - K (n. of clusters) = 2\n",
      "        - max_iter (max n. of iterations) = 2\n",
      "        - distance measure = cosine\n",
      "        \n",
      "Silhouette coefficient computed with cosine distance: 0.704\n",
      "Within-cluster Sum of Squared Distances (using cosine distance): 14.332\n",
      "--------------------------------------------------------------------------------------\n",
      "Running K-means using K = 2\n",
      "Training K-means clustering using the following parameters: \n",
      "        - K (n. of clusters) = 2\n",
      "        - max_iter (max n. of iterations) = 2\n",
      "        - distance measure = cosine\n",
      "        \n",
      "Silhouette coefficient computed with cosine distance: 0.858\n",
      "Within-cluster Sum of Squared Distances (using cosine distance): 24.439\n",
      "--------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "clustering_results_dict = dict()\n",
    "\n",
    "for s in seasons:\n",
    "    clustering_results_dict[s] = do_clustering(\n",
    "        k_range=k_range, \n",
    "        input_df=df_dict[s], \n",
    "        max_iter=2,\n",
    "        # max_iter=MAX_ITER,\n",
    "        featuresCol=\"clustering_features_min_max\",\n",
    "        clusterCol=\"cluster_id\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_df = reduce(\n",
    "    DataFrame.unionAll, \n",
    "    [\n",
    "        clustering_results_dict[s][\"df_with_centroid_full\"] for s in seasons\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutating single seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in seasons:\n",
    "    plot_clustering_results(clustering_results_dict[s], k_range=K_RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating seasons all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_results_dict[\"avg\"] = dict.fromkeys(K_RANGE)\n",
    "\n",
    "for k in clustering_results_dict[\"avg\"].keys():\n",
    "    clustering_results_dict[\"avg\"][k] = {\n",
    "        \"wssd_k\": 0,\n",
    "        \"silhouette_k\": 0\n",
    "    }\n",
    "\n",
    "sum_wssd = dict.fromkeys(K_RANGE, 0)\n",
    "sum_silhouette = dict.fromkeys(K_RANGE, 0)\n",
    "\n",
    "for s in seasons:\n",
    "    for k in K_RANGE:\n",
    "        sum_wssd[k] += clustering_results_dict[s][k][\"wssd_k\"]\n",
    "        sum_silhouette[k] += clustering_results_dict[s][k][\"silhouette_k\"]\n",
    "    \n",
    "avg_wssd = dict()\n",
    "avg_silhouette = dict()\n",
    "\n",
    "for k in K_RANGE:\n",
    "    avg_wssd[k] = sum_wssd[k] / len(k_range)\n",
    "    avg_silhouette[k] = sum_silhouette[k] / len(k_range)\n",
    "\n",
    "    clustering_results_dict[\"avg\"][k][\"wssd_k\"] = avg_wssd[k]\n",
    "    clustering_results_dict[\"avg\"][k][\"silhouette_k\"] = avg_silhouette[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_results(\n",
    "    clustering_results_dict[\"avg\"], k_range=K_RANGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute distance between player and centroid of its cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_distance_from_centroid_UDF = udf(\n",
    "    lambda player, centroid: float(\n",
    "        Vectors.squared_distance(\n",
    "            player, centroid\n",
    "        )\n",
    "    ), FloatType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K_RANGE:\n",
    "    pre_processed_df = pre_processed_df.withColumn(\n",
    "        \"distance_from_centroid\" + \"_k_\" + k,\n",
    "        compute_distance_from_centroid_UDF(\n",
    "            col(\"clustering_features_min_max\"),\n",
    "            col(\"centroid\" + \"_k_\" + k)\n",
    "        )\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From players to teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = pre_processed_df.groupBy(\n",
    "    [\"season\", \"club_name\", \"macro_role\"]\n",
    ").agg(\n",
    "    {\n",
    "        \"distance_from_centroid\" + \"_k_\" + str(k): \"avg\" for k in K_RANGE \n",
    "    }\n",
    ")\n",
    "\n",
    "for k in K_RANGE:\n",
    "    teams_df = teams_df.withColumnRenamed(\n",
    "        \"avg(distance_from_centroid\" + \"_k_\" + str(k) + \")\",\n",
    "        \"avg_distance_from_centroid\" + \"_k_\" + str(k)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subquery(macro_role, k):\n",
    "    return f\"\"\"(\n",
    "        case\n",
    "            when macro_role='{macro_role}' then avg_distance_from_centroid_k_{k} \n",
    "        else NULL\n",
    "        end\n",
    "    ) as avg_dist_macro_role_{int(macro_role)}_k_{k}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df.createOrReplaceTempView(\"t\")\n",
    "\n",
    "temp = dict()\n",
    "\n",
    "for k in K_RANGE:\n",
    "    temp[k] = (\n",
    "        spark.sql(\n",
    "            f\"\"\"\n",
    "                select season, club_name, {generate_subquery(0.0, k)}, {generate_subquery(1.0, k)}, {generate_subquery(2.0, k)}, {generate_subquery(3.0, k)}, {generate_subquery(4.0, k)}, {generate_subquery(5.0, k)}, {generate_subquery(6.0, k)}, {generate_subquery(7.0, k)}\n",
    "                from t\n",
    "            \"\"\"\n",
    "        )\n",
    "        .groupBy(\"season\", \"club_name\")\n",
    "        .agg(\n",
    "            # TODO use for loop as in second cell of \"from players to teams\"\n",
    "            sum(f\"avg_dist_macro_role_0_k_{k}\").alias(f\"avg_dist_macro_role_0_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_1_k_{k}\").alias(f\"avg_dist_macro_role_1_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_2_k_{k}\").alias(f\"avg_dist_macro_role_2_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_3_k_{k}\").alias(f\"avg_dist_macro_role_3_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_4_k_{k}\").alias(f\"avg_dist_macro_role_4_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_5_k_{k}\").alias(f\"avg_dist_macro_role_5_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_6_k_{k}\").alias(f\"avg_dist_macro_role_6_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_7_k_{k}\").alias(f\"avg_dist_macro_role_7_k_{k}\"),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE executing this cell n times, w/o restoring teams_df --> \n",
    "# n copies of avg_dist_macro_role_[0:7]_k_[2, 6]\n",
    "\n",
    "teams_df = temp[K_RANGE[0]]\n",
    "\n",
    "for i in range(1, len(K_RANGE)):\n",
    "    teams_df = teams_df.join(\n",
    "        temp[K_RANGE[i]],\n",
    "        on=[\"season\", \"club_name\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_distances_dict = dict()\n",
    "avg_distances_vec_dict = dict()\n",
    "\n",
    "for k in K_RANGE:\n",
    "    \n",
    "    avg_distances_dict[k] = [\n",
    "        f\"avg_dist_macro_role_{i}_k_{k}\" for i in range(0, NUM_MACRO_ROLES)\n",
    "    ]\n",
    "\n",
    "    avg_distances_vec_dict[k] = f\"avg_dist_vec_k_{k}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import builtins\n",
    "import itertools\n",
    "\n",
    "global_max = teams_df.select(\n",
    "    greatest(\n",
    "        *list(\n",
    "            itertools.chain.from_iterable(\n",
    "                avg_distances_dict.values()\n",
    "            )\n",
    "        )\n",
    "    ).alias(\"row_wise_max\")\n",
    ").collect()\n",
    "\n",
    "global_max = [row[\"row_wise_max\"] for row in global_max]\n",
    "\n",
    "global_max = builtins.max(global_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = teams_df.fillna(global_max * 1.5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K_RANGE:\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=avg_distances_dict[k], outputCol=avg_distances_vec_dict[k]\n",
    "    )\n",
    "\n",
    "    teams_df = assembler.transform(teams_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(\n",
    "    teams_df,\n",
    "    on=[\"club_name\", \"season\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit(\n",
    "    [0.9, 0.1]\n",
    "    # NOTE reactive 90/10 split, keep 70/30 just when using one league\n",
    "    # [0.7, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS_CV = 4\n",
    "FEATURES_COL = [f\"avg_dist_vec_k_{k}\" for k in K_RANGE]\n",
    "LABEL_COL = \"points\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = [\"r2\", \"mse\"]\n",
    "evaluation_metrics_cv = [\"r2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"lr_attempt3_predictions\"\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    # .addGrid(lr_raw.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "    # .addGrid(lr_raw.solver, [\"auto\", \"normal\", \"l-bfgs\"])\n",
    "    # .addGrid(lr_raw.standardization, [True, False])\n",
    "    # .addGrid(lr_raw.loss, [\"squaredError\", \"huber\"])\n",
    "    # huber loss does NOT support ElasticNet Regularization :(\n",
    "    # .addGrid(lr_raw.fitIntercept, [True, False])\n",
    "    # .addGrid(lr_raw.elasticNetParam, [0.0, 0.25, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"dtr_attempt3_predictions\"\n",
    "\n",
    "estimator = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"rfr_attempt3_predictions\"\n",
    "\n",
    "estimator = RandomForestRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"gbtr_attempt3_predictions\"\n",
    "\n",
    "estimator = GBTRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = [\"accuracy\", \"f1\"]\n",
    "evaluation_metrics_cv = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "FEATURES_COL = [f\"avg_dist_vec_k_{k}\" for k in K_RANGE]\n",
    "LABEL_COLS = [\"macro_place_naive\", \"macro_place_complex\"]\n",
    "PREDICTION_COL = \"svmc_attempt3_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LinearSVC(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "# for label_col in LABEL_COLS:\n",
    "#     cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "#         estimator=estimator,\n",
    "#         param_grid=param_grid,\n",
    "#         feature_vec=[FEATURES_COL],\n",
    "#         label_col=LABEL_COL,\n",
    "#         prediction_col=PREDICTION_COL,\n",
    "#         evaluation_metrics = evaluation_metrics,\n",
    "#         evaluation_metrics_cv=evaluation_metrics_cv\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models[label_col], \n",
    "        evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "PREDICTION_COL = \"lrc_attempt3_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LogisticRegression(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, [FEATURES_COL])\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "# for label_col in LABEL_COLS:\n",
    "#     print(f\"Learning with label {label_col}\")\n",
    "#     cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "#         estimator=estimator,\n",
    "#         param_grid=param_grid,\n",
    "#         feature_vec=[FEATURES_COL],\n",
    "#         label_col=label_col,\n",
    "#         prediction_col=PREDICTION_COL,\n",
    "#         evaluation_metrics=evaluation_metrics,\n",
    "#         evaluation_metrics_cv=evaluation_metrics_cv\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Evaluating with label {label_col}\")\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning with label macro_place_naive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning with label macro_place_complex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"dtc_attempt3_predictions\"\n",
    "\n",
    "estimator = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with label macro_place_naive\n",
      "Train set evaluation\n",
      "                        CrossValidatorModel\n",
      "                        Best model elected by accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0\n",
      "f1: 0.0\n",
      "Test set evaluation\n",
      "                        CrossValidatorModel\n",
      "                        Best model elected by accuracy\n",
      "accuracy: 0.0\n",
      "f1: 0.0\n",
      "Evaluating with label macro_place_complex\n",
      "Train set evaluation\n",
      "                        CrossValidatorModel\n",
      "                        Best model elected by accuracy\n",
      "accuracy: 0.0\n",
      "f1: 0.0\n",
      "Test set evaluation\n",
      "                        CrossValidatorModel\n",
      "                        Best model elected by accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0\n",
      "f1: 0.0\n"
     ]
    }
   ],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Evaluating with label {label_col}\")\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning with label macro_place_naive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning with label macro_place_complex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "PREDICTION_COL = \"rfc_attempt3_predictions\"\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics = [\n",
    "            \"accuracy\",\n",
    "            \"falsePositiveRateByLabel\",\n",
    "            \"precisionByLabel\",\n",
    "            \"recallByLabel\",\n",
    "            \"truePositiveRateByLabel\",\n",
    "            \"weightedFalsePositiveRate\",\n",
    "            \"weightedTruePositiveRate\",\n",
    "            \"weightedPrecision\",\n",
    "            \"weightedRecall\",\n",
    "        ],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with label macro_place_naive\n",
      "\n",
      "                    Training set evaluation\n",
      "                    RandomForestClassificationModel\n",
      "                    Best model according to metric: accuracy\n",
      "                    \tAccuracy:                  1.0\n",
      "                    \tfalsePositiveRateByLabel:  [0.0, 0.0, 0.0]\n",
      "                    \tprecisionByLabel:          [1.0, 1.0, 1.0]\n",
      "                    \trecallByLabel:             [1.0, 1.0, 1.0]\n",
      "                    \ttruePositiveRateByLabel:   [1.0, 1.0, 1.0]\n",
      "                    \tweightedFalsePositiveRate: 0.0\n",
      "                    \tweightedTruePositiveRate:  1.0\n",
      "                    \tweightedPrecision:         1.0\n",
      "                    \tweightedRecall:            1.0\n",
      "                    \n",
      "\n",
      "                Test set evaluation\n",
      "                RandomForestClassificationModel\n",
      "                Best model according to metric accuracy\n",
      "                \n",
      "accuracy: 0.0\n",
      "falsePositiveRateByLabel: 0.0\n",
      "precisionByLabel: 0.0\n",
      "recallByLabel: 0.0\n",
      "truePositiveRateByLabel: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weightedFalsePositiveRate: 0.0\n",
      "weightedTruePositiveRate: 0.0\n",
      "weightedPrecision: 0.0\n",
      "weightedRecall: 0.0\n",
      "Evaluating with label macro_place_complex\n",
      "\n",
      "                    Training set evaluation\n",
      "                    RandomForestClassificationModel\n",
      "                    Best model according to metric: accuracy\n",
      "                    \tAccuracy:                  1.0\n",
      "                    \tfalsePositiveRateByLabel:  [0.0, 0.0, 0.0]\n",
      "                    \tprecisionByLabel:          [1.0, 1.0, 1.0]\n",
      "                    \trecallByLabel:             [1.0, 1.0, 1.0]\n",
      "                    \ttruePositiveRateByLabel:   [1.0, 1.0, 1.0]\n",
      "                    \tweightedFalsePositiveRate: 0.0\n",
      "                    \tweightedTruePositiveRate:  1.0\n",
      "                    \tweightedPrecision:         1.0\n",
      "                    \tweightedRecall:            1.0\n",
      "                    \n",
      "\n",
      "                Test set evaluation\n",
      "                RandomForestClassificationModel\n",
      "                Best model according to metric accuracy\n",
      "                \n",
      "accuracy: 0.0\n",
      "falsePositiveRateByLabel: 0.0\n",
      "precisionByLabel: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recallByLabel: 0.0\n",
      "truePositiveRateByLabel: 0.0\n",
      "weightedFalsePositiveRate: 0.0\n",
      "weightedTruePositiveRate: 0.0\n",
      "weightedPrecision: 0.0\n",
      "weightedRecall: 0.0\n"
     ]
    }
   ],
   "source": [
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Evaluating with label {label_col}\")\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning with label macro_place_naive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                7]0]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "PREDICTION_COL = \"mlpc_attempt3_predictions\"\n",
    "\n",
    "estimator = MultilayerPerceptronClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .addGrid(estimator.layers, [[NUM_MACRO_ROLES, NUM_MACRO_PLACES]])\n",
    "    .addGrid(estimator.solver, [\"gd\"])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in [\"macro_place_naive\"]:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=[\n",
    "            \"accuracy\",\n",
    "            \"falsePositiveRateByLabel\",\n",
    "            \"precisionByLabel\",\n",
    "            \"recallByLabel\",\n",
    "            \"truePositiveRateByLabel\",\n",
    "            \"weightedFalsePositiveRate\",\n",
    "            \"weightedTruePositiveRate\",\n",
    "            \"weightedPrecision\",\n",
    "            \"weightedRecall\",\n",
    "        ],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with label macro_place_naive\n",
      "\n",
      "                    Training set evaluation\n",
      "                    MultilayerPerceptronClassificationModel\n",
      "                    Best model according to metric: accuracy\n",
      "                    \tAccuracy:                  0.4\n",
      "                    \tfalsePositiveRateByLabel:  [0.25, 0.0, 0.23076923076923078, 0.32, 0.0]\n",
      "                    \tprecisionByLabel:          [0.2222222222222222, 0.0, 0.4, 0.5, 0.0]\n",
      "                    \trecallByLabel:             [0.2857142857142857, 0.0, 0.4444444444444444, 0.8, 0.0]\n",
      "                    \ttruePositiveRateByLabel:   [0.2857142857142857, 0.0, 0.4444444444444444, 0.8, 0.0]\n",
      "                    \tweightedFalsePositiveRate: 0.20076923076923076\n",
      "                    \tweightedTruePositiveRate:  0.4\n",
      "                    \tweightedPrecision:         0.29015873015873017\n",
      "                    \tweightedRecall:            0.4\n",
      "                    \n",
      "\n",
      "                Test set evaluation\n",
      "                MultilayerPerceptronClassificationModel\n",
      "                Best model according to metric accuracy\n",
      "                \n",
      "accuracy: 0.0\n",
      "f1: 0.0\n"
     ]
    }
   ],
   "source": [
    "for label_col in [\"macro_place_naive\"]:\n",
    "    print(f\"Evaluating with label {label_col}\")\n",
    "    evaluate_learning_models(\n",
    "        cross_validated_models=cross_validated_models[label_col],\n",
    "        evaluators=evaluators[label_col],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering-learning cross evaluation\n",
    "\n",
    "Classic left-right plot, with:\n",
    "Left Y --> elbow result\n",
    "Right Y --> accuracy\n",
    "X axis --> # clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 4: thinking \"Deep\", shallow injecting some priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PLACE = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the prior (RP coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df.select(\"season\", \"club_name\", \"place\").createOrReplaceTempView(\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = spark.sql(\n",
    "    f\"\"\"\n",
    "    select t.season, t.club_name,\n",
    "        avg(\n",
    "            (\n",
    "                select sub.place\n",
    "                where sub.season < t.season and sub.club_name == t.club_name\n",
    "            )\n",
    "        ) as rp_coeff\n",
    "    from t, t as sub\n",
    "    group by t.season, t.club_name\n",
    "    order by t.season desc\n",
    "    \"\"\"\n",
    ").fillna(MAX_PLACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = rp_df.withColumn(\"rp_coeff\", MAX_PLACE - col(\"rp_coeff\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_RP_TRADEOFF = [0.5, 1, 1.5]\n",
    "\n",
    "add_normalize_by_rp_UDF = udf(\n",
    "    lambda points, rp, tradeoff: points + tradeoff * rp, DoubleType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = rp_df.join(\n",
    "    df, on=[\"club_name\", \"season\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "RP_NORMALIZED_COLS = [\n",
    "    f\"avg(overall)_rp_normalized_tradeoff_{tradeoff}\".replace(\n",
    "        \".\", \"-\"\n",
    "    ) for tradeoff in ADD_RP_TRADEOFF\n",
    "]\n",
    "\n",
    "for tradeoff, col_name in zip(ADD_RP_TRADEOFF, RP_NORMALIZED_COLS):\n",
    "    rp_df = rp_df.withColumn(\n",
    "        col_name, add_normalize_by_rp_UDF(\n",
    "            col(\"avg(overall)\"), col(\"rp_coeff\"), lit(tradeoff)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[col_name], \n",
    "        outputCol=col_name + \"_vec\"\n",
    "    )\n",
    "\n",
    "    rp_df = assembler.transform(rp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = rp_df.randomSplit(\n",
    "    # [0.9, 0.1]\n",
    "    [0.4, 0.6]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS_CV = 4\n",
    "# TODO restore all values\n",
    "FEATURES_COL = [RP_NORMALIZED_COLS[0] + \"_vec\"]\n",
    "# FEATURES_COL = [\n",
    "#     rp_normalized_col + \"_vec\" for rp_normalized_col in RP_NORMALIZED_COLS \n",
    "# ]\n",
    "LABEL_COL = \"points\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = [\"r2\", \"mse\"]\n",
    "evaluation_metrics_cv = [\"r2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ) / 400]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"lr_attempt4_predictions\"\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    # .addGrid(lr_raw.regParam, [0.0, 0.001, 0.01, 0.1, 0.5, 1])\n",
    "    # .addGrid(lr_raw.solver, [\"auto\", \"normal\", \"l-bfgs\"])\n",
    "    # .addGrid(lr_raw.standardization, [True, False])\n",
    "    # .addGrid(lr_raw.loss, [\"squaredError\", \"huber\"])\n",
    "    # huber loss does NOT support ElasticNet Regularization :(\n",
    "    # .addGrid(lr_raw.fitIntercept, [True, False])\n",
    "    # .addGrid(lr_raw.elasticNetParam, [0.0, 0.25, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                    Training set evaluation\n",
      "                    LinearRegressionModel\n",
      "                    Best model according to metric: r2\n",
      "                    \tRMSE:        11.3\n",
      "                    \tR2:          0.53\n",
      "                    \tAdjusted R2: 0.5\n",
      "                    \n",
      "\n",
      "                Test set evaluation\n",
      "                LinearRegressionModel\n",
      "                Best model according to metric r2\n",
      "                \n",
      "r2: 0.15630461159361153\n",
      "mse: 390.26254586292924\n"
     ]
    }
   ],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                + 0) / 400]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"dtr_attempt4_predictions\"\n",
    "\n",
    "estimator = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set evaluation\n",
      "                        CrossValidatorModel\n",
      "                        Best model elected by r2\n",
      "r2: 0.9880442450048018\n",
      "mse: 3.2450980392156867\n",
      "Test set evaluation\n",
      "                        CrossValidatorModel\n",
      "                        Best model elected by r2\n",
      "r2: 0.12146250404129544\n",
      "mse: 406.37922705314014\n"
     ]
    }
   ],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                + 0) / 400]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"rfr_attempt4_predictions\"\n",
    "\n",
    "estimator = RandomForestRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set evaluation\n",
      "                        CrossValidatorModel\n",
      "                        Best model elected by r2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.8144197176499781\n",
      "mse: 65.9984081679894\n",
      "Test set evaluation\n",
      "                        CrossValidatorModel\n",
      "                        Best model elected by r2\n",
      "r2: 0.11078756153929059\n",
      "mse: 364.4342346125731\n"
     ]
    }
   ],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                0) / 400]0]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"gbtr_attempt4_predictions\"\n",
    "\n",
    "estimator = GBTRegressor()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models, evaluators = learn_best_model(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    feature_vec=[FEATURES_COL],\n",
    "    label_col=LABEL_COL,\n",
    "    prediction_col=PREDICTION_COL,\n",
    "    evaluation_metrics=evaluation_metrics,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set evaluation\n",
      "                        CrossValidatorModel\n",
      "                        Best model elected by r2\n",
      "r2: 0.9999852190669405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 0.0052565824386878466\n",
      "Test set evaluation\n",
      "                        CrossValidatorModel\n",
      "                        Best model elected by r2\n",
      "r2: -0.1263864182428014\n",
      "mse: 461.6374608084736\n"
     ]
    }
   ],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = [\"accuracy\", \"f1\"]\n",
    "evaluation_metrics_cv = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO restore all values\n",
    "# LABEL_COLS = [\"macro_place_naive\", \"macro_place_complex\"]\n",
    "LABEL_COLS = [\"macro_place_naive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "PREDICTION_COL = \"svmc_attempt4_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LinearSVC(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "# for label_col in LABEL_COLS:\n",
    "#     cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "#         estimator=estimator,\n",
    "#         param_grid=param_grid,\n",
    "#         feature_vec=[FEATURES_COL],\n",
    "#         label_col=LABEL_COL,\n",
    "#         prediction_col=PREDICTION_COL,\n",
    "#         evaluation_metrics = evaluation_metrics,\n",
    "#         evaluation_metrics_cv=evaluation_metrics_cv\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "PREDICTION_COL = \"lrc_attempt4_predictions\"\n",
    "\n",
    "estimator = OneVsRest(\n",
    "    classifier=LogisticRegression(),\n",
    ")\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "# for label_col in LABEL_COLS:\n",
    "#     print(f\"Learning with label {label_col}\")\n",
    "#     cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "#         estimator=estimator,\n",
    "#         param_grid=param_grid,\n",
    "#         feature_vec=[FEATURES_COL],\n",
    "#         label_col=label_col,\n",
    "#         prediction_col=PREDICTION_COL,\n",
    "#         evaluation_metrics=evaluation_metrics,\n",
    "#         evaluation_metrics_cv=evaluation_metrics_cv\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning with label macro_place_naive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9021:(30198 + 16) / 40000][Stage 9022:(0 + 0) / 200][Stage 9025:(208 + 0) / 400]0]0]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.7_1/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4965/1920394773.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLABEL_COLS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Learning with label {label_col}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4965/2018860908.py\u001b[0m in \u001b[0;36mlearn_best_model\u001b[0;34m(estimator, param_grid, feature_vec, label_col, prediction_col, evaluation_metrics, evaluation_metrics_cv)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevaluation_metrics_cv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         cross_validated_models[metric] = cross_validations[\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         ].fit(train_df)\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.7_1/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.7_1/lib/python3.9/site-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0minheritable_thread_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam))\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.7_1/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    856\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.7_1/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9025:=========> (363 + 16) / 400][Stage 9027:>             (0 + 0) / 200] / 400]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "PREDICTION_COL = \"dtc_attempt3_predictions\"\n",
    "\n",
    "estimator = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=evaluation_metrics,\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "PREDICTION_COL = \"rfc_attempt3_predictions\"\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, [LABEL_COL])\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in LABEL_COLS:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics = [\n",
    "            \"accuracy\",\n",
    "            \"falsePositiveRateByLabel\",\n",
    "            \"precisionByLabel\",\n",
    "            \"recallByLabel\",\n",
    "            \"truePositiveRateByLabel\",\n",
    "            \"weightedFalsePositiveRate\",\n",
    "            \"weightedTruePositiveRate\",\n",
    "            \"weightedPrecision\",\n",
    "            \"weightedRecall\",\n",
    "        ],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "PREDICTION_COL = \"mlpc_attempt4_predictions\"\n",
    "\n",
    "estimator = MultilayerPerceptronClassifier()\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(estimator.featuresCol, FEATURES_COL)\n",
    "    .addGrid(estimator.labelCol, LABEL_COLS)\n",
    "    .addGrid(estimator.predictionCol, [PREDICTION_COL])\n",
    "    .addGrid(estimator.layers, [[NUM_MACRO_ROLES, NUM_MACRO_PLACES]])\n",
    "    .addGrid(estimator.solver, [\"gd\"])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cross_validated_models = dict()\n",
    "evaluators = dict()\n",
    "\n",
    "for label_col in [\"macro_place_naive\"]:\n",
    "    print(f\"Learning with label {label_col}\")\n",
    "    cross_validated_models[label_col], evaluators[label_col] = learn_best_model(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        feature_vec=[FEATURES_COL],\n",
    "        label_col=label_col,\n",
    "        prediction_col=PREDICTION_COL,\n",
    "        evaluation_metrics=[\n",
    "            \"accuracy\",\n",
    "            \"falsePositiveRateByLabel\",\n",
    "            \"precisionByLabel\",\n",
    "            \"recallByLabel\",\n",
    "            \"truePositiveRateByLabel\",\n",
    "            \"weightedFalsePositiveRate\",\n",
    "            \"weightedTruePositiveRate\",\n",
    "            \"weightedPrecision\",\n",
    "            \"weightedRecall\",\n",
    "        ],\n",
    "        evaluation_metrics_cv=evaluation_metrics_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_learning_models(\n",
    "    cross_validated_models=cross_validated_models,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_metrics_cv=evaluation_metrics_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RP impact\n",
    "\n",
    "plot showing that the more the weight of the RP coefficient is increased, the more the accuracy ofc goes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d6993cb2f9ce9a59d5d7380609d9cb5192a9dedd2735a011418ad9e827eb538"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dbee12c48506d0ea52066729cdbd5c841c92d7e5282187c802887604930b19f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
