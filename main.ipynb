{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of Big Data FC project is to predict how many points a football team belonging to the principal European Football Associations will end the season with, according to the characteristics of its players.\n",
    "\n",
    "In order to reach the goal, data relative to the football players will first be loaded, in order to then compose the football teams.\n",
    "After that, a second dataset will be used to gather seasonal rankings, for every football team\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wall of imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, a series of imports that will be used across the entire project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PySpark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "import json\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# import copy\n",
    "\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "# from pyspark.ml.feature import StandardScaler\n",
    "# from pyspark.ml.feature import PCA\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# # %matplotlib inline\n",
    "# %matplotlib widget\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from pyspark.ml.clustering import KMeans\n",
    "# from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# RANDOM_SEED = None\n",
    "\n",
    "# from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# from functools import reduce\n",
    "\n",
    "# from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# from pyspark_dist_explore import hist\n",
    "\n",
    "# from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# from pyspark.ml import Pipeline\n",
    "\n",
    "# from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# from pyspark.ml.tuning import CrossValidator\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# from operator import itemgetter\n",
    "\n",
    "# from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# from pyspark.ml.stat import Summarizer\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# from pyspark.ml.feature import UnivariateFeatureSelector\n",
    "# from pyspark.ml.linalg import DenseVector\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt \n",
    "# import seaborn as sns\n",
    "# import re\n",
    "\n",
    "# import sklearn\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.preprocessing import scale\n",
    "# from sklearn.feature_selection import RFE\n",
    "# # from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.linear_model import Lasso\n",
    "# # from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# # from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "# import warnings # supress warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the Spark context, with a given configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/16 17:18:21 WARN Utils: Your hostname, macchinario resolves to a loopback address: 127.0.1.1; using 192.168.1.12 instead (on interface enp0s31f6)\n",
      "22/06/16 17:18:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/16 17:18:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.ui.port\", \"4050\")\n",
    "    .set(\"spark.executor.memory\", \"4G\")\n",
    "    .set(\"spark.driver.memory\", \"20G\")\n",
    "    .set(\"spark.driver.maxResultSize\", \"10G\")\n",
    ")\n",
    "# .set(\"spark.master\", \"spark://192.168.1.189:4050\")\n",
    "\n",
    "\n",
    "# create the context\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading football players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to load football players data, which comes from to two different sources:\n",
    "\n",
    "* For the modern era (seasons starting from 2015 to 2020), [FIFA 15-21 complete dataset](https://www.kaggle.com/datasets/stefanoleone992/fifa-21-complete-player-dataset)\n",
    "\n",
    "* For the \"legacy\" era (seasons starting from 2007 to 2014), [sofifa.com](https://sofifa.com), a website specialized in storing data taken from EA Sports FIFA game, has been manually scraped.\n",
    "To do so, a [custom-made scraper](https://github.com/Big-Data-FC/scraper) has been developed. \n",
    "\n",
    "Scraped datasets are available [here](https://github.com/Big-Data-FC/datasets) \n",
    "\n",
    "From now on, \"modern\" will be used for seasons between 2015 and 2020, whilst \"legacy\" for all the previous years.\n",
    "\n",
    "Two different dataframes for modern and legacy will be used, there are substancial differences in the structures, among these different years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "modern_df = spark.read.csv(\n",
    "    \"data/players_*.csv\", sep=\",\", inferSchema=True, header=True, multiLine=True\n",
    ")\n",
    "\n",
    "legacy_df = spark.read.csv(\n",
    "    \"data/scraped_players_*.csv\", sep=\",\", inferSchema=True, header=True, \n",
    "    multiLine=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing football players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the European Leagues supported by Big-Data-FC\n",
    "leagues = [\n",
    "    \"Spain Primera Division\",\n",
    "    \"German 1. Bundesliga\",\n",
    "    \"French Ligue 1\",\n",
    "    \"English Premier League\",\n",
    "    \"Italian Serie A\",\n",
    "    \"Holland Eredivisie\",\n",
    "]\n",
    "\n",
    "# These are the seasons supported by Big-Data-FC\n",
    "seasons_modern = [\"20\", \"19\", \"18\", \"17\", \"16\", \"15\", \"14\"] \n",
    "# seasons_modern = [\"20\", \"19\"] \n",
    "seasons_legacy = [\"13\", \"12\", \"11\", \"10\", \"09\", \"08\", \"07\"]\n",
    "# seasons_legacy = [ ]\n",
    "seasons = seasons_legacy + seasons_modern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behing \"macro role\" is pretty simple: basically, a macro role is a collection of \"affine\" football roles.\n",
    "\n",
    "For example, \"central midfielder\", \"advanced midfielder\", \"left|right wing\" roles can be grouped together in the same macro_role, \"midfielder\", according to the position in the field that the role covers.\n",
    "\n",
    "Macro roles will be used later on, in a subsequent learning phase.\n",
    "For this reason, much more about them will be touched in future points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_roles = [\"0.0\", \"1.0\", \"2.0\", \"3.0\"]\n",
    "\n",
    "roles_to_macro_roles_dict = {\n",
    "    \"GK\": \"0\",\n",
    "    \"LB\": \"1\",\n",
    "    \"RB\": \"1\",\n",
    "    \"RWB\": \"1\",\n",
    "    \"LWB\": \"1\",\n",
    "    \"CB\": \"1\",\n",
    "    \"CDM\": \"2\",\n",
    "    \"CM\": \"2\",\n",
    "    \"RM\": \"2\",\n",
    "    \"LM\": \"2\",\n",
    "    \"CAM\": \"2\",\n",
    "    \"RW\": \"3\",\n",
    "    \"LW\": \"3\",\n",
    "    \"ST\": \"3\",\n",
    "    \"LF\": \"3\",\n",
    "    \"RF\": \"3\",\n",
    "    \"CF\": \"3\",\n",
    "}\n",
    "\n",
    "NUM_MACRO_ROLES = 4\n",
    "\n",
    "roles_to_macro_role_UDF = udf(\n",
    "    lambda roles: float(\n",
    "        roles_to_macro_roles_dict[roles.split(\",\")[0]]\n",
    "    ), \n",
    "    StringType()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all available columns in the various datasets, only the ones in `columuns` will be taken into considerations, because they are the only ones that can be used as features.\n",
    "\n",
    "In fact, the other columns include either graphical or non-informative information, such as jersey number, celebration moves, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"short_name\",\n",
    "    \"club_name\",\n",
    "    \"league_name\",\n",
    "    \"season\",\n",
    "    \"player_positions\",\n",
    "    \"macro_role\",\n",
    "    \"overall\",\n",
    "    \"value\",\n",
    "    \"pace\",\n",
    "    \"shooting\",\n",
    "    \"passing\",\n",
    "    \"dribbling\",\n",
    "    \"defending\",\n",
    "    \"physic\",\n",
    "    \"attacking_crossing\",\n",
    "    \"attacking_finishing\",\n",
    "    \"attacking_heading_accuracy\",\n",
    "    \"attacking_short_passing\",\n",
    "    \"skill_dribbling\",\n",
    "    \"skill_fk_accuracy\",\n",
    "    \"skill_long_passing\",\n",
    "    \"skill_ball_control\",\n",
    "    \"movement_acceleration\",\n",
    "    \"movement_sprint_speed\",\n",
    "    \"movement_reactions\",\n",
    "    \"power_shot_power\",\n",
    "    \"power_stamina\",\n",
    "    \"power_strength\",\n",
    "    \"power_long_shots\",\n",
    "    \"mentality_aggression\",\n",
    "    \"mentality_penalties\",\n",
    "    \"defending_standing_tackle\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets do **not** explicitly include the year to which the record refers to.\n",
    "\n",
    "Rather, this information is implicitly stored in a URL, which has its own field.\n",
    "For this reason, a function to extract such information from aforementioned field is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(url):\n",
    "    url_split = url.split(\"/\")\n",
    "\n",
    "    # FIFA years must be scaled by a negative factor of one (i.e. 2021 has to be 2020, etc.)\n",
    "    # This is needed to ensure compatibility with the seasonal score dataset\n",
    "    return str(\n",
    "        (int(url_split[-2 if url_split[-1] == \"\" else -1][0:2]) - 1)\n",
    "    ).zfill(2)\n",
    "\n",
    "get_season_UDF = udf(lambda url: get_season(url), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the actual pre-process.\n",
    "\n",
    "Some actions are needed by legacy and modern both, whilst other are exclusive to either one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting season from the player URL, as per previous cell\n",
    "pre_processed_modern_df = modern_df.withColumn(\n",
    "    \"season\", get_season_UDF(col(\"player_url\"))\n",
    ")\n",
    "pre_processed_legacy_df = legacy_df.withColumn(\n",
    "    \"season\", get_season_UDF(col(\"player_url\"))\n",
    ")\n",
    "\n",
    "# Taking only the players playing for teams in supported Leagues, \n",
    "# in the supported seasons\n",
    "pre_processed_modern_df = pre_processed_modern_df.where(\n",
    "    (pre_processed_modern_df.league_name.isin(leagues))\n",
    "    &\n",
    "    (pre_processed_modern_df.season.isin(seasons_modern))\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.where(\n",
    "    (pre_processed_legacy_df.league_name.isin(leagues))\n",
    "    &\n",
    "    (pre_processed_legacy_df.season.isin(seasons_legacy))\n",
    ")\n",
    "\n",
    "# Dropping duplicate players\n",
    "pre_processed_modern_df = pre_processed_modern_df.dropDuplicates([\"player_url\"])\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.dropDuplicates([\"player_url\"])\n",
    "\n",
    "# Selected columns have been checked for absence of null/missing data.\n",
    "# Nevertheless, to ensure compatibility and reusability with other datasets, \n",
    "# a null-filling sweep is done\n",
    "pre_processed_modern_df = pre_processed_modern_df.na.fill(0)\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.na.fill(0)\n",
    "\n",
    "# Getting the macro role of the player, according to its field position\n",
    "pre_processed_modern_df = pre_processed_modern_df.withColumn(\n",
    "    \"macro_role\", roles_to_macro_role_UDF(col(\"player_positions\"))\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumn(\n",
    "    \"macro_role\", roles_to_macro_role_UDF(col(\"player_positions\"))\n",
    ")\n",
    "\n",
    "# Renaming the \"value_eur\" field to \"value\" to have compatiblity with legacy\n",
    "pre_processed_modern_df = pre_processed_modern_df.withColumnRenamed(\n",
    "    \"value_eur\", \"value\"\n",
    ")\n",
    "\n",
    "# Renaming some legacy columns, so as they have the same name as in the modern\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"pas\", \"passing\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"dri\", \"dribbling\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.drop(col(\"defending\"))\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"def\", \"defending\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"phy\", \"physic\"\n",
    ")\n",
    "\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"sho\", \"shooting\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"pac\", \"pace\"\n",
    ")\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.withColumnRenamed(\n",
    "    \"bov\", \"overall\"\n",
    ")\n",
    "\n",
    "# Keeping only the needed columns.\n",
    "pre_processed_modern_df = pre_processed_modern_df.select(columns)\n",
    "pre_processed_legacy_df = pre_processed_legacy_df.select(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing, both dataframes have the same set of columns, so they can be concatenated together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_df = pre_processed_modern_df.unionByName(\n",
    "    pre_processed_legacy_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result.\n",
    "\n",
    "For graphical reasons, only a selection of the columns will be shown, just to give an idea of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+------+----------------+----------+-------+\n",
      "|   short_name|           club_name|         league_name|season|player_positions|macro_role|overall|\n",
      "+-------------+--------------------+--------------------+------+----------------+----------+-------+\n",
      "|   D. Boateng|            SD Eibar|Spain Primera Div...|    14|         CDM, CM|       2.0|     71|\n",
      "|    S. Padoin|            Juventus|     Italian Serie A|    14|          RM, CM|       2.0|     73|\n",
      "|    S. Padoin|            Juventus|     Italian Serie A|    15|      LB, RM, CM|       1.0|     75|\n",
      "|    S. Padoin|            Cagliari|     Italian Serie A|    18|      RB, LM, CM|       1.0|     71|\n",
      "|  A. Aquilani|       UD Las Palmas|Spain Primera Div...|    17|         CM, CDM|       2.0|     75|\n",
      "|      S. Pepe|            Juventus|     Italian Serie A|    14|          RM, LM|       2.0|     75|\n",
      "|         Xavi|        FC Barcelona|Spain Primera Div...|    14|              CM|       2.0|     86|\n",
      "|   F. Modesto|Sporting Club de ...|      French Ligue 1|    14|          CB, RB|       1.0|     72|\n",
      "|     A. Boruc|         Bournemouth|English Premier L...|    17|              GK|       0.0|     77|\n",
      "|       Aduriz|Athletic Club de ...|Spain Primera Div...|    15|              ST|       3.0|     82|\n",
      "|       Aduriz|Athletic Club de ...|Spain Primera Div...|    19|              ST|       3.0|     82|\n",
      "|Victor Valdés|   Manchester United|English Premier L...|    15|              GK|       0.0|     82|\n",
      "|    G. Pegolo|            Sassuolo|     Italian Serie A|    16|              GK|       0.0|     72|\n",
      "|    D. Brivio|               Genoa|     Italian Serie A|    16|              LB|       1.0|     73|\n",
      "|      Y. Pelé|Olympique de Mars...|      French Ligue 1|    17|              GK|       0.0|     75|\n",
      "| M. Tacalfred|      Stade de Reims|      French Ligue 1|    14|              CB|       1.0|     71|\n",
      "| M. Tacalfred|      Stade de Reims|      French Ligue 1|    15|              CB|       1.0|     73|\n",
      "|     J. Audel|           FC Nantes|      French Ligue 1|    15|      LM, RM, ST|       2.0|     71|\n",
      "|     C. Maury|         GFC Ajaccio|      French Ligue 1|    15|              GK|       0.0|     68|\n",
      "|    T. Starke|   FC Bayern München|German 1. Bundesliga|    14|              GK|       0.0|     76|\n",
      "+-------------+--------------------+--------------------+------+----------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pre_processed_df.select(*columns[0:7]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building football teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_teams_df = pre_processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having pre-processed the football players, it's time to build the football teams.\n",
    "\n",
    "For the sake of the learning stage of the project, teams are differenciated across different seasons. So, for example, Real Madrid of season 2020 is **different** than Real Madrid of season 2018.\n",
    "\n",
    "A football team is the set of the average of the features of all of its football players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that are considered as features\n",
    "PLAYER_FEATURES = [\n",
    "    \"overall\",\n",
    "    \"value\",\n",
    "    \"pace\",\n",
    "    \"shooting\",\n",
    "    \"passing\",\n",
    "    \"dribbling\",\n",
    "    \"defending\",\n",
    "    \"physic\",\n",
    "    \"attacking_crossing\",\n",
    "    \"attacking_finishing\",\n",
    "    \"attacking_heading_accuracy\",\n",
    "    \"attacking_short_passing\",\n",
    "    \"skill_dribbling\",\n",
    "    \"skill_fk_accuracy\",\n",
    "    \"skill_long_passing\",\n",
    "    \"skill_ball_control\",\n",
    "    \"movement_acceleration\",\n",
    "    \"movement_sprint_speed\",\n",
    "    \"movement_reactions\",\n",
    "    \"power_shot_power\",\n",
    "    \"power_stamina\",\n",
    "    \"power_strength\",\n",
    "    \"power_long_shots\",\n",
    "    \"mentality_aggression\",\n",
    "    \"mentality_penalties\",\n",
    "    \"defending_standing_tackle\"\n",
    "]\n",
    "\n",
    "# Apposing the avg pre-fix to features\n",
    "PLAYER_FEATURES_AVG = [\n",
    "    \"avg(\" + player_feature + \")\" for player_feature in PLAYER_FEATURES\n",
    "]\n",
    "\n",
    "# Target variable of the learning stage\n",
    "TARGET_VARIABLE = \"points\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composing the football team, as introduced before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_teams_df = football_teams_df.select(\n",
    "    \"season\", \"club_name\", *PLAYER_FEATURES\n",
    ").groupBy(\n",
    "    [\"season\", \"club_name\"]\n",
    ").agg(\n",
    "    { player_feature: \"avg\" for player_feature in PLAYER_FEATURES }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result.\n",
    "\n",
    "For graphical reasons, only a selection of the columns will be shown, just to give an idea of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----------------+--------------------+------------------+\n",
      "|season|           club_name|     avg(overall)|          avg(value)|         avg(pace)|\n",
      "+------+--------------------+-----------------+--------------------+------------------+\n",
      "|    18|        ADO Den Haag|66.33333333333333|  1114166.6666666667|            56.625|\n",
      "|    16|       Chievo Verona|             71.3|  2411666.6666666665|              55.9|\n",
      "|    16|         Southampton|71.93939393939394|   5041060.606060606| 64.66666666666667|\n",
      "|    17|Athletic Club de ...|            75.75|   9794642.857142856|59.214285714285715|\n",
      "|    15|   Manchester United|76.25925925925925|1.3582037037037037E7| 59.96296296296296|\n",
      "|    19|   Manchester United|76.84848484848484|1.5187121212121213E7| 67.03030303030303|\n",
      "|    19|          Hertha BSC|71.75757575757575|   5872575.757575758| 63.84848484848485|\n",
      "|    15|         Bournemouth|69.89655172413794|   2162758.620689655| 65.10344827586206|\n",
      "|    20|FC Girondins de B...| 70.6896551724138|  3988620.6896551726| 55.44827586206897|\n",
      "|    15|             Watford|71.32142857142857|  2790178.5714285714|62.785714285714285|\n",
      "|    16|     West Ham United|72.54545454545455|   7179242.424242424| 66.93939393939394|\n",
      "|    15|          1. FC Köln|71.79166666666667|  3654166.6666666665|62.666666666666664|\n",
      "|    16|       SC Heerenveen|66.76666666666667|  1257333.3333333333|              59.4|\n",
      "|    18|    AS Saint-Étienne|70.11111111111111|   4653518.518518519| 63.51851851851852|\n",
      "|    14|Sporting Club de ...|66.21212121212122|   996363.6363636364| 60.93939393939394|\n",
      "|    15|     Atlético Madrid|            76.92|            1.2458E7|             64.84|\n",
      "|    17|    Stade Rennais FC|69.15151515151516|   3382575.757575758| 61.36363636363637|\n",
      "|    17|        Swansea City|71.24242424242425|   5101818.181818182| 58.06060606060606|\n",
      "|    17| Eintracht Frankfurt|72.33333333333333|   5289393.939393939| 64.27272727272727|\n",
      "|    15|       VfB Stuttgart|70.79310344827586|  3495862.0689655175|62.206896551724135|\n",
      "+------+--------------------+-----------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "football_teams_df.select(\n",
    "    \"season\", \"club_name\", *PLAYER_FEATURES_AVG[0:3]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading football teams seasonal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having dealt with the football players and composed them into football clubs, it's time to get \"target\" data for the learning stage: the final ranking, for every team of every year.\n",
    "\n",
    "First step is to load the data from disk, which has been taken from the [European Football Dataset](https://www.kaggle.com/datasets/josephvm/european-club-football-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_scores_df = (\n",
    "    spark.read.csv(\n",
    "        \"data/all_tables_fixed_renamed_leagues.csv\",\n",
    "        sep=\",\",\n",
    "        inferSchema=True,\n",
    "        header=True,\n",
    "        multiLine=True,\n",
    "    )\n",
    "    .withColumnRenamed(\"Year\", \"season\")\n",
    "    .withColumnRenamed(\"Team\", \"club_name\")\n",
    "    .withColumnRenamed(\"P\", \"points\")\n",
    "    .withColumnRenamed(\"Place\", \"place\")\n",
    "    .withColumnRenamed(\"League\", \"league\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing football teams seasonal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all the available columns, only the ones in `seasonal_scores_columns` will be taken into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_scores_columns = [\n",
    "    \"season\", \"club_name\", \"points\", \"place\", \"league\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonal score dataset uses abbreviated version of football team names (for example: \"BAR\" for Barcelona, \"LEI\" for Leicester, etc.), which would cause incompatibility with modern and legacy FIFA datasets.\n",
    "\n",
    "For this reason, a custom, hand-made, mapping procedure has been developed, so as every contracted name can successfully be expanded to the correct FIFA name.\n",
    "\n",
    "Furthermore, name inconsistencies (for example, \"Torino\" and \"Torino FC\") have been solved, by hand, in this step too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/clubs_map.json\")\n",
    "club_name_abbr_to_ext = json.load(f)\n",
    "f.close()\n",
    "\n",
    "ABBREVIATED_CLUB_NAME_NOT_FOUD = \"ABBREVIATED_CLUB_NAME_NOT_FOUD\"\n",
    "GENERAL_EXCEPTION = \"GENERAL_EXCEPTION\"\n",
    "\n",
    "def extend_club_name(club_name_abbr):\n",
    "    try:\n",
    "        return club_name_abbr_to_ext[club_name_abbr]\n",
    "    except KeyError as e:\n",
    "        return ABBREVIATED_CLUB_NAME_NOT_FOUD\n",
    "    except Exception as e:\n",
    "        return GENERAL_EXCEPTION\n",
    "\n",
    "extend_club_name_UDF = udf(\n",
    "    lambda club_name_abbr: extend_club_name(str(club_name_abbr)),\n",
    "    StringType(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rankings dataset the seasons are expressed as YYYY, whilst FIFA uses the YY encoding.\n",
    "\n",
    "For this reason, to guarantee compatibility, season value in rankings dataset is abbreviated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviate_season_UDF = udf(\n",
    "    lambda season: str(season)[-2:],\n",
    "    StringType(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_seasonal_scores_df = seasonal_scores_df\n",
    "\n",
    "# Abbreviating season, as per previous cell\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"season\", abbreviate_season_UDF(col(\"season\"))\n",
    ")\n",
    "\n",
    "# Keeping only supported leagues in supported seasons\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.where(\n",
    "    (pre_processed_seasonal_scores_df.season.isin(seasons))\n",
    "    & \n",
    "    (pre_processed_seasonal_scores_df.league.isin(leagues))\n",
    ")\n",
    "\n",
    "# Selecting only the desired columns\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.select(\n",
    "    seasonal_scores_columns\n",
    ")\n",
    "\n",
    "# Although data has been checked for duplicates and missing value, to ensure \n",
    "# operabiloty with other datasets, the pre-processing steps are still performed\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.dropDuplicates(\n",
    "    seasonal_scores_columns\n",
    ")\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.na.fill(0)\n",
    "\n",
    "# Extending club names, as per previous cell\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"club_name\", extend_club_name_UDF(col(\"club_name\"))\n",
    ")\n",
    "# Checking whether club name expansions went all good or not\n",
    "if pre_processed_seasonal_scores_df.filter(\n",
    "    col(\"club_name\") == ABBREVIATED_CLUB_NAME_NOT_FOUD\n",
    ").count() > 0:\n",
    "    print(\"WARN: some clubs have NOT been found\")\n",
    "    print(\"Please check your data\")\n",
    "\n",
    "# Casting points to float, as required by learning procedures\n",
    "pre_processed_seasonal_scores_df = pre_processed_seasonal_scores_df.withColumn(\n",
    "    \"points\", pre_processed_seasonal_scores_df.points.cast(DoubleType())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------+-----+--------------------+\n",
      "|season|           club_name|points|place|              league|\n",
      "+------+--------------------+------+-----+--------------------+\n",
      "|    18|   Tottenham Hotspur|  71.0|    4|English Premier L...|\n",
      "|    18|   Manchester United|  66.0|    6|English Premier L...|\n",
      "|    08|RC Recreativo de ...|  33.0|   20|Spain Primera Div...|\n",
      "|    16|            RC Celta|  45.0|   13|Spain Primera Div...|\n",
      "|    16|       UD Las Palmas|  39.0|   14|Spain Primera Div...|\n",
      "|    15|               Carpi|  38.0|   18|     Italian Serie A|\n",
      "|    12|        FC Groningen|  43.0|    7|  Holland Eredivisie|\n",
      "|    13|           FC Twente|  63.0|    3|  Holland Eredivisie|\n",
      "|    12|      Stade de Reims|  43.0|   14|      French Ligue 1|\n",
      "|    16|Sporting Club de ...|  34.0|   20|      French Ligue 1|\n",
      "|    16|   Tottenham Hotspur|  86.0|    2|English Premier L...|\n",
      "|    07|          Sevilla FC|  64.0|    5|Spain Primera Div...|\n",
      "|    18|      Rayo Vallecano|  32.0|   20|Spain Primera Div...|\n",
      "|    17|         Hannover 96|  39.0|   13|German 1. Bundesliga|\n",
      "|    19|   Borussia Dortmund|  69.0|    2|German 1. Bundesliga|\n",
      "|    19|                Ajax|  56.0|    1|  Holland Eredivisie|\n",
      "|    09|  Olympique Lyonnais|  72.0|    2|      French Ligue 1|\n",
      "|    10|            OGC Nice|  46.0|   17|      French Ligue 1|\n",
      "|    12|Olympique de Mars...|  71.0|    2|      French Ligue 1|\n",
      "|    20|           FC Nantes|  40.0|   18|      French Ligue 1|\n",
      "+------+--------------------+------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_processed_seasonal_scores_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining football teams features with their seasonal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two dataframes (players and seasonal scores) need to be merged together to form a single dataframe.\n",
    "This can easily be done by joining on the key (`season`, `club_name`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = football_teams_df.join(\n",
    "    pre_processed_seasonal_scores_df,\n",
    "    on = [\"season\", \"club_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check whether some clubs were left out by the aforementioned join operation, two differences are computed:\n",
    "\n",
    "* Seasonal scores dataframe - joined dataframe.\n",
    "* Football teams dataframe - joined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Some football teams have been left out the join (pre_processed_seasonal_scores_df)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        club_name|\n",
      "+-----------------+\n",
      "| FC Hansa Rostock|\n",
      "|Urbs Reggina 1914|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Some football teams have been left out the join (football_teams_df)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           club_name|\n",
      "+--------------------+\n",
      "|    RC Celta de Vigo|\n",
      "|Montpellier Hérau...|\n",
      "|           SC Bastia|\n",
      "|      Udinese Calcio|\n",
      "|RCD Espanyol de B...|\n",
      "|  F.C. Hansa Rostock|\n",
      "|U.S. Sassuolo Calcio|\n",
      "|Unión Deportiva A...|\n",
      "|      Real Madrid CF|\n",
      "|      U.C. Sampdoria|\n",
      "| Sport-Club Freiburg|\n",
      "| Real Betis Balompié|\n",
      "|         Torino F.C.|\n",
      "|      TSG Hoffenheim|\n",
      "|  Atlético de Madrid|\n",
      "|            AC Milan|\n",
      "|     N.E.C. Nijmegen|\n",
      "|Levante Unión Dep...|\n",
      "|  Urbs Reggina 1914 |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff = pre_processed_seasonal_scores_df.select(\"club_name\").subtract(df.select(\"club_name\")).distinct()\n",
    "\n",
    "if diff.count() > 0:\n",
    "    print(\"WARN: Some football teams have been left out the join (pre_processed_seasonal_scores_df)\")\n",
    "    diff.show()\n",
    "\n",
    "diff = football_teams_df.select(\"club_name\").subtract(df.select(\"club_name\")).distinct()\n",
    "if diff.count() > 0:\n",
    "    print(\"WARN: Some football teams have been left out the join (football_teams_df)\")\n",
    "    diff.show()\n",
    "\n",
    "del diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result.\n",
    "\n",
    "For graphical reasons, only a selection of the columns will be shown, just to give an idea of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-----------------+------------------+------+-----+\n",
      "|season|              league|           club_name|     avg(overall)|         avg(pace)|points|place|\n",
      "+------+--------------------+--------------------+-----------------+------------------+------+-----+\n",
      "|    19|German 1. Bundesliga|          Hertha BSC|71.75757575757575| 63.84848484848485|  41.0|   10|\n",
      "|    20|      French Ligue 1|FC Girondins de B...| 70.6896551724138| 55.44827586206897|  45.0|   12|\n",
      "|    19|English Premier L...|   Manchester United|76.84848484848484| 67.03030303030303|  66.0|    3|\n",
      "|    20|      French Ligue 1|          LOSC Lille|70.03030303030303| 62.21212121212121|  83.0|    1|\n",
      "|    20|Spain Primera Div...|         Real Madrid|            79.25|           66.9375|  84.0|    2|\n",
      "|    20|  Holland Eredivisie|          FC Utrecht|             69.0|              64.0|  53.0|    6|\n",
      "|    19|      French Ligue 1| Paris Saint-Germain| 75.9090909090909| 68.36363636363636|  68.0|    1|\n",
      "|    19|     Italian Serie A|          Fiorentina|             70.4|              58.2|  49.0|   10|\n",
      "|    20|  Holland Eredivisie|           FC Twente|             64.0| 60.69565217391305|  41.0|   10|\n",
      "|    19|German 1. Bundesliga|  1. FC Union Berlin|68.78787878787878|59.515151515151516|  41.0|   11|\n",
      "|    20|      French Ligue 1|    AS Saint-Étienne|67.96969696969697| 61.18181818181818|  46.0|   11|\n",
      "|    20|     Italian Serie A|             Udinese|70.15151515151516|55.484848484848484|  40.0|   14|\n",
      "|    19|  Holland Eredivisie|        ADO Den Haag|            64.84|             55.56|  19.0|   17|\n",
      "|    19|German 1. Bundesliga| Eintracht Frankfurt|73.54545454545455|59.333333333333336|  45.0|    9|\n",
      "|    19|  Holland Eredivisie|          PEC Zwolle|66.07407407407408|              61.0|  26.0|   15|\n",
      "|    20|      French Ligue 1|Olympique de Mars...|72.03030303030303| 62.09090909090909|  60.0|    5|\n",
      "|    20|German 1. Bundesliga|       VfB Stuttgart|68.33333333333333|61.666666666666664|  45.0|    9|\n",
      "|    20|      French Ligue 1|RC Strasbourg Alsace|71.08333333333333|            56.375|  42.0|   15|\n",
      "|    20|  Holland Eredivisie|            FC Emmen|63.86666666666667|              58.7|  30.0|   16|\n",
      "|    19|     Italian Serie A|            Atalanta|73.78571428571429| 60.07142857142857|  78.0|    3|\n",
      "+------+--------------------+--------------------+-----------------+------------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"season\", \"league\", \"club_name\", \"avg(overall)\", \"avg(pace)\", \"points\", \"place\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1: \"naive\" player features\n",
    "\n",
    "Naive --> simply take the given features and NOT crafting them from other learning processes (i.e. NO clustering or stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FEATURES = PLAYER_FEATURES_AVG\n",
    "ALL_FEATURES.remove(\"avg(overall)\")\n",
    "ALL_FEATURES.remove(\"avg(value)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=ALL_FEATURES, outputCol=\"all_vec\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_target_relation(\n",
    "    data, x, y, n_rows = 12, n_cols = 2, figsize = (20, 40), color = \"#000000\"\n",
    "):\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "    for x_ind, x_value in enumerate(x):\n",
    "        ax = sns.regplot(\n",
    "            data=pdf,\n",
    "            x=x_value,\n",
    "            y=y,\n",
    "            color = color,\n",
    "            ax=axes[x_ind // n_cols, x_ind % n_cols],\n",
    "        )\n",
    "\n",
    "\n",
    "    fig.tight_layout(pad=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distribution(\n",
    "    data, features, figsize = (20, 40), \n",
    "    color = \"#000000\"\n",
    "):\n",
    "\n",
    "    n_cols = 2\n",
    "    n_rows = int(len(features) / n_cols) if len(features) >= n_cols else n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 30))\n",
    "\n",
    "    for feature_ind, feature in enumerate(features):\n",
    "        _ = sns.histplot(\n",
    "            data[feature],\n",
    "            kde=True,\n",
    "            color=color,\n",
    "            facecolor=color,\n",
    "            ax=axes[feature_ind // n_cols, feature_ind % n_cols],\n",
    "        )\n",
    "\n",
    "    fig.tight_layout(pad=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(\n",
    "    data, features, title = \"Pearson Correlation Matrix\", figsize = (16,12)\n",
    "):\n",
    "\n",
    "    mask = np.zeros_like(data[features].corr(), dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    with sns.axes_style(\"white\"):  # Temporarily set the background to white\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        plt.title(title, fontsize=24)\n",
    "\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "        _ = sns.heatmap(\n",
    "            data[features].corr(),\n",
    "            linewidths=0.25,\n",
    "            vmax=0.7,\n",
    "            square=True,\n",
    "            ax=ax,\n",
    "            cmap=cmap,\n",
    "            linecolor=\"w\",\n",
    "            annot=True,\n",
    "            annot_kws={\"size\": 8},\n",
    "            mask=mask,\n",
    "            cbar_kws={\"shrink\": 0.9},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[explain what does \"raw\" mean]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_RAW = \"#332FD0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES, TARGET_VARIABLE, figsize=(4,4), color=COLOR_RAW\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES, color = COLOR_RAW, figsize=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(pdf, ALL_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have some features with skewed distributions, we'll try to standardize, to see whether it helps with feature skewness or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(\n",
    "    inputCol=\"all_vec\", \n",
    "    outputCol=\"all_vec_std\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized_df = scaler.fit(standardized_df).transform(standardized_df)\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_STD = \"#9254C8\"\n",
    "\n",
    "ALL_FEATURES_STD = [\n",
    "    player_feature + \"_std\" for player_feature in ALL_FEATURES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = standardized_df.toPandas()\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO make a commodity function, because it will be used in other normalizations as well\n",
    "\n",
    "pdf = pdf.reindex(\n",
    "    columns=list(pdf.columns) + ALL_FEATURES_STD\n",
    ")\n",
    "\n",
    "pdf[ALL_FEATURES_STD] = pdf[\n",
    "    \"all_vec_std\"\n",
    "].transform(\n",
    "    {\n",
    "        ALL_FEATURES_STD[i]: itemgetter(i) for i, p in enumerate(ALL_FEATURES_STD)\n",
    "    }\n",
    "\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_STD, TARGET_VARIABLE, figsize=(4,4), color=COLOR_STD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    pdf, ALL_FEATURES_STD, color = COLOR_STD, figsize=(10,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[state why pearson correlation matrix does NOT make sense to be plotted again]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FEATURES_LOG = [\n",
    "    player_feature + \"_log\" for player_feature in ALL_FEATURES\n",
    "]\n",
    "\n",
    "COLOR_LOG = \"#E15FED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_log_UDF = udf(\n",
    "    lambda value: float(np.log2(value)), DoubleType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, fl in zip(ALL_FEATURES, ALL_FEATURES_LOG):\n",
    "    # log_df = log_df.withColumn(fl, to_log_UDF(col(f)))\n",
    "    df = df.withColumn(fl, to_log_UDF(col(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = log_df.toPandas()\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_LOG, TARGET_VARIABLE, figsize=(4,4), color=COLOR_LOG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES_LOG, color = COLOR_LOG, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-max transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FEATURES_MIN_MAX = [\n",
    "    player_feature + \"_min_max\" for player_feature in ALL_FEATURES\n",
    "]\n",
    "\n",
    "COLOR_MIN_MAX = \"#6EDCD9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"all_vec\", \n",
    "    outputCol=\"all_vec_min_max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_df = scaler.fit(min_max_df).transform(min_max_df)\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = min_max_df.toPandas()\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO make a commodity function, because it will be used in other normalizations as well\n",
    "\n",
    "pdf = pdf.reindex(\n",
    "    columns=list(pdf.columns) + ALL_FEATURES_MIN_MAX\n",
    ")\n",
    "\n",
    "pdf[ALL_FEATURES_MIN_MAX] = pdf[\n",
    "    \"all_vec_min_max\"\n",
    "].transform(\n",
    "    {\n",
    "        ALL_FEATURES_MIN_MAX[i]: itemgetter(i) for i, p in enumerate(ALL_FEATURES_MIN_MAX)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, ALL_FEATURES_MIN_MAX, TARGET_VARIABLE, figsize=(4,4), color=COLOR_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, ALL_FEATURES_MIN_MAX, color = COLOR_MIN_MAX, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO place learning on ## titles here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2: \"less is more\"\n",
    "\n",
    "Ok, considering all features gives trash results.\n",
    "\n",
    "What if we embrace the \"less is more idea\" and try to improve the results by means of using less features?\n",
    "\n",
    "Nevertheless, feature correlation is very high, so, intrinsicly, it already did NOT make much sense to consider them all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (on min-max normalized data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_df = min_max_df\n",
    "# pca_df = df\n",
    "\n",
    "PCA_NUM_COMPONENTS = 5\n",
    "PCA_NUM_COMPONENTS_TO_PLOT = 2\n",
    "\n",
    "pca = PCA(\n",
    "    k=PCA_NUM_COMPONENTS, \n",
    "    inputCol=\"all_vec_min_max\", \n",
    "    outputCol=\"all_vec_min_max_pcs\"\n",
    ")\n",
    "\n",
    "# pca_model = pca.fit(pca_df)\n",
    "# pca_df = pca_model.transform(pca_df)\n",
    "pca_model = pca.fit(df)\n",
    "df = pca_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "_ = sns.barplot(\n",
    "    x=[i for i in range(PCA_NUM_COMPONENTS_TO_PLOT)],\n",
    "    y=pca_model.explainedVariance.values[0:PCA_NUM_COMPONENTS_TO_PLOT],\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_xlabel(\"Eigenvalues\", labelpad=16, fontsize=16)\n",
    "_ = ax.set_ylabel(\"Proportion of Variance\", fontsize=16)\n",
    "_ = ax.set_xticklabels(\n",
    "    [f\"Principal Component {i}\" for i in range(PCA_NUM_COMPONENTS_TO_PLOT)], \n",
    "    rotation=0\n",
    ")\n",
    "_ = ax.set_title(\"Explained variance of each Principal Component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(data, x, y, c, x_label, y_label):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "    _ = plt.scatter(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        c = c,\n",
    "        edgecolor=\"none\",\n",
    "        alpha=1,\n",
    "        cmap=\"rainbow\",\n",
    "        axes=ax\n",
    "    )\n",
    "\n",
    "    _ = ax.set_xlabel(x_label, labelpad=20, fontsize=16)\n",
    "    _ = ax.set_ylabel(y_label, fontsize=16)\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_pdf = pca_df.toPandas()\n",
    "pca_pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    pca_pdf, \n",
    "    pca_pdf.all_vec_min_max_pcs.map(lambda x: x[0]),\n",
    "    pca_pdf.all_vec_min_max_pcs.map(lambda x: x[1]),\n",
    "    pca_pdf.points,\n",
    "    \"Principal Component 0\",\n",
    "    \"Principal Component 1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate whether points should be normlized to same scale as PCA values\n",
    "scatter_plot(\n",
    "    pca_pdf, \n",
    "    pca_pdf.all_vec_min_max_pcs.map(lambda x: x[0]),\n",
    "    pca_pdf.points,\n",
    "    pca_pdf.points,\n",
    "    \"Principal Component 0\",\n",
    "    \"Points\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate whether points should be normlized to same scale as PCA values\n",
    "scatter_plot(\n",
    "    pca_pdf, \n",
    "    pca_pdf.all_vec_min_max_pcs.map(lambda x: x[1]),\n",
    "    pca_pdf.points,\n",
    "    pca_pdf.points,\n",
    "    \"Principal Component 1\",\n",
    "    \"Points\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[bridge between this and Feature selection]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TNSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log scaling reduces skewedness of \"avg(mentality_penalties)\" feature BUT it increases the skewedness of all the other features.\n",
    "The other scalings (z-score and min-max) do NOT appear to be different than the \"raw\" data distribution.\n",
    "\n",
    "For this reason and due to the limited amount of resources available on Google Colab, we decided to stick with the min-max scaled data.\n",
    "In fact, the min-max scaling places \"for free\" all the features in the same scale, which is a very important consideration for SVM, which will be used in the upcoming sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection_df = min_max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = UnivariateFeatureSelector(\n",
    "    featuresCol=\"all_vec\",\n",
    "    labelCol=TARGET_VARIABLE, \n",
    "    selectionMode=\"percentile\"\n",
    ").setFeatureType(\"continuous\").setLabelType(\"categorical\").setSelectionThreshold(0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result = dict()\n",
    "\n",
    "# for thr in range(0.1, 1.1, 0.1):\n",
    "# for thr in np.linspace(1, 1, 1):\n",
    "for thr in [0.1]:\n",
    "\n",
    "    selector.setSelectionThreshold(thr)\n",
    "    selector.setOutputCol(\"ufs_\" + str(thr).replace(\".\", \"-\")),\n",
    "    fit_result[str(thr)] = selector.fit(df)\n",
    "    # feature_selection_df = fit_result[str(thr)].transform(\n",
    "    #     feature_selection_df\n",
    "    # )\n",
    "    df = fit_result[str(thr)].transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection_pdf = feature_selection_df.toPandas()\n",
    "feature_selection_pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    feature_selection_pdf,\n",
    "    feature_selection_pdf[\"ufs_0-1\"].map(lambda x: x[0]),\n",
    "    feature_selection_pdf[\"ufs_0-1\"].map(lambda x: x[1]),\n",
    "    feature_selection_pdf.points,\n",
    "    \"Feature 0\",\n",
    "    \"Feature 1\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    feature_selection_pdf,\n",
    "    feature_selection_pdf[\"ufs_0-1\"].map(lambda x: x[0]),\n",
    "    feature_selection_pdf.points,\n",
    "    feature_selection_pdf.points,\n",
    "    \"Feature 0\",\n",
    "    \"Points\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    feature_selection_pdf,\n",
    "    feature_selection_pdf[\"ufs_0-1\"].map(lambda x: x[1]),\n",
    "    feature_selection_pdf.points,\n",
    "    feature_selection_pdf.points,\n",
    "    \"Feature 1\",\n",
    "    \"Points\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = list(\n",
    "    map(\n",
    "        lambda i: ALL_FEATURES[i], fit_result[\"0.1\"].selectedFeatures\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, selected_features, TARGET_VARIABLE, figsize=(4,4), \n",
    "    color=COLOR_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at it from the original pearson matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state that these data distribs are trash.\n",
    "\n",
    "So, for this reason, talk about overall and value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall as feature (min-max normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features are so correlated and performances of attempt 1 are trash, why not considering just the overall as a feature?\n",
    "\n",
    "Maybe, we're lucky and the overall captures some other characteristics thay may steer the prediction a little bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERALL = [\"avg(overall)\"]\n",
    "\n",
    "COLOR_OVERALL_MIN_MAX = \"green\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=OVERALL, outputCol=\"overall_vec\"\n",
    ")\n",
    "\n",
    "# min_max_df = assembler.transform(min_max_df)\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERALL_MIN_MAX = [\"avg(overall)_min_max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"overall_vec\", \n",
    "    outputCol=\"overall_vec_min_max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_df = scaler.fit(min_max_df).transform(min_max_df)\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = min_max_df.toPandas()\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO make a commodity function, because it will be used in other normalizations as well\n",
    "\n",
    "pdf = pdf.reindex(\n",
    "    columns=list(pdf.columns) + OVERALL_MIN_MAX\n",
    ")\n",
    "\n",
    "pdf[OVERALL_MIN_MAX] = pdf[\n",
    "    \"overall_vec_min_max\"\n",
    "].transform(\n",
    "    {\n",
    "        OVERALL_MIN_MAX[i]: itemgetter(i) for i, p in enumerate(OVERALL_MIN_MAX)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, OVERALL_MIN_MAX, TARGET_VARIABLE, figsize=(4,4), color=COLOR_OVERALL_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look, they are basically the same. In fact, if we check their correlation, we get... [high correlation on pearson]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, OVERALL_MIN_MAX, color = COLOR_OVERALL_MIN_MAX, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value as feature (min-max normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, we found this paper: \n",
    "\n",
    "So, why not try their approach as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE = [\"avg(value)\"]\n",
    "\n",
    "COLOR_VALUE_MIN_MAX = \"lime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=VALUE, outputCol=\"value_vec\"\n",
    ")\n",
    "\n",
    "# min_max_df = assembler.transform(min_max_df)\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT BREAKS HERE, if using legacy data as well :C\n",
    "\n",
    "# TODO handle value attribute in legacy datasets\n",
    "# in legacy datasets it's encoded as \"ValueMagnitudeCurrency\" (i.e. 70M€)\n",
    "# gotta convert it to full length, to be compatible with modern (and actually usable!)\n",
    "\n",
    "# df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE_MIN_MAX = [\"avg(value)_min_max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"value_vec\", \n",
    "    outputCol=\"value_vec_min_max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_df = scaler.fit(min_max_df).transform(min_max_df)\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = min_max_df.toPandas()\n",
    "# NOTE keep the following line disabled!\n",
    "# gotta accumulate overall and value in the same pandas DF, so as we can use it\n",
    "# for the Pearson correlation matrix!\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO make a commodity function, because it will be used in other normalizations as well\n",
    "\n",
    "pdf = pdf.reindex(\n",
    "    columns=list(pdf.columns) + VALUE_MIN_MAX\n",
    ")\n",
    "\n",
    "pdf[VALUE_MIN_MAX] = pdf[\n",
    "    \"value_vec_min_max\"\n",
    "].transform(\n",
    "    {\n",
    "        VALUE_MIN_MAX[i]: itemgetter(i) for i, p in enumerate(VALUE_MIN_MAX)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-target relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO set an appropriate figsize for Google Colab\n",
    "plot_feature_target_relation(\n",
    "    pdf, VALUE_MIN_MAX, TARGET_VARIABLE, figsize=(4,4), color=COLOR_VALUE_MIN_MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(pdf, VALUE_MIN_MAX, color = COLOR_VALUE_MIN_MAX, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint at a very high correlation, then show it with pearson matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.select(\"overall_vec_min_max\", \"value_vec_min_max\").toPandas()\n",
    "\n",
    "# need to flat every element of the columns, because they are in vectors\n",
    "pdf[\"overall_vec_min_max\"] = pdf[\"overall_vec_min_max\"].map(\n",
    "    lambda x: x[0]\n",
    ")\n",
    "pdf[\"value_vec_min_max\"] = pdf[\"value_vec_min_max\"].map(\n",
    "    lambda x: x[0]\n",
    ")\n",
    "\n",
    "plot_correlation_matrix(\n",
    "    pdf, \n",
    "    [\"overall_vec_min_max\", \"value_vec_min_max\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment this correlation, and move on with life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning for attempt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[\n",
    "    state that via hyperparam grid we'll set the feature column, meaning that we'll try to train the models on all of the attemps\n",
    "\n",
    "[state expected results, according to correlations and similar stuff]\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection_learning_df = feature_selection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection_train_df, feature_selection_test_df = feature_selection_learning_df.randomSplit(\n",
    "#     # [0.9, 0.1]\n",
    "#     [0.7, 0.3]\n",
    "# )\n",
    "\n",
    "learning_train_df, learning_test_df = df.randomSplit(\n",
    "    # [0.9,0.1]\n",
    "    # NOTE reactive 90/10 split, keep 70/30 just when using one league\n",
    "    [0.7,0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS_CV = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_regressor(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    regressor,\n",
    "    regressor_evaluation_metrics,\n",
    "    cv_evaluation_metrics,\n",
    "    hyperparams_grid\n",
    "):\n",
    "\n",
    "    cv_evaluators = {\n",
    "        metric: RegressionEvaluator(\n",
    "            labelCol=\"points\",\n",
    "            metricName=metric,\n",
    "        )\n",
    "        for metric in cv_evaluation_metrics\n",
    "    }\n",
    "\n",
    "    regressor_evaluators = {\n",
    "        metric: RegressionEvaluator(\n",
    "            labelCol=\"points\",\n",
    "            metricName=metric,\n",
    "        )\n",
    "        for metric in regressor_evaluation_metrics\n",
    "    }\n",
    "\n",
    "    cross_validations = {\n",
    "        metric: CrossValidator(\n",
    "            estimator=regressor,\n",
    "            estimatorParamMaps=hyperparams_grid,\n",
    "            evaluator=cv_evaluators[metric],\n",
    "            numFolds=NUM_FOLDS_CV,\n",
    "            collectSubModels=True\n",
    "        )\n",
    "        for metric in cv_evaluation_metrics\n",
    "    }\n",
    "\n",
    "    cross_validated = dict()\n",
    "\n",
    "    for metric in cv_evaluation_metrics:\n",
    "\n",
    "        cross_validated[metric] = cross_validations[\n",
    "            metric\n",
    "        ].fit(train_df)\n",
    "\n",
    "    if (isinstance(regressor, LinearRegression)):\n",
    "        \n",
    "        for metric in cv_evaluation_metrics:\n",
    "\n",
    "            training_result = cross_validated[\n",
    "                metric\n",
    "            ].bestModel.summary\n",
    "\n",
    "            print(\n",
    "                \"***** Evaluating Training Set, (Linear Regression, best model according to metric {}) *****\".format(\n",
    "                    metric\n",
    "                )\n",
    "            )\n",
    "            print(\"RMSE: {:.3f}\".format(training_result.rootMeanSquaredError))\n",
    "            print(\"R2: {:.3f}\".format(training_result.r2))\n",
    "            print(\"Adjusted R2: {:.3f}\".format(training_result.r2adj))\n",
    "            print()\n",
    "\n",
    "        predictions = {\n",
    "            metric: cross_validated[\n",
    "                metric\n",
    "            ].bestModel.transform(test_df)\n",
    "            for metric in cv_evaluation_metrics\n",
    "        }\n",
    "\n",
    "        for m, model in cross_validated.items():\n",
    "\n",
    "                print(\n",
    "                    \"*** {} Set, (rf, best model elected by {}) ***\".format(\n",
    "                        \"Test\", m\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                for e, evaluator in regressor_evaluators.items():\n",
    "\n",
    "                    print(\n",
    "                        \"{}: {}\".format(\n",
    "                            evaluator.getMetricName(), evaluator.evaluate(\n",
    "                                predictions[m]\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                print(\"*******************************\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        for stage_name, stage_df in zip([\"Train\", \"Test\"], [train_df, test_df]):\n",
    "            predictions = {\n",
    "                metric: cross_validated[\n",
    "                    metric\n",
    "                ].bestModel.transform(stage_df)\n",
    "                for metric in cv_evaluation_metrics\n",
    "            }\n",
    "\n",
    "            for m, model in cross_validated.items():\n",
    "\n",
    "                print(\n",
    "                    \"*** {} Set, (rf, best model elected by {}) ***\".format(\n",
    "                        stage_name, m\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                for e, evaluator in regressor_evaluators.items():\n",
    "\n",
    "                    print(\n",
    "                        \"{}: {}\".format(\n",
    "                            evaluator.getMetricName(), evaluator.evaluate(\n",
    "                                predictions[m]\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                print(\"*******************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_classifier(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    classifier,\n",
    "    classifier_evaluation_metrics,\n",
    "    cv_evaluation_metrics,\n",
    "    hyperparams_grid\n",
    "):\n",
    "\n",
    "    cv_evaluators = {\n",
    "        metric: MulticlassClassificationEvaluator(\n",
    "            labelCol=\"macro_place\",\n",
    "            metricName=metric,\n",
    "        )\n",
    "        for metric in cv_evaluation_metrics\n",
    "    }\n",
    "\n",
    "    classifier_evaluators = {\n",
    "        metric: MulticlassClassificationEvaluator(\n",
    "            labelCol=\"macro_place\",\n",
    "            metricName=metric,\n",
    "        )\n",
    "        for metric in classifier_evaluation_metrics\n",
    "    }\n",
    "\n",
    "    cross_validations = {\n",
    "        metric: CrossValidator(\n",
    "            estimator=classifier,\n",
    "            estimatorParamMaps=hyperparams_grid,\n",
    "            evaluator=cv_evaluators[metric],\n",
    "            numFolds=NUM_FOLDS_CV,\n",
    "            collectSubModels=True\n",
    "        )\n",
    "        for metric in cv_evaluation_metrics\n",
    "    }\n",
    "\n",
    "    cross_validated = dict()\n",
    "\n",
    "    for metric in cv_evaluation_metrics:\n",
    "\n",
    "        cross_validated[metric] = cross_validations[\n",
    "            metric\n",
    "        ].fit(train_df)\n",
    "\n",
    "    if (isinstance(classifier, LinearRegression)):\n",
    "        \n",
    "        for metric in cv_evaluation_metrics:\n",
    "\n",
    "            training_result = cross_validated[\n",
    "                metric\n",
    "            ].bestModel.summary\n",
    "\n",
    "            print(\n",
    "                \"***** Evaluating Training Set, (Linear Regression, best model according to metric {}) *****\".format(\n",
    "                    metric\n",
    "                )\n",
    "            )\n",
    "            print(\"RMSE: {:.3f}\".format(training_result.rootMeanSquaredError))\n",
    "            print(\"R2: {:.3f}\".format(training_result.r2))\n",
    "            print(\"Adjusted R2: {:.3f}\".format(training_result.r2adj))\n",
    "            print()\n",
    "\n",
    "        predictions = {\n",
    "            metric: cross_validated[\n",
    "                metric\n",
    "            ].bestModel.transform(test_df)\n",
    "            for metric in cv_evaluation_metrics\n",
    "        }\n",
    "\n",
    "        for m, model in cross_validated.items():\n",
    "\n",
    "                print(\n",
    "                    \"*** {} Set, (rf, best model elected by {}) ***\".format(\n",
    "                        \"Test\", m\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                for e, evaluator in classifier_evaluators.items():\n",
    "\n",
    "                    print(\n",
    "                        \"{}: {}\".format(\n",
    "                            evaluator.getMetricName(), evaluator.evaluate(\n",
    "                                predictions[m]\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                print(\"*******************************\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        for stage_name, stage_df in zip([\"Train\", \"Test\"], [train_df, test_df]):\n",
    "            predictions = {\n",
    "                metric: cross_validated[\n",
    "                    metric\n",
    "                ].bestModel.transform(stage_df)\n",
    "                for metric in cv_evaluation_metrics\n",
    "            }\n",
    "\n",
    "            for m, model in cross_validated.items():\n",
    "\n",
    "                print(\n",
    "                    \"*** {} Set, (rf, best model elected by {}) ***\".format(\n",
    "                        stage_name, m\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                for e, evaluator in classifier_evaluators.items():\n",
    "\n",
    "                    print(\n",
    "                        \"{}: {}\".format(\n",
    "                            evaluator.getMetricName(), evaluator.evaluate(\n",
    "                                predictions[m]\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                print(\"*******************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_train_df, lr_test_df = feature_selection_train_df, feature_selection_test_df\n",
    "lr_train_df, lr_test_df = learning_train_df, learning_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(\n",
    "    # featuresCol=\"feature_vec_ufs_0.1\", \n",
    "    labelCol=\"points\"\n",
    ")\n",
    "\n",
    "lr_evaluation_metrics = [\"r2\", \"mse\"]\n",
    "lr_evaluation_metrics_cv = [\"r2\"]\n",
    "\n",
    "lr_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        lr.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    #TODO add intermediate values: prof uses [0.0, 0.5, 1]\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 1])\n",
    "    #TODO add intermediate values: prof uses [0.0, 0.05, 0.1]\n",
    "    # .addGrid(lr.regParam, [0.0, 0.1])\n",
    "    # .addGrid(lr.fitIntercept, [True, False])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "find_best_regressor(\n",
    "    train_df=lr_train_df, \n",
    "    test_df=lr_test_df, \n",
    "    regressor=lr,\n",
    "    regressor_evaluation_metrics=lr_evaluation_metrics,\n",
    "    cv_evaluation_metrics=lr_evaluation_metrics_cv,\n",
    "    hyperparams_grid=lr_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_train_df, dt_test_df = feature_selection_train_df, feature_selection_test_df\n",
    "dt_train_df, dt_test_df = learning_train_df, learning_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(\n",
    "    # featuresCol=\"feature_vec\", \n",
    "    labelCol=\"points\"\n",
    ")\n",
    "\n",
    "dt_evaluation_metrics = [\"r2\", \"mse\"]\n",
    "dt_evaluation_metrics_cv = [\"r2\"]\n",
    "\n",
    "dt_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        dt.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    # .addGrid(dt.standardization, [True, False])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "find_best_regressor(\n",
    "    train_df=dt_train_df, \n",
    "    test_df=dt_test_df, \n",
    "    regressor=dt,\n",
    "    regressor_evaluation_metrics=dt_evaluation_metrics,\n",
    "    cv_evaluation_metrics=dt_evaluation_metrics_cv,\n",
    "    hyperparams_grid=dt_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_df, rf_test_df = learning_train_df, learning_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(\n",
    "    # featuresCol=\"feature_vec\", \n",
    "    labelCol=\"points\"\n",
    ")\n",
    "\n",
    "rf_evaluation_metrics = [\"r2\", \"mse\"]\n",
    "rf_evaluation_metrics_cv = [\"r2\"]\n",
    "\n",
    "rf_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        rf.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    # .addGrid(rf.standardization, [True, False])\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_regressor(\n",
    "    train_df=rf_train_df, \n",
    "    test_df=rf_test_df, \n",
    "    regressor=rf,\n",
    "    regressor_evaluation_metrics=rf_evaluation_metrics,\n",
    "    cv_evaluation_metrics=rf_evaluation_metrics_cv,\n",
    "    hyperparams_grid=rf_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_train_df, gbt_test_df = learning_train_df, learning_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(\n",
    "    # featuresCol=\"feature_vec\", \n",
    "    labelCol=\"points\"\n",
    ")\n",
    "\n",
    "gbt_evaluation_metrics = [\"r2\", \"mse\"]\n",
    "gbt_evaluation_metrics_cv = [\"r2\"]\n",
    "\n",
    "gbt_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        gbt.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    # .addGrid(gbt.standardization, [True, False])\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_regressor(\n",
    "    train_df=gbt_train_df, \n",
    "    test_df=gbt_test_df, \n",
    "    regressor=gbt,\n",
    "    regressor_evaluation_metrics=gbt_evaluation_metrics,\n",
    "    cv_evaluation_metrics=gbt_evaluation_metrics_cv,\n",
    "    hyperparams_grid=gbt_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate more advanced, dynamic macro placements\n",
    "def get_macro_place(place, league):\n",
    "\n",
    "    if league == \"German Bundesliga\":\n",
    "        if 1 <= place <= 4:\n",
    "            return 0.0\n",
    "        if 5 <= place <= 7:\n",
    "            return 1.0\n",
    "        if 8 <= place <= 10:\n",
    "            return 2.0\n",
    "        if 11 <= place <= 15:\n",
    "            return 3.0\n",
    "        if 16 <= place <= 18:\n",
    "            return 4.0\n",
    "\n",
    "    elif league == \"Holland Eredivise\":\n",
    "        if place == 1:\n",
    "            return 0.0\n",
    "        if 2 <= place <= 3:\n",
    "            return 1.0\n",
    "        if 4 <= place <= 9:\n",
    "            return 2.0\n",
    "        if 10 <= place <= 15:\n",
    "            return 3.0\n",
    "        if 16 <= place <= 18:\n",
    "            return 4.0\n",
    "\n",
    "\n",
    "    elif league == \"French League 1\":\n",
    "        if 1 <= place <= 2:\n",
    "            return 0.0\n",
    "        if 3 <= place <= 5:\n",
    "            return 1.0\n",
    "        if 6 <= place <= 11:\n",
    "            return 2.0\n",
    "        if 12 <= place <= 17:\n",
    "            return 3.0\n",
    "        if 18 <= place <= 20:\n",
    "            return  4.0\n",
    "    \n",
    "    else: #It, Sp, En\n",
    "        if 1 <= place <= 4:\n",
    "            return 0.0\n",
    "        if 5 <= place <= 7:\n",
    "            return 1.0\n",
    "        if 8 <= place <= 12:\n",
    "            return 2.0\n",
    "        if 13 <= place <= 17:\n",
    "            return 3.0\n",
    "        if 18 <= place <= 20:\n",
    "            return 4.0\n",
    "\n",
    "    return None\n",
    "\n",
    "get_macro_place_UDF = udf(\n",
    "    lambda place, league: get_macro_place(float(place), league),\n",
    "    DoubleType(),\n",
    ")\n",
    "\n",
    "NUM_MACRO_PLACES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_df = feature_selection_learning_df\n",
    "classification_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = classification_df.withColumn(\n",
    "    \"macro_place\", get_macro_place_UDF(col(\"place\"), col(\"League\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_train_df, classification_test_df = classification_df.randomSplit(\n",
    "    # [0.9,0.1]\n",
    "    # NOTE reactive 90/10 split, keep 70/30 just when using one league\n",
    "    [0.7,0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(\n",
    "    classification_df.toPandas(), \n",
    "    [\"macro_place\"], \n",
    "    color = \"teal\", \n",
    "    figsize=(10,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_train_df, svm_test_df = classification_train_df, classification_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(\n",
    "    featuresCol=\"ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "ovr = OneVsRest(\n",
    "    classifier=svm,\n",
    "    featuresCol=\"feature_vec_ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "svm_evaluation_metrics = [\"accuracy\"]\n",
    "svm_evaluation_metrics_cv = [\"accuracy\"]\n",
    "\n",
    "svm_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        ovr.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_best_classifier(\n",
    "#     train_df=svm_train_df, \n",
    "#     test_df=svm_test_df, \n",
    "#     classifier=ovr,\n",
    "#     classifier_evaluation_metrics=svm_evaluation_metrics,\n",
    "#     cv_evaluation_metrics=svm_evaluation_metrics_cv,\n",
    "#     hyperparams_grid=svm_param_grid\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lor_train_df, lor_test_df = classification_train_df, classification_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO same as SVM\n",
    "\n",
    "lor = LogisticRegression(\n",
    "    # featuresCol=\"ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "ovr = OneVsRest(\n",
    "    classifier=lor,\n",
    "    # featuresCol=\"ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "lor_evaluation_metrics = [\"accuracy\"]\n",
    "lor_evaluation_metrics_cv = [\"accuracy\"]\n",
    "\n",
    "lor_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        ovr.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_classifier(\n",
    "    train_df=lor_train_df, \n",
    "    test_df=lor_test_df, \n",
    "    classifier=ovr,\n",
    "    classifier_evaluation_metrics=lor_evaluation_metrics,\n",
    "    cv_evaluation_metrics=lor_evaluation_metrics_cv,\n",
    "    hyperparams_grid=lor_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_train_df, ct_test_df = classification_train_df, classification_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = DecisionTreeClassifier(\n",
    "    # featuresCol=\"ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "# ovr = OneVsRest(\n",
    "#     classifier=ct,\n",
    "#     featuresCol=\"feature_vec_ufs_0-1\", \n",
    "#     labelCol=\"macro_place\"\n",
    "# )\n",
    "\n",
    "ct_evaluation_metrics = [\"accuracy\"]\n",
    "ct_evaluation_metrics_cv = [\"accuracy\"]\n",
    "\n",
    "ct_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        ct.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_classifier(\n",
    "    train_df=ct_train_df, \n",
    "    test_df=ct_test_df, \n",
    "    classifier=ct,\n",
    "    classifier_evaluation_metrics=ct_evaluation_metrics,\n",
    "    cv_evaluation_metrics=ct_evaluation_metrics_cv,\n",
    "    hyperparams_grid=ct_param_grid\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_train_df, rfc_test_df = classification_train_df, classification_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(\n",
    "    # featuresCol=\"feature_vec_ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "# ovr = OneVsRest(\n",
    "#     classifier=rfc,\n",
    "#     featuresCol=\"feature_vec_ufs_0-1\", \n",
    "#     labelCol=\"macro_place\"\n",
    "# )\n",
    "\n",
    "rfc_evaluation_metrics = [\"accuracy\"]\n",
    "rfc_evaluation_metrics_cv = [\"accuracy\"]\n",
    "\n",
    "rfc_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        rfc.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_classifier(\n",
    "    train_df=rfc_train_df, \n",
    "    test_df=rfc_test_df, \n",
    "    classifier=rfc,\n",
    "    classifier_evaluation_metrics=rfc_evaluation_metrics,\n",
    "    cv_evaluation_metrics=rfc_evaluation_metrics_cv,\n",
    "    hyperparams_grid=rfc_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_train_df, mlp_test_df = classification_train_df, classification_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultilayerPerceptronClassifier(\n",
    "    # featuresCol=\"feature_vec_ufs_0-1\", \n",
    "    labelCol=\"macro_place\"\n",
    ")\n",
    "\n",
    "# ovr = OneVsRest(\n",
    "#     classifier=mlp,\n",
    "#     featuresCol=\"feature_vec_ufs_0-1\", \n",
    "#     labelCol=\"macro_place\"\n",
    "# )\n",
    "\n",
    "mlp_evaluation_metrics = [\"accuracy\"]\n",
    "mlp_evaluation_metrics_cv = [\"accuracy\"]\n",
    "\n",
    "mlp_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(mlp.layers, [[NUM_FEATURES, 4, 4, 2, NUM_MACRO_PLACES]])\n",
    "    .addGrid(\n",
    "        mlp.featuresCol, [\n",
    "            \"ufs_0-1\", \n",
    "            # \"value_vec_min_max\",\n",
    "            # \"all_vec_min_max_pcs\",\n",
    "            # \"all_vec_min_max\"\n",
    "        ]\n",
    "    )\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_classifier(\n",
    "    train_df=mlp_train_df, \n",
    "    test_df=mlp_test_df, \n",
    "    classifier=mlp,\n",
    "    classifier_evaluation_metrics=mlp_evaluation_metrics,\n",
    "    cv_evaluation_metrics=mlp_evaluation_metrics_cv,\n",
    "    hyperparams_grid=mlp_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom ranking-based evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare ranking of prediction value with actual ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 3: clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_df = pre_processed_df\n",
    "\n",
    "CLUSTERING_FEATURES = copy.deepcopy(PLAYER_FEATURES)\n",
    "CLUSTERING_FEATURES.remove(\"overall\")\n",
    "CLUSTERING_FEATURES.remove(\"value\")\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=CLUSTERING_FEATURES, outputCol=\"all_vec\"\n",
    ")\n",
    "\n",
    "clustering_df = assembler.transform(clustering_df)\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"all_vec\", outputCol=\"all_vec_min_max\"\n",
    ")\n",
    "\n",
    "clustering_df = scaler.fit(clustering_df).transform(clustering_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(\n",
    "    dataset,\n",
    "    n_clusters,\n",
    "    distance_measure=\"euclidean\",\n",
    "    max_iter=20,\n",
    "    features_col=\"features\",\n",
    "    prediction_col=\"cluster\",\n",
    "    random_seed=RANDOM_SEED,\n",
    "):\n",
    "\n",
    "    print(\n",
    "        f\"\"\"Training K-means clustering using the following parameters: \n",
    "        - K (n. of clusters) = {n_clusters}\n",
    "        - max_iter (max n. of iterations) = {max_iter}\n",
    "        - distance measure = {distance_measure}\n",
    "        - random seed = {random_seed if random_seed is not None else \"NONE USED!\"}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # if distance_measure == \"cosine\":\n",
    "\n",
    "        # NOTE we already start from normalized/std data, so, NO need to do it\n",
    "\n",
    "        # Normalize inputs to unit-length vectors\n",
    "        # dataset = Normalizer(\n",
    "        #     inputCol=features_col, outputCol=features_col + \"_norm\", p=1\n",
    "        # ).transform(dataset)\n",
    "\n",
    "        # features_col = features_col + \"_norm\"\n",
    "\n",
    "    # Train a K-means model\n",
    "    kmeans = KMeans(\n",
    "        featuresCol=features_col,\n",
    "        predictionCol=prediction_col,\n",
    "        k=n_clusters,\n",
    "        initMode=\"k-means||\",\n",
    "        initSteps=5,\n",
    "        tol=0.000001,\n",
    "        maxIter=max_iter,\n",
    "        # seed=random_seed,\n",
    "        distanceMeasure=distance_measure,\n",
    "    )\n",
    "\n",
    "    model = kmeans.fit(dataset)\n",
    "    # here there are all the relevant clustering information\n",
    "\n",
    "    # Make clusters\n",
    "    clusters_df = model.transform(dataset)\n",
    "\n",
    "    return model, clusters_df\n",
    "\n",
    "\n",
    "def evaluate_k_means(\n",
    "    clusters,\n",
    "    metric_name=\"silhouette\",\n",
    "    distance_measure=\"squaredEuclidean\",  # cosine\n",
    "    prediction_col=\"cluster\",\n",
    "    featuresCol = \"features\"\n",
    "):\n",
    "\n",
    "    # Evaluate clustering by computing Silhouette score\n",
    "    evaluator = ClusteringEvaluator(\n",
    "        metricName=metric_name,\n",
    "        distanceMeasure=distance_measure,\n",
    "        predictionCol=prediction_col,\n",
    "        featuresCol=featuresCol\n",
    "    )\n",
    "\n",
    "    return evaluator.evaluate(clusters)\n",
    "\n",
    "\n",
    "def do_clustering(\n",
    "    k_range, \n",
    "    input_df, \n",
    "    max_iter,\n",
    "    featuresCol,\n",
    "    clusterCol\n",
    "):\n",
    "    clustering_results = {}\n",
    "\n",
    "    clusters_df = input_df\n",
    "\n",
    "    # for k in tqdm( range(5, max_k_clusters + 1, 5), desc = \"Performing clustering\" ):\n",
    "    # for k in tqdm(range(2, max_k_clusters + 1, 4), desc=\"Performing clustering\"):\n",
    "    for k in tqdm(k_range, desc=\"Performing clustering\"):\n",
    "\n",
    "        print(f\"Running K-means using K = {k}\")\n",
    "\n",
    "        # model, clusters_df = k_means(tf_idf_df, k, max_iter=50, distance_measure=\"cosine\") # Alternatively, distance_measure=\"euclidean\"\n",
    "        # model, clusters_df = k_means(input_df, k, max_iter=max_iter, distance_measure=\"cosine\") # Alternatively, distance_measure=\"euclidean\"\n",
    "        model, clusters_df = k_means(\n",
    "            # input_df,\n",
    "            clusters_df, \n",
    "            k, \n",
    "            max_iter=max_iter, \n",
    "            distance_measure=\"cosine\", \n",
    "            features_col=featuresCol,\n",
    "            prediction_col=clusterCol + \"_k_\" + str(k),\n",
    "        )  # Alternatively, distance_measure=\"euclidean\"\n",
    "        # silhouette_k = evaluate_k_means(clusters_df, distance_measure=\"cosine\") # Alternatively, distance_measure=\"squaredEuclidean\"\n",
    "        silhouette_k = evaluate_k_means(\n",
    "            clusters_df, \n",
    "            distance_measure=\"cosine\",\n",
    "            prediction_col=clusterCol + \"_k_\" + str(k),\n",
    "            featuresCol=featuresCol\n",
    "\n",
    "        )  # Alternatively, distance_measure=\"squaredEuclidean\"\n",
    "        # wssd_k = model.summary.trainingCost\n",
    "        wssd_k = model.summary\n",
    "\n",
    "        print(\n",
    "            \"Silhouette coefficient computed with cosine distance: {:.3f}\".format(\n",
    "                silhouette_k\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"Within-cluster Sum of Squared Distances (using cosine distance): {:.3f}\".format(\n",
    "                wssd_k.trainingCost\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"--------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "\n",
    "        l = list(\n",
    "            enumerate(\n",
    "                model.clusterCenters()\n",
    "            )\n",
    "        )\n",
    "        l = [(ind, DenseVector(c)) for ind, c in l]\n",
    "        # print(l)\n",
    "        schema = [\"cluster_id\"  + \"_k_\" + str(k), \"centroid\"  + \"_k_\" + str(k)]\n",
    "\n",
    "        schema = StructType([ \n",
    "            StructField(\"cluster_id\" + \"_k_\" + str(k),IntegerType(),True), \n",
    "            StructField(\"centroid\" + \"_k_\" + str(k),VectorUDT(),True), \n",
    "        ])\n",
    "        centr_df = spark.createDataFrame(data=l, schema=schema)\n",
    "\n",
    "        # centr_df.show()\n",
    "\n",
    "        # df_with_centroids = clusters_df.join(\n",
    "        #     centr_df, on=[\"cluster_id\"]\n",
    "        # )\n",
    "\n",
    "        clusters_df = clusters_df.join(\n",
    "            centr_df, on=[\"cluster_id\" + \"_k_\" + str(k)]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        clustering_results[str(k)] = {\n",
    "            \"silhouette_k\"      : silhouette_k,\n",
    "            \"wssd_k\"            : wssd_k,\n",
    "            \"model\"             : model,\n",
    "            \"df\"                : clusters_df,\n",
    "            # \"cluster_centroids\" : model.clusterCenters(),\n",
    "            # \"centr_df\" : centr_df,\n",
    "            # \"df_with_centroids\" : df_with_centroids\n",
    "        }\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Free up memory space at the end of each iteration\n",
    "        # del model\n",
    "        # del clusters_df\n",
    "        # gc.collect() # garbage collector\n",
    "    \n",
    "    clustering_results[\"df_with_centroid_full\"] = clusters_df\n",
    "\n",
    "    return clustering_results\n",
    "\n",
    "\n",
    "def plot_clustering_results(clustering_results):\n",
    "\n",
    "    # print(clustering_results)\n",
    "\n",
    "    k_col = list(clustering_results.keys())[:-1]\n",
    "    wssd_col = [\n",
    "        clustering_results[k][\"wssd_k\"].trainingCost for k in k_col \n",
    "    ]\n",
    "    silhouette_col = [\n",
    "        clustering_results[k][\"silhouette_k\"] for k in k_col \n",
    "    ]\n",
    "\n",
    "    # print(k_col)\n",
    "    # print(wssd_col)\n",
    "    # print(silhouette_col)\n",
    "\n",
    "    plot_df_temp = pd.DataFrame([k_col, wssd_col, silhouette_col]).transpose()\n",
    "    plot_df_temp.columns = [\"K\", \"WSSD\", \"Silhouette\"]\n",
    "    # print(plot_df_temp)\n",
    "\n",
    "    # Create a 1x1 figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    _ = sns.pointplot(\n",
    "        data=plot_df_temp, x=\"K\", y=\"WSSD\", ax=ax, color=\"orangered\"\n",
    "    )\n",
    "    _ = ax.set_xlabel(\"K\")\n",
    "    _ = ax.set_ylabel(\"WSSD\")\n",
    "    \n",
    "    # Create a 1x1 figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    _ = sns.pointplot(\n",
    "        data=plot_df_temp, x=\"K\", y=\"Silhouette\", ax=ax, color=\"orangered\"\n",
    "    )\n",
    "    _ = ax.set_xlabel(\"K\")\n",
    "    _ = ax.set_ylabel(\"Silhouette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_df_dict = {\n",
    "    s: clustering_df.filter(col(\"season\") == s) for s in seasons\n",
    "}\n",
    "\n",
    "MAX_K_CLUSTERS = 8\n",
    "MAX_ITER = 20\n",
    "\n",
    "k_range = range(2, MAX_K_CLUSTERS, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_results_dict = dict()\n",
    "\n",
    "for s in [\"20\"]:\n",
    "    clustering_results_dict[s] = do_clustering(\n",
    "        k_range, \n",
    "        clustering_df_dict[s], \n",
    "        MAX_ITER,\n",
    "        \"all_vec_min_max\",\n",
    "        \"cluster_id\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_together_df = reduce(\n",
    "    DataFrame.unionAll, \n",
    "    [\n",
    "        clustering_results_dict[s][\"df_with_centroid_full\"] for s in [\"20\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutating single seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in [\"20\"]:\n",
    "    plot_clustering_results(clustering_results_dict[s])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating seasons all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO select K according to clustering evaluation\n",
    "K = [str(k) for k in k_range]\n",
    "# K = [\"2\",\"6\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute distance between player and centroid of its cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_distance_from_centroid_UDF = udf(\n",
    "    lambda player, centroid: float(\n",
    "        Vectors.squared_distance(\n",
    "            player, centroid\n",
    "        )\n",
    "    ), FloatType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K:\n",
    "    all_together_df = all_together_df.withColumn(\n",
    "        \"distance_from_centroid\" + \"_k_\" + str(k),\n",
    "        compute_distance_from_centroid_UDF(\n",
    "            col(\"all_vec_min_max\"),\n",
    "            col(\"centroid\" + \"_k_\" + str(k))\n",
    "        )\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From players to teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = all_together_df.groupBy(\n",
    "    [\"season\", \"club_name\", \"macro_role\"]\n",
    ").agg(\n",
    "    { \n",
    "        \"distance_from_centroid\" + \"_k_\" + str(k): \"avg\" for k in K \n",
    "    }\n",
    ")\n",
    "\n",
    "for k in K:\n",
    "    teams_df = teams_df.withColumnRenamed(\n",
    "        \"avg(distance_from_centroid\" + \"_k_\" + str(k) + \")\",\n",
    "        \"avg_distance_from_centroid\" + \"_k_\" + str(k)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subquery(macro_role, k):\n",
    "    return f\"\"\"(\n",
    "        case\n",
    "            when macro_role='{macro_role}' then avg_distance_from_centroid_k_{k} \n",
    "        else NULL\n",
    "        end\n",
    "    ) as avg_dist_macro_role_{int(macro_role)}_k_{k}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df.createOrReplaceTempView(\"t\")\n",
    "\n",
    "temp = dict()\n",
    "\n",
    "for k in K:\n",
    "\n",
    "    temp[k] = (\n",
    "        spark.sql(\n",
    "            f\"\"\"\n",
    "                select season, club_name, {generate_subquery(0.0, k)}, {generate_subquery(1.0, k)}, {generate_subquery(2.0, k)}, {generate_subquery(3.0, k)}, {generate_subquery(4.0, k)}, {generate_subquery(5.0, k)}, {generate_subquery(6.0, k)}, {generate_subquery(7.0, k)}\n",
    "                from t\n",
    "            \"\"\"\n",
    "        )\n",
    "        .groupBy(\"season\", \"club_name\")\n",
    "        .agg(\n",
    "            # TODO use for loop as in second cell of \"from players to teams\"\n",
    "            sum(f\"avg_dist_macro_role_0_k_{k}\").alias(f\"avg_dist_macro_role_0_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_1_k_{k}\").alias(f\"avg_dist_macro_role_1_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_2_k_{k}\").alias(f\"avg_dist_macro_role_2_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_3_k_{k}\").alias(f\"avg_dist_macro_role_3_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_4_k_{k}\").alias(f\"avg_dist_macro_role_4_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_5_k_{k}\").alias(f\"avg_dist_macro_role_5_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_6_k_{k}\").alias(f\"avg_dist_macro_role_6_k_{k}\"),\n",
    "            sum(f\"avg_dist_macro_role_7_k_{k}\").alias(f\"avg_dist_macro_role_7_k_{k}\"),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE executing this cell n times, w/o restoring teams_df --> \n",
    "# n copies of avg_dist_macro_role_[0:7]_k_[2, 6]\n",
    "\n",
    "teams_df = temp[str(K[0])]\n",
    "\n",
    "for i in range(1, len(K)):\n",
    "    teams_df = teams_df.join(\n",
    "        temp[str(K[i])], on=[\"season\", \"club_name\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_distances_dict = dict()\n",
    "avg_distances_vec_dict = dict()\n",
    "\n",
    "for k in K:\n",
    "    \n",
    "    avg_distances_dict[k] = [\n",
    "        f\"avg_dist_macro_role_{i}_k_{k}\" for i in range(0, NUM_MACRO_ROLES)\n",
    "    ]\n",
    "\n",
    "    avg_distances_vec_dict[k] = f\"avg_dist_vec_k_{k}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "\n",
    "global_max = teams_df.select(\n",
    "    greatest(\n",
    "        *list(\n",
    "            itertools.chain.from_iterable(\n",
    "                avg_distances_dict.values()\n",
    "            )\n",
    "        )\n",
    "    ).alias(\"row_wise_max\")\n",
    ").collect()\n",
    "\n",
    "global_max = [row[\"row_wise_max\"] for row in global_max]\n",
    "\n",
    "global_max = builtins.max(global_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = teams_df.fillna(global_max * 1.5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K:\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=avg_distances_dict[k], outputCol=avg_distances_vec_dict[k]\n",
    "    )\n",
    "\n",
    "    teams_df = assembler.transform(teams_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_learning_df = teams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_learning_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_learning_df = clustering_learning_df.join(\n",
    "    df, \n",
    "    on=[\"club_name\", \"season\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_learning_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_learning_df_train_df, clustering_learning_df_test_df = clustering_learning_df.randomSplit(\n",
    "    # [0.9,0.1]\n",
    "    # NOTE reactive 90/10 split, keep 70/30 just when using one league\n",
    "    [0.7,0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS_CV = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_df, lr_test_df = clustering_learning_df_train_df, clustering_learning_df_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(\n",
    "    # featuresCol=\"feature_vec_ufs_0.1\", \n",
    "    labelCol=\"points\"\n",
    ")\n",
    "\n",
    "# lr_evaluation_metrics = [\"r2\", \"mse\"]\n",
    "lr_evaluation_metrics = [\"r2\"]\n",
    "lr_evaluation_metrics_cv = [\"r2\"]\n",
    "\n",
    "lr_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        lr.featuresCol,\n",
    "         [\"avg_dist_vec_k_2\"]\n",
    "        # avg_distances_vec_dict\n",
    "    )\n",
    "    #TODO add intermediate values: prof uses [0.0, 0.5, 1]\n",
    "    # .addGrid(lr.elasticNetParam, [0.0, 1])\n",
    "    #TODO add intermediate values: prof uses [0.0, 0.05, 0.1]\n",
    "    # .addGrid(lr.regParam, [0.0, 0.1])\n",
    "    # .addGrid(lr.fitIntercept, [True, False])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "find_best_regressor(\n",
    "    train_df=lr_train_df, \n",
    "    test_df=lr_test_df, \n",
    "    regressor=lr,\n",
    "    regressor_evaluation_metrics=lr_evaluation_metrics,\n",
    "    cv_evaluation_metrics=lr_evaluation_metrics_cv,\n",
    "    hyperparams_grid=lr_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering-learning cross evaluation\n",
    "\n",
    "Classic left-right plot, with:\n",
    "Left Y --> elbow result\n",
    "Right Y --> accuracy\n",
    "X axis --> # clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 4: thinking \"Deep\", shallow injecting some priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PLACE = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the prior (RP coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df.select(\"season\", \"club_name\", \"place\").createOrReplaceTempView(\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = spark.sql(\n",
    "    f\"\"\"\n",
    "    select t.season, t.club_name,\n",
    "        avg(\n",
    "            (\n",
    "                select sub.place\n",
    "                where sub.season < t.season and sub.club_name == t.club_name\n",
    "            )\n",
    "        ) as rp_coeff\n",
    "    from t, t as sub\n",
    "    group by t.season, t.club_name\n",
    "    order by t.season desc\n",
    "    \"\"\"\n",
    ").fillna(MAX_PLACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = rp_df.withColumn(\"rp_coeff\", MAX_PLACE - col(\"rp_coeff\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df.select(\"club_name\", \"season\", \"rp_coeff\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_RP_TRADEOFF = 1\n",
    "\n",
    "add_normalize_by_rp_UDF = udf(\n",
    "    lambda points, rp: points + ADD_RP_TRADEOFF * rp, DoubleType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = rp_df.join(\n",
    "    df, on=[\"club_name\", \"season\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_df = rp_df.withColumn(\n",
    "    \"avg(overall)_rp_normalized\", add_normalize_by_rp_UDF(\n",
    "        col(\"avg(overall)\"), col(\"rp_coeff\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"avg(overall)_rp_normalized\"], \n",
    "    outputCol=\"feature_vec\"\n",
    ")\n",
    "\n",
    "rp_df = assembler.transform(rp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS_CV = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rp_df = rp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rp_train_df, learning_rp_test_df = learning_rp_df.randomSplit(\n",
    "    # [0.9, 0.1]\n",
    "    [0.7, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_df, lr_test_df = learning_rp_train_df, learning_rp_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(\n",
    "    # featuresCol=\"feature_vec_ufs_0.1\", \n",
    "    labelCol=\"points\"\n",
    ")\n",
    "\n",
    "lr_evaluation_metrics = [\"r2\", \"mse\"]\n",
    "lr_evaluation_metrics_cv = [\"r2\"]\n",
    "\n",
    "lr_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\n",
    "        lr.featuresCol, [\n",
    "            \"feature_vec\", \n",
    "        ]\n",
    "    )\n",
    "    #TODO add intermediate values: prof uses [0.0, 0.5, 1]\n",
    "    # .addGrid(lr.elasticNetParam, [0.0, 1])\n",
    "    #TODO add intermediate values: prof uses [0.0, 0.05, 0.1]\n",
    "    # .addGrid(lr.regParam, [0.0, 0.1])\n",
    "    # .addGrid(lr.fitIntercept, [True, False])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "find_best_regressor(\n",
    "    train_df=lr_train_df, \n",
    "    test_df=lr_test_df, \n",
    "    regressor=lr,\n",
    "    regressor_evaluation_metrics=lr_evaluation_metrics,\n",
    "    cv_evaluation_metrics=lr_evaluation_metrics_cv,\n",
    "    hyperparams_grid=lr_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RP impact\n",
    "\n",
    "plot showing that the more the weight of the RP coefficient is increased, the more the accuracy ofc goes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bdc-scraper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dbee12c48506d0ea52066729cdbd5c841c92d7e5282187c802887604930b19f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
